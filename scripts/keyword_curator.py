#!/usr/bin/env python3
"""
Keyword Curator - Semi-automated keyword research for blog content

Generates keyword candidates using Claude API based on KEYWORD_STRATEGY.md
Provides interactive selection interface for human filtering (5 minutes weekly)

Usage:
    python scripts/keyword_curator.py
    python scripts/keyword_curator.py --count 15
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import requests

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

try:
    import certifi
except ImportError:
    safe_print("Warning: certifi not installed - SSL verification may fail")
    certifi = None

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent))
from utils.security import safe_print, mask_secrets

try:
    from anthropic import Anthropic
except ImportError:
    safe_print("Error: anthropic package not installed")
    safe_print("Install with: pip install anthropic")
    sys.exit(1)


CURATION_PROMPT_WITH_TRENDS = """Ïó≠Ìï†:
ÎÑàÎäî Í¥ëÍ≥† ÏàòÏùµ ÏµúÏ†ÅÌôîÎ•º ÏúÑÌïú ÌÇ§ÏõåÎìú ÌÅêÎ†àÏù¥ÌÑ∞Îã§.
ÏïÑÎûò Ïã§ÏãúÍ∞Ñ Ìä∏Î†åÎìú Í≤ÄÏÉâ Í≤∞Í≥ºÏôÄ Ïª§ÎÆ§ÎãàÌã∞ ÌÜ†ÌîΩÏùÑ Î∞îÌÉïÏúºÎ°ú **Í≥†CPC, Í∞êÏ†ï Î∞òÏùëÌòï** ÌÇ§ÏõåÎìúÎ•º Ï†úÏïàÌïòÎùº.

üìä **ÏÜåÏä§ ÎπÑÏ§ë**: Google Trends 40% + Community 40% + Evergreen 20%
üìä **Ïñ∏Ïñ¥ ÎπÑÏ§ë**: EN 40% ({en_count}Í∞ú), KO 40% ({ko_count}Í∞ú), JA 20% ({ja_count}Í∞ú)

Ïã§ÏãúÍ∞Ñ Ìä∏Î†åÎìú Îç∞Ïù¥ÌÑ∞ (Ïñ∏Ïñ¥Î≥ÑÎ°ú Íµ¨Î∂ÑÎê®):

üá∫üá∏ English (US) Trends:
{trends_en}

üá∞üá∑ Korean (KR) Trends:
{trends_ko}

üáØüáµ Japanese (JP) Trends:
{trends_ja}

üåê **Community Topics (HackerNews, Dev.to, Lobsters, ProductHunt)**:
{community_topics}

**Ï§ëÏöî**: Community TopicsÎäî ÏòÅÏñ¥ ÏõêÎ¨∏Ïù¥ÏßÄÎßå, ÌïúÍµ≠/ÏùºÎ≥∏ ÎèÖÏûêÏóêÍ≤åÎèÑ Ïú†Ïö©Ìïú ÎÇ¥Ïö©Ïù¥Î©¥ KO/JA Î≤ÑÏ†ÑÏúºÎ°úÎèÑ Ï†úÏïàÌïòÎùº.
Ïòà: "OpenAI releases new model" ‚Üí EN ÏõêÎ¨∏ + KO "OpenAI ÏÉà Î™®Îç∏ Ï∂úÏãú" + JA "OpenAIÊñ∞„É¢„Éá„É´Áô∫Ë°®"

**üî¥ Ï§ëÏöî Í∑úÏπô: Ïñ∏Ïñ¥-ÌÇ§ÏõåÎìú Îß§Ïπ≠ (CRITICAL - ÏúÑÎ∞ò Ïãú Ï¶âÏãú Í±∞Î∂Ä)**
1. English (US) Ìä∏Î†åÎìúÏùò Query ‚Üí language: "en"ÏúºÎ°úÎßå ÏÇ¨Ïö©
2. Korean (KR) Ìä∏Î†åÎìúÏùò Query ‚Üí language: "ko"Î°úÎßå ÏÇ¨Ïö©
3. Japanese (JP) Ìä∏Î†åÎìúÏùò Query ‚Üí language: "ja"Î°úÎßå ÏÇ¨Ïö©
4. **Ï†àÎåÄÎ°ú ÏùºÎ≥∏Ïñ¥ ÌÇ§ÏõåÎìúÎ•º ÌïúÍµ≠Ïñ¥ Í≤åÏãúÎ¨ºÏóê ÏÇ¨Ïö©ÌïòÍ±∞ÎÇò, ÌïúÍµ≠Ïñ¥ ÌÇ§ÏõåÎìúÎ•º ÏùºÎ≥∏Ïñ¥ Í≤åÏãúÎ¨ºÏóê ÏÇ¨Ïö©ÌïòÏßÄ Îßê Í≤É**
5. ÏúÑ Ìä∏Î†åÎìú Îç∞Ïù¥ÌÑ∞Ïùò QueryÎ•º Í∑∏ÎåÄÎ°ú keywordÎ°ú ÏÇ¨Ïö©ÌïòÎùº. Ï†àÎåÄ Ïû¨Ìï¥ÏÑùÌïòÍ±∞ÎÇò Ïû¨ÏûëÏÑ±ÌïòÏßÄ Îßê Í≤É.

**üö® Ïñ∏Ïñ¥ Î¨∏Ïûê Í≤ÄÏ¶ù Í∑úÏπô (Î∞òÎìúÏãú Ï§ÄÏàò):**
- **ÏòÅÏñ¥(en) ÌÇ§ÏõåÎìú**: ÌïúÍ∏Ä(Í∞Ä-Ìû£), ÌûàÎùºÍ∞ÄÎÇò(„ÅÅ-„Çì), Í∞ÄÌÉÄÏπ¥ÎÇò(„Ç°-„É∂), ÌïúÏûê(‰∏Ä-ÈæØ) Ìè¨Ìï® Í∏àÏßÄ
  - Ïò¨Î∞îÎ•∏ Ïòà: "NBA", "Kobe Bryant", "quad cortex"
  - ÏûòÎ™ªÎêú Ïòà: "Î∂âÏùÄÏÇ¨Îßâ" (ÌïúÍ∏Ä Ìè¨Ìï®), "„Éï„Ç©„Éº„Éà„Éä„Ç§„Éà" (Í∞ÄÌÉÄÏπ¥ÎÇò Ìè¨Ìï®)
- **ÌïúÍµ≠Ïñ¥(ko) ÌÇ§ÏõåÎìú**: Î∞òÎìúÏãú ÌïúÍ∏Ä(Í∞Ä-Ìû£) Ìè¨Ìï® ÌïÑÏöî
  - Ïò¨Î∞îÎ•∏ Ïòà: "Î∂âÏùÄÏÇ¨Îßâ", "ÍπÄÏó∞ÏïÑ", "u23" (ÏòÅÎ¨∏ ÏïΩÏñ¥Îäî ÌóàÏö©)
  - ÏûòÎ™ªÎêú Ïòà: "red desert" (ÌïúÍ∏Ä ÏóÜÏùå), "„Éï„Ç©„Éº„Éà„Éä„Ç§„Éà" (ÏùºÎ≥∏Ïñ¥)
- **ÏùºÎ≥∏Ïñ¥(ja) ÌÇ§ÏõåÎìú**: Î∞òÎìúÏãú ÌûàÎùºÍ∞ÄÎÇò/Í∞ÄÌÉÄÏπ¥ÎÇò/ÌïúÏûê Ìè¨Ìï® ÌïÑÏöî
  - Ïò¨Î∞îÎ•∏ Ïòà: "„Éï„Ç©„Éº„Éà„Éä„Ç§„Éà", "‰∏âÁ¨òËñ´", "Âú∞ÈúáÈÄüÂ†±"
  - ÏûòÎ™ªÎêú Ïòà: "fortnite" (ÏùºÎ≥∏Ïñ¥ Î¨∏Ïûê ÏóÜÏùå), "Î∂âÏùÄÏÇ¨Îßâ" (ÌïúÍ∏Ä)

Î™©Ìëú:
ÌïúÍµ≠Ïñ¥ / ÏòÅÏñ¥ / ÏùºÎ≥∏Ïñ¥ Í∞ÅÍ∞ÅÏóêÏÑú
**Î∂àÏïà, Î∂ÑÎÖ∏, Í∂ÅÍ∏àÏ¶ù**ÏùÑ Ïú†Î∞úÌïòÎäî ÌÇ§ÏõåÎìúÎßå Ï†úÏïàÌïòÎùº.

Í∏àÏßÄ:
- Ï∂îÏÉÅÏ†ÅÏù∏ Ìä∏Î†åÎìú ÏöîÏïΩ ("AI Ìä∏Î†åÎìú", "ÏÉàÎ°úÏö¥ Í∏∞Ïà†")
- ÍµêÏú°/Ï†ïÎ≥¥ÏÑ± ÌÇ§ÏõåÎìú ("~ÌïòÎäî Î∞©Î≤ï", "~ÎûÄ Î¨¥ÏóáÏù∏Í∞Ä")
- Í∏çÏ†ïÏ†ÅÏù¥Í≥† ÌèâÌôîÎ°úÏö¥ ÌÇ§ÏõåÎìú
- **QueryÎ•º Ïû¨Ìï¥ÏÑùÌïòÍ±∞ÎÇò Îã§Ïãú Ïì∞Îäî Í≤É**
- **Í∞ôÏùÄ ÌÇ§ÏõåÎìúÎ•º Îã§Î•∏ Ïπ¥ÌÖåÍ≥†Î¶¨Î°ú Ï§ëÎ≥µ Ï†úÏïàÌïòÎäî Í≤É** (Ïòà: "Áõ∏ËëâÈõÖÁ¥Ä"Î•º techÏôÄ society Î™®ÎëêÏóê Ï†úÏïàÌïòÏßÄ Îßê Í≤É. ÌïòÎÇòÏùò ÌÇ§ÏõåÎìúÎäî ÌïòÎÇòÏùò Ïπ¥ÌÖåÍ≥†Î¶¨Îßå Í∞ÄÏ†∏Ïïº Ìï®)

Ï∂úÎ†• ÌòïÏãù:
Î∞òÎìúÏãú JSON ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌïòÎùº.

[
  {{
    "keyword": "ÏúÑ Ìä∏Î†åÎìú Îç∞Ïù¥ÌÑ∞Ïùò QueryÎ•º Í∑∏ÎåÄÎ°ú Î≥µÏÇ¨ (Ïû¨Ìï¥ÏÑù Í∏àÏßÄ)",
    "raw_search_title": "ÏÇ¨Ïö©ÏûêÍ∞Ä Íµ¨Í∏ÄÏóê Í≤ÄÏÉâÌï† Îïå Ï†ïÌôïÌûà ÏûÖÎ†•ÌïòÎäî Í≤ÄÏÉâÏñ¥ (keywordÏôÄ ÎèôÏùºÌïòÍ≤å)",
    "editorial_title": "Í∏∞ÏÇ¨ Ï†úÎ™© ÌòïÏãùÏùò ÎèÖÏûê ÏπúÌôîÏ†Å Ï†úÎ™©",
    "core_fear_question": "ÏÇ¨Ïö©ÏûêÏùò ÌïµÏã¨ ÎëêÎ†§ÏõÄÏùÑ Îã¥ÏùÄ ÏßàÎ¨∏ Ìïú Î¨∏Ïû•",
    "language": "ko",
    "category": "tech",  # ONLY: tech, business, society, entertainment, sports (5 categories + 1 reserved)
    "search_intent": "ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÄÍ∏à ÎãπÏû• Í≤ÄÏÉâÌïòÎäî Ïù¥Ïú† (ÌñâÎèôÌïòÏßÄ ÏïäÏúºÎ©¥ Î¨¥ÏóáÏùÑ ÏûÉÎäîÏßÄ)",
    "angle": "Ïù¥ ÌÇ§ÏõåÎìúÎ•º Îã§Î£∞ ÎïåÏùò Í¥ÄÏ†ê",
    "competition_level": "low",
    "why_it_works": "ÏÇ¨Ïö©ÏûêÍ∞Ä ÏßÄÍ∏à ÌñâÎèôÌïòÏßÄ ÏïäÏúºÎ©¥ ÏòÅÍµ¨Ï†ÅÏúºÎ°ú Î¨¥ÏóáÏùÑ ÏûÉÎäîÏßÄ (ÎßàÍ∞ê/Í∏∞Ìöå ÏÜêÏã§ Ï§ëÏã¨)",
    "purpose": "high competitionÏù∏ Í≤ΩÏö∞ÏóêÎßå: Traffic acquisition / Brand positioning / Viral content Ï§ë ÌïòÎÇò",
    "keyword_type": "trend",
    "priority": 7,
    "risk_level": "safe",
    "name_policy": "no_real_names",
    "intent_signal": "STATE_CHANGE"
  }}
]

Ï§ëÏöî:
- keyword_typeÏùÄ Î¨¥Ï°∞Í±¥ "trend"Îßå ÏÇ¨Ïö© (evergreen Í∏àÏßÄ)
- categoryÎäî **5Í∞ú Ïπ¥ÌÖåÍ≥†Î¶¨Îßå** ÏÇ¨Ïö©: "tech", "business", "society", "entertainment", "sports"
- **Ïπ¥ÌÖåÍ≥†Î¶¨ Î∂ÑÎ∞∞ ÎπÑÏú® (Ï§ëÏöî)**:
  * tech: 40% (Í∞ÄÏû• ÎÜíÏùÄ CPM, Ïö∞ÏÑ†ÏàúÏúÑ ÏµúÍ≥†)
  * business: 20%
  * society: 15%
  * sports: 15%
  * entertainment: 10%
- Tech Í¥ÄÎ†® ÌÇ§ÏõåÎìúÎäî ÏµúÎåÄÌïú ÎßéÏù¥ ÏÑ†ÌÉùÌï† Í≤É (AI, ML, cloud, programming, frameworks, devops Îì±)
  - ‚ö†Ô∏è "finance", "lifestyle", "education"ÏùÄ Îçî Ïù¥ÏÉÅ ÏÇ¨Ïö© Í∏àÏßÄ (Í∞ÅÍ∞Å business, society, techÎ°ú ÌÜµÌï©Îê®)
- languageÎäî "en", "ko", "ja" Ï§ë ÌïòÎÇò (ÎπÑÏú®: EN 40%, KO 40%, JA 20%)
- competition_levelÏùÄ "low", "medium", "high" Ï§ë ÌïòÎÇò
- priorityÎäî 1-10 ÏÇ¨Ïù¥Ïùò Ïà´Ïûê (ÎÜíÏùÑÏàòÎ°ù Ïö∞ÏÑ†ÏàúÏúÑ ÎÜíÏùå)
- risk_levelÏùÄ "safe", "caution", "high_risk" Ï§ë ÌïòÎÇò (Í∏∞Î≥∏Í∞í: "safe")
- name_policyÎäî "no_real_names", "generic_only" Ï§ë ÌïòÎÇò (Í∏∞Î≥∏Í∞í: "no_real_names")
- intent_signalÏùÄ "STATE_CHANGE", "PROMISE_BROKEN", "SILENCE", "DEADLINE_LOST", "COMPARISON" Ï§ë ÌïòÎÇò
- ÏßÄÍ∏à ÏãúÏ†ê(2026ÎÖÑ 1Ïõî)ÏóêÏÑú ÌòÑÏã§Ï†ÅÏù∏ ÌÇ§ÏõåÎìúÎßå Ï†úÏïà
- ÏòàÏãúÎäî Ï†àÎåÄ ÏÇ¨Ïö©ÌïòÏßÄ ÎßêÍ≥†, Ïã§Ï†ú Í≤ÄÏÉâ Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÏùÄ ÌÇ§ÏõåÎìúÎßå Ï†úÏïà
- **Ï§ëÏöî**: ÏúÑ Ïã§ÏãúÍ∞Ñ Ìä∏Î†åÎìú Îç∞Ïù¥ÌÑ∞Ïùò QueryÎ•º keyword ÌïÑÎìúÏóê Í∑∏ÎåÄÎ°ú Î≥µÏÇ¨Ìï† Í≤É
- **keyword ÌïÑÎìúÎäî Ï†àÎåÄ Ïû¨ÏûëÏÑ±ÌïòÏßÄ ÎßêÍ≥† QueryÎ•º Ï†ïÌôïÌûà Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©**
- **Ï§ëÏöî**: 5Í∞ú Ïπ¥ÌÖåÍ≥†Î¶¨(tech, business, society, entertainment, sports)Î•º Î∞òÎìúÏãú Í≥†Î•¥Í≤å Î∂ÑÎ∞∞Ìï† Í≤É

**üî¥ Ïπ¥ÌÖåÍ≥†Î¶¨ Î∂ÑÎ•ò Í∞ÄÏù¥Îìú (5Í∞ú Ïπ¥ÌÖåÍ≥†Î¶¨ - CRITICAL):**
- **tech**: Í∏∞Ïà†, IT, AI, Í≤åÏûÑ, Ïï±, ÏÜåÌîÑÌä∏Ïõ®Ïñ¥, **ÍµêÏú° Í∏∞Ïà†(EdTech), Ïò®ÎùºÏù∏ ÌïôÏäµ**
  - ÏòàÏãú: "AI", "ChatGPT", "iPhone", "Í≤åÏûÑ", "Ïò®ÎùºÏù∏ Í∞ïÏùò", "ÏΩîÎî© ÍµêÏú°"
  - ‚ö†Ô∏è Ïù¥Ï†Ñ "education" Ïπ¥ÌÖåÍ≥†Î¶¨ ÎÇ¥Ïö© Ìè¨Ìï®

- **business**: Í≤ΩÏ†ú, Í∏∞ÏóÖ, Ï£ºÏãù, Î∂ÄÎèôÏÇ∞, Ï∞ΩÏóÖ, **Í∏àÏúµ, Ìà¨Ïûê, ÏÑ∏Í∏à, Î≥¥Ìóò, Ïó∞Í∏à, ÏãúÏû•**
  - ÏòàÏãú: "ÌÖåÏä¨Îùº Ï£ºÍ∞Ä", "Î∂ÄÎèôÏÇ∞ ÏãúÏû•", "Ïä§ÌÉÄÌä∏ÏóÖ", "ÎπÑÌä∏ÏΩîÏù∏", "Í∏àÎ¶¨", "ÌôòÏú®"
  - ‚ö†Ô∏è Ïù¥Ï†Ñ "finance" Ïπ¥ÌÖåÍ≥†Î¶¨ ÎÇ¥Ïö© Ìè¨Ìï®

- **society**: ÏÇ¨Ìöå Ïù¥Ïäà, Ï†ïÏπò, Ï†ïÏ±Ö, Î≤îÏ£Ñ, Ïû¨ÎÇú, **Í±¥Í∞ï, Ïó¨Ìñâ, ÏùåÏãù, Ìå®ÏÖò, ÎùºÏù¥ÌîÑÏä§ÌÉÄÏùº**
  - ÏòàÏãú: "ÏßÄÏßÑÏÜçÎ≥¥", "Ï†ïÎ∂Ä Ï†ïÏ±Ö", "Îã§Ïù¥Ïñ¥Ìä∏", "Ïó¨ÌñâÏßÄ Ï∂îÏ≤ú", "Í±¥Í∞ï Í¥ÄÎ¶¨", "Ìä∏Î†åÎìú"
  - ‚ö†Ô∏è Ïù¥Ï†Ñ "lifestyle" Ïπ¥ÌÖåÍ≥†Î¶¨ ÎÇ¥Ïö© Ìè¨Ìï®
  - **Ï£ºÏùò**: Ïä§Ìè¨Ï∏† Í¥ÄÎ†®ÏùÄ Î¨¥Ï°∞Í±¥ sportsÎ°ú (ÏÇ¨Ìöå Ïù¥ÏäàÎùºÎèÑ)

- **entertainment**: ÏòÅÌôî, ÎìúÎùºÎßà, ÏùåÏïÖ, ÏòàÎä•, Ïó∞ÏòàÏù∏ (Îã®, Ïä§Ìè¨Ï∏† ÏÑ†ÏàòÎäî Ï†úÏô∏)
  - ÏòàÏãú: "ÎÑ∑ÌîåÎ¶≠Ïä§", "BTS", "Ïò§ÏßïÏñ¥Í≤åÏûÑ", "ÏòÅÌôî Î¶¨Î∑∞"

- **sports**: Î™®Îì† Ïö¥Îèô Í≤ΩÍ∏∞, ÏÑ†Ïàò, ÌåÄ (Ï∂ïÍµ¨, ÏïºÍµ¨, ÎÜçÍµ¨, UFC, eÏä§Ìè¨Ï∏†, U23 Îì± Ï†ÑÎ∂Ä)
  - ÏòàÏãú: "UFC", "u23", "ÏÜêÌù•ÎØº", "KBO", "NBA", "wimbledon"
  - **Ï§ëÏöî**: Í≤©Ìà¨Í∏∞, Ï≤≠ÏÜåÎÖÑ Ïä§Ìè¨Ï∏†ÎèÑ Î¨¥Ï°∞Í±¥ sports
- **education**: ÍµêÏú°, ÎåÄÌïô, ÏûÖÏãú, ÏûêÍ≤©Ï¶ù, ÌïôÏäµ

Ïñ∏Ïñ¥Î≥Ñ ÌÜ§ Ï∞®Ïù¥:
- üá∫üá∏ English: rights, compensation, legal leverage, lawsuits Ï§ëÏã¨
- üá∞üá∑ Korean: Î∂àÍ≥µÏ†ï, Ï¢åÏ†à, ÏÜåÎπÑÏûê Î≥¥Ìò∏, Ï±ÖÏûÑ Ï∂îÍ∂Å Ï§ëÏã¨
- üáØüáµ Japanese: Î∂àÌà¨Î™ÖÏÑ±, Í≥µÏãù Ï†àÏ∞®, Ï†ÅÏ†àÌïú ÎåÄÏùë Î∞©Î≤ï Ï§ëÏã¨

**üî¥ ÏïàÏ†Ñ Í∞ÄÏù¥ÎìúÎùºÏù∏:**

Ï£ºÏùòÏÇ¨Ìï≠:
- Î™ÖÏòàÌõºÏÜê/ÎπÑÎÇú/ÎπÑÎ∞© ÌëúÌòÑ Í∏àÏßÄ
- ÏÇ¨Ïã§ Í∏∞Î∞òÏùò trending ÌÇ§ÏõåÎìúÎäî Ïã§Î™Ö ÏÇ¨Ïö© Í∞ÄÎä•

Í∞Å ÌÇ§ÏõåÎìúÏóê Î¶¨Ïä§ÌÅ¨ Î†àÎ≤® ÌëúÏãú:
- "risk_level": "safe" (Í∏∞Î≥∏Í∞í)
- "risk_level": "caution" (ÎÖºÎûÄ Í∞ÄÎä•ÏÑ± ÏûàÏùå)

Í∞Å ÌÇ§ÏõåÎìúÏóê Ïã§Î™Ö Ï†ïÏ±Ö ÌëúÏãú:
- "name_policy": "no_real_names" (Ïã§Î™Ö Î∂àÌïÑÏöî)
- "name_policy": "real_names_ok" (trending Îâ¥Ïä§ Îì± Ïã§Î™Ö Ìè¨Ìï® Í∞ÄÎä•)

**Ï§ëÎ≥µ Î∞©ÏßÄ Í∑úÏπô:**
- Intent signals: STATE_CHANGE, PROMISE_BROKEN, SILENCE, DEADLINE_LOST, COMPARISON
- Í∞ôÏùÄ signalÏùÑ Í∞ÄÏßÑ ÌÇ§ÏõåÎìúÎäî Ïñ∏Ïñ¥Îãπ ÏµúÎåÄ 2Í∞úÍπåÏßÄÎßå
- 5Í∞ú signalÏùÑ Ïñ∏Ïñ¥Î≥ÑÎ°ú Í∑†Îì±ÌïòÍ≤å Î∂ÑÎ∞∞

**üö® Ïñ∏Ïñ¥Î≥Ñ ÌÇ§ÏõåÎìú ÏÉùÏÑ± Í∑úÏπô (Ï†àÎåÄ Ï§ÄÏàò):**
Î∞òÎìúÏãú Ï†ïÌôïÌûà {count}Í∞úÏùò ÌÇ§ÏõåÎìúÎ•º ÏÉùÏÑ±ÌïòÎùº:
- ÏòÅÏñ¥(en): Ï†ïÌôïÌûà {per_lang}Í∞ú (1Í∞úÎùºÎèÑ Î∂ÄÏ°±ÌïòÍ±∞ÎÇò Ï¥àÍ≥ºÌïòÎ©¥ Ïïà Îê®)
- ÌïúÍµ≠Ïñ¥(ko): Ï†ïÌôïÌûà {per_lang}Í∞ú (1Í∞úÎùºÎèÑ Î∂ÄÏ°±ÌïòÍ±∞ÎÇò Ï¥àÍ≥ºÌïòÎ©¥ Ïïà Îê®)
- ÏùºÎ≥∏Ïñ¥(ja): Ï†ïÌôïÌûà {per_lang}Í∞ú (1Í∞úÎùºÎèÑ Î∂ÄÏ°±ÌïòÍ±∞ÎÇò Ï¥àÍ≥ºÌïòÎ©¥ Ïïà Îê®)
- Ï¥ùÌï©: Ï†ïÌôïÌûà {count}Í∞ú

**Ïñ∏Ïñ¥Î≥Ñ Ìä∏Î†åÎìú Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö© Í∑úÏπô:**
- üá∫üá∏ English (US) TrendsÏóêÏÑú {per_lang}Í∞ú ÌÇ§ÏõåÎìú Ï∂îÏ∂ú ‚Üí language: "en"
- üá∞üá∑ Korean (KR) TrendsÏóêÏÑú {per_lang}Í∞ú ÌÇ§ÏõåÎìú Ï∂îÏ∂ú ‚Üí language: "ko"
- üáØüáµ Japanese (JP) TrendsÏóêÏÑú {per_lang}Í∞ú ÌÇ§ÏõåÎìú Ï∂îÏ∂ú ‚Üí language: "ja"
- ÎßåÏïΩ Ìïú Ïñ∏Ïñ¥Ïùò Ìä∏Î†åÎìúÍ∞Ä Î∂ÄÏ°±ÌïòÎ©¥, Îã§Î•∏ Ïñ∏Ïñ¥ Ìä∏Î†åÎìúÎ•º Ï†àÎåÄ ÏÇ¨Ïö©ÌïòÏßÄ ÎßêÍ≥† Ìï¥Îãπ Ïñ∏Ïñ¥Î°ú ÏÉàÎ°úÏö¥ ÌÇ§ÏõåÎìúÎ•º ÏÉùÏÑ±ÌïòÎùº

Í∞Å Ïñ∏Ïñ¥ ÎÇ¥ÏóêÏÑú 5Í∞ú Ïπ¥ÌÖåÍ≥†Î¶¨(tech, business, society, entertainment, sports)Î•º ÏµúÎåÄÌïú Í∑†Îì±ÌïòÍ≤å Î∂ÑÎ∞∞ÌïòÎêò,
Î∞òÎìúÏãú Í∞Å Ïñ∏Ïñ¥Î≥ÑÎ°ú Ï†ïÌôïÌûà {per_lang}Í∞úÏî© ÏÉùÏÑ±ÌïòÎäî Í≤ÉÏù¥ ÏµúÏö∞ÏÑ†Ïù¥Îã§.

‚ö†Ô∏è **Ïπ¥ÌÖåÍ≥†Î¶¨ Î≥ÄÍ≤Ω ÏÇ¨Ìï≠ (2026-01-25):**
- Í∏∞Ï°¥ 8Í∞ú ‚Üí ÏÉàÎ°úÏö¥ 5Í∞ú Ïπ¥ÌÖåÍ≥†Î¶¨Î°ú ÌÜµÌï©
- "education" ‚Üí "tech"Î°ú ÌÜµÌï©
- "finance" ‚Üí "business"Î°ú ÌÜµÌï©
- "lifestyle" ‚Üí "society"Î°ú ÌÜµÌï©"""


CURATION_PROMPT_EVERGREEN = """Ïó≠Ìï†:
ÎÑàÎäî **Ïû•Í∏∞ Ìä∏ÎûòÌîΩ** ÌôïÎ≥¥Î•º ÏúÑÌïú Evergreen ÌÇ§ÏõåÎìú ÌÅêÎ†àÏù¥ÌÑ∞Îã§.
ÏïÑÎûò Evergreen ÌÇ§ÏõåÎìú ÌíÄÏóêÏÑú **Í≤ÄÏÉâÎüâÏù¥ ÏßÄÏÜçÎêòÎäî, ÍµêÏú°/Í∞ÄÏù¥ÎìúÏÑ±** ÌÇ§ÏõåÎìúÎ•º Ï†úÏïàÌïòÎùº.

Evergreen ÌÇ§ÏõåÎìú ÌíÄ (Ïñ∏Ïñ¥Î≥ÑÎ°ú Íµ¨Î∂ÑÎê®):

üá∫üá∏ English Keywords:
{evergreen_en}

üá∞üá∑ Korean Keywords:
{evergreen_ko}

üáØüáµ Japanese Keywords:
{evergreen_ja}

**Î™©Ìëú:**
- **ÏßÄÏÜçÏ†Å Í≤ÄÏÉâ ÏàòÏöî**: 1ÎÖÑ ÌõÑÏóêÎèÑ Í≤ÄÏÉâÎêòÎäî Ï£ºÏ†ú
- **ÍµêÏú°/Í∞ÄÏù¥ÎìúÏÑ±**: "how to", "guide", "Î∞©Î≤ï", "Í∞ÄÏù¥Îìú" Îì±
- **ÎÇÆÏùÄ Í≤ΩÏüÅ**: low~medium competition ÏúÑÏ£º
- **Ïã§Ïö©Ï†Å Í∞ÄÏπò**: ÎèÖÏûêÏóêÍ≤å Ïã§ÏßàÏ†Å ÎèÑÏõÄÏù¥ ÎêòÎäî ÎÇ¥Ïö©

**Í∏àÏßÄ:**
- ÏãúÏÇ¨ÏÑ± ÌÜ†ÌîΩ (ÏÜçÎ≥¥, ÏÇ¨Í±¥ ÏÇ¨Í≥†)
- Ïã§Î™Ö Ïù∏Î¨º Í¥ÄÎ†® (Ïó∞ÏòàÏù∏, Ï†ïÏπòÏù∏)
- ÎÖºÎûÄ/Í∞êÏ†ï ÏûêÍ∑πÌòï ÌÇ§ÏõåÎìú
- Ï∂îÏÉÅÏ†Å Ï£ºÏ†ú ("AIÏùò ÎØ∏Îûò", "Í∏∞Ïà† Ìä∏Î†åÎìú")

Ï∂úÎ†• ÌòïÏãù:
Î∞òÎìúÏãú JSON ÌòïÏãùÏúºÎ°úÎßå ÏùëÎãµÌïòÎùº.

[
  {{
    "keyword": "ÏúÑ Evergreen ÌÇ§ÏõåÎìú ÌíÄÏóêÏÑú ÏÑ†ÌÉùÌïú ÌÇ§ÏõåÎìú (ÎòêÎäî Ïú†ÏÇ¨ Î≥ÄÌòï)",
    "raw_search_title": "ÏÇ¨Ïö©ÏûêÍ∞Ä Íµ¨Í∏ÄÏóê Í≤ÄÏÉâÌï† Îïå Ï†ïÌôïÌûà ÏûÖÎ†•ÌïòÎäî Í≤ÄÏÉâÏñ¥",
    "editorial_title": "Í∏∞ÏÇ¨ Ï†úÎ™© ÌòïÏãùÏùò ÎèÖÏûê ÏπúÌôîÏ†Å Ï†úÎ™©",
    "core_question": "ÏÇ¨Ïö©ÏûêÍ∞Ä Ìï¥Í≤∞ÌïòÍ≥† Ïã∂ÏùÄ ÌïµÏã¨ ÏßàÎ¨∏ (ÍµêÏú°Ï†Å)",
    "language": "ko",
    "category": "tech",
    "search_intent": "ÏÇ¨Ïö©ÏûêÍ∞Ä Ïù¥ ÌÇ§ÏõåÎìúÎ•º Í≤ÄÏÉâÌïòÎäî Ïã§ÏßàÏ†Å Ïù¥Ïú† (ÌïôÏäµ, Î¨∏Ï†ú Ìï¥Í≤∞, ÏùòÏÇ¨ Í≤∞Ï†ï Îì±)",
    "angle": "Ïù¥ ÌÇ§ÏõåÎìúÎ•º Îã§Î£∞ ÎïåÏùò Í¥ÄÏ†ê (ÍµêÏú°, ÎπÑÍµê, Í∞ÄÏù¥Îìú Îì±)",
    "competition_level": "low",
    "why_evergreen": "Ïù¥ ÌÇ§ÏõåÎìúÍ∞Ä Ïû•Í∏∞Í∞Ñ Í≤ÄÏÉâÎê† Ïù¥Ïú† (ÏßÄÏÜçÏ†Å ÏàòÏöî Í∑ºÍ±∞)",
    "keyword_type": "evergreen",
    "priority": 6,
    "risk_level": "safe",
    "name_policy": "no_real_names",
    "content_depth": "comprehensive"
  }}
]

Ï§ëÏöî:
- keyword_typeÏùÄ Î¨¥Ï°∞Í±¥ "evergreen"Îßå ÏÇ¨Ïö© (trend Í∏àÏßÄ)
- categoryÎäî **5Í∞ú Ïπ¥ÌÖåÍ≥†Î¶¨Îßå** ÏÇ¨Ïö©: "tech", "business", "society", "entertainment", "sports"
- languageÎäî "en", "ko", "ja" Ï§ë ÌïòÎÇò (ÎπÑÏú®: EN 40%, KO 40%, JA 20%)
- competition_levelÏùÄ "low", "medium"Îßå ÏÇ¨Ïö© (high Í∏àÏßÄ - Í≤ΩÏüÅ ÌîºÌï† Í≤É)
- priorityÎäî 5-8 ÏÇ¨Ïù¥ (EvergreenÏùÄ Ìä∏Î†åÎìúÎ≥¥Îã§ ÎÇÆÏùÄ Ïö∞ÏÑ†ÏàúÏúÑ)
- risk_levelÏùÄ Î¨¥Ï°∞Í±¥ "safe" (EvergreenÏùÄ ÏïàÏ†ÑÌï¥Ïïº Ìï®)
- content_depthÎäî "comprehensive" (ÏÉÅÏÑ∏Ìïú Í∞ÄÏù¥Îìú ÌòïÏãù)

**üî¥ Ïπ¥ÌÖåÍ≥†Î¶¨Î≥Ñ Evergreen ÌÇ§ÏõåÎìú ÏòàÏãú:**
- **tech**: "ÌîÑÎ°úÍ∑∏ÎûòÎ∞ç ÎèÖÌïô Î∞©Î≤ï", "web development roadmap", "AI tools for beginners"
- **business**: "passive income ideas", "Î∂ÄÏóÖ ÏïÑÏù¥ÎîîÏñ¥", "startup funding guide"
- **society**: "mental health management", "Í±¥Í∞ïÌïú ÏÉùÌôú ÏäµÍ¥Ä", "climate change solutions"
- **entertainment**: "Netflix recommendations", "ÏùåÏïÖ Ï†úÏûë ÏûÖÎ¨∏", "photography tips"
- **sports**: "home workout routine", "ÎßàÎùºÌÜ§ ÌõàÎ†® Í≥ÑÌöç", "yoga for beginners"

**üö® Ïñ∏Ïñ¥Î≥Ñ ÌÇ§ÏõåÎìú ÏÉùÏÑ± Í∑úÏπô:**
Î∞òÎìúÏãú Ï†ïÌôïÌûà {count}Í∞úÏùò ÌÇ§ÏõåÎìúÎ•º ÏÉùÏÑ±ÌïòÎùº:
- ÏòÅÏñ¥(en): Ï†ïÌôïÌûà {per_lang}Í∞ú
- ÌïúÍµ≠Ïñ¥(ko): Ï†ïÌôïÌûà {per_lang}Í∞ú
- ÏùºÎ≥∏Ïñ¥(ja): Ï†ïÌôïÌûà {per_lang}Í∞ú
- Ï¥ùÌï©: Ï†ïÌôïÌûà {count}Í∞ú

Í∞Å Ïñ∏Ïñ¥ ÎÇ¥ÏóêÏÑú 5Í∞ú Ïπ¥ÌÖåÍ≥†Î¶¨Î•º Í∑†Îì±ÌïòÍ≤å Î∂ÑÎ∞∞Ìï† Í≤É.
"""


class KeywordCurator:
    def __init__(self, api_key: str = None, google_api_key: str = None, google_cx: str = None):
        """Initialize keyword curator with Claude API and Google Custom Search"""
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            safe_print("‚ùå ERROR: ANTHROPIC_API_KEY not found")
            safe_print("   Please set it as environment variable")
            safe_print("   Example: export ANTHROPIC_API_KEY='your-key-here'")
            raise ValueError("ANTHROPIC_API_KEY not found")

        # Brave Search API (replacing Google Custom Search)
        self.brave_api_key = os.environ.get("BRAVE_API_KEY")

        # Keep Google API keys for backward compatibility (deprecated)
        self.google_api_key = google_api_key or os.environ.get("GOOGLE_API_KEY")
        self.google_cx = google_cx or os.environ.get("GOOGLE_CX")

        if not self.brave_api_key:
            safe_print("‚ö†Ô∏è  Brave Search API key not found")
            safe_print("   Set BRAVE_API_KEY environment variable")
            safe_print("   Falling back to Claude-only mode")
            if self.google_api_key and self.google_cx:
                safe_print("   Note: Google Custom Search API is deprecated for new users")

        try:
            self.client = Anthropic(api_key=self.api_key)
            self.model = "claude-sonnet-4-5-20250929"
            safe_print("  ‚úì Anthropic API client initialized successfully")
        except Exception as e:
            safe_print(f"‚ùå ERROR: Failed to initialize Anthropic client")
            safe_print(f"   Error: {mask_secrets(str(e))}")
            raise

        # Load existing queue
        self.queue_path = Path("data/topics_queue.json")
        try:
            self.queue_data = self._load_queue()
            safe_print(f"  ‚úì Loaded topic queue: {len(self.queue_data.get('topics', []))} topics")
        except Exception as e:
            safe_print(f"‚ö†Ô∏è  WARNING: Failed to load existing queue, starting fresh")
            safe_print(f"   Error: {str(e)}")
            self.queue_data = {"topics": []}

    def _load_queue(self) -> Dict:
        """Load existing topic queue"""
        if not self.queue_path.exists():
            return {"topics": []}

        with open(self.queue_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _save_queue(self):
        """Save updated topic queue"""
        try:
            # Ensure parent directory exists
            self.queue_path.parent.mkdir(parents=True, exist_ok=True)

            with open(self.queue_path, 'w', encoding='utf-8') as f:
                json.dump(self.queue_data, f, indent=2, ensure_ascii=False)
        except IOError as e:
            safe_print(f"‚ùå ERROR: Failed to save queue to filesystem")
            safe_print(f"   Path: {self.queue_path}")
            safe_print(f"   Error: {str(e)}")
            raise
        except Exception as e:
            safe_print(f"‚ùå ERROR: Unexpected error saving queue")
            safe_print(f"   Error: {str(e)}")
            raise

    def detect_intent_signals(self, query: str) -> list:
        """Detect intent signals from query for deduplication"""
        signals = []

        # State transition patterns
        if any(word in query.lower() for word in ["after", "Í∞ëÏûêÍ∏∞", "suddenly", "Á™ÅÁÑ∂", "overnight"]):
            signals.append("STATE_CHANGE")

        # Promise broken patterns
        if any(word in query.lower() for word in ["promised", "supposed to", "ÏïΩÏÜç", "Áô∫Ë°®", "denied", "Í±∞Î∂Ä", "ÊãíÂê¶"]):
            signals.append("PROMISE_BROKEN")

        # Silence patterns
        if any(word in query.lower() for word in ["no response", "ignored", "Ë™¨Êòé„Å™„Åó", "Î¨¥ÏùëÎãµ", "Ïπ®Î¨µ"]):
            signals.append("SILENCE")

        # Deadline/time loss patterns
        if any(word in query.lower() for word in ["deadline", "too late", "ÎßàÍ∞ê", "ÊúüÈôê", "ÎÜìÏπ®", "ÈÄÉ„Åó"]):
            signals.append("DEADLINE_LOST")

        # Comparison/injustice patterns
        if any(word in query.lower() for word in ["others got", "only me", "ÎÇòÎßå", "Ëá™ÂàÜ„Å†„Åë"]):
            signals.append("COMPARISON")

        return signals if signals else ["GENERAL"]

    def fetch_community_topics(self) -> Dict[str, List[Dict]]:
        """Fetch trending topics from HackerNews, Reddit, and ProductHunt"""
        safe_print(f"\n{'='*60}")
        safe_print(f"  üåê Fetching topics from community sources...")
        safe_print(f"{'='*60}\n")

        community_topics = []
        verify_ssl = certifi.where() if certifi else True

        # 1. HackerNews - Top Stories + Top Comments (free, no auth)
        try:
            safe_print("  ‚Üí Fetching from HackerNews (with top comments)...")
            hn_top_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            response = requests.get(hn_top_url, timeout=10, verify=verify_ssl)
            response.raise_for_status()
            story_ids = response.json()[:10]  # Top 10 stories

            for story_id in story_ids[:5]:  # Fetch details for top 5
                try:
                    item_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                    item_resp = requests.get(item_url, timeout=5, verify=verify_ssl)
                    item_resp.raise_for_status()
                    item = item_resp.json()

                    if item and item.get('title'):
                        # Fetch top 3 comments for additional context
                        top_comments = []
                        comment_ids = item.get('kids', [])[:3]  # Top 3 comment IDs
                        for comment_id in comment_ids:
                            try:
                                comment_url = f"https://hacker-news.firebaseio.com/v0/item/{comment_id}.json"
                                comment_resp = requests.get(comment_url, timeout=3, verify=verify_ssl)
                                comment_resp.raise_for_status()
                                comment = comment_resp.json()
                                if comment and comment.get('text'):
                                    # Strip HTML tags and limit length
                                    import re
                                    clean_text = re.sub('<[^<]+?>', '', comment.get('text', ''))
                                    if len(clean_text) > 500:
                                        clean_text = clean_text[:500] + '...'
                                    top_comments.append(clean_text)
                            except Exception:
                                continue

                        community_topics.append({
                            'title': item['title'],
                            'source': 'HackerNews',
                            'url': item.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                            'score': item.get('score', 0),
                            'comments': item.get('descendants', 0),
                            'top_comments': top_comments  # NEW: Developer insights
                        })
                except Exception:
                    continue

            hn_count = len([t for t in community_topics if t['source'] == 'HackerNews'])
            safe_print(f"    ‚úì Found {hn_count} topics from HackerNews (with comments)")

        except Exception as e:
            safe_print(f"    ‚ö†Ô∏è HackerNews fetch failed: {mask_secrets(str(e))}")

        # 2. Dev.to - Developer community (free, no auth needed)
        try:
            safe_print("  ‚Üí Fetching from Dev.to (top articles)...")
            devto_url = "https://dev.to/api/articles?top=1&per_page=5"
            headers = {'User-Agent': 'JakesTechInsights/1.0'}
            response = requests.get(devto_url, headers=headers, timeout=10, verify=verify_ssl)
            response.raise_for_status()
            data = response.json()

            for article in data[:5]:
                if article.get('title'):
                    community_topics.append({
                        'title': article['title'],
                        'source': 'Dev.to',
                        'url': article.get('url', ''),
                        'score': article.get('positive_reactions_count', 0),
                        'comments': article.get('comments_count', 0)
                    })

            devto_count = len([t for t in community_topics if t['source'] == 'Dev.to'])
            safe_print(f"    ‚úì Found {devto_count} topics from Dev.to")

        except Exception as e:
            safe_print(f"    ‚ö†Ô∏è Dev.to fetch failed: {mask_secrets(str(e))}")

        # 3a. Lobsters - Tech community (free, no auth needed)
        try:
            safe_print("  ‚Üí Fetching from Lobsters (hottest)...")
            lobsters_url = "https://lobste.rs/hottest.json"
            response = requests.get(lobsters_url, headers={'User-Agent': 'JakesTechInsights/1.0'}, timeout=10, verify=verify_ssl)
            response.raise_for_status()
            data = response.json()

            for article in data[:5]:
                if article.get('title'):
                    community_topics.append({
                        'title': article['title'],
                        'source': 'Lobsters',
                        'url': article.get('url') or article.get('short_id_url', ''),
                        'score': article.get('score', 0),
                        'comments': article.get('comment_count', 0)
                    })

            lobsters_count = len([t for t in community_topics if t['source'] == 'Lobsters'])
            safe_print(f"    ‚úì Found {lobsters_count} topics from Lobsters")

        except Exception as e:
            safe_print(f"    ‚ö†Ô∏è Lobsters fetch failed: {mask_secrets(str(e))}")

        # 3. ProductHunt - Using Atom feed with descriptions (no auth needed)
        try:
            safe_print("  ‚Üí Fetching from ProductHunt (with descriptions)...")
            import xml.etree.ElementTree as ET
            ph_feed_url = "https://www.producthunt.com/feed"
            response = requests.get(ph_feed_url, timeout=10, verify=verify_ssl)
            response.raise_for_status()

            root = ET.fromstring(response.content)
            # ProductHunt uses Atom format, not RSS - need to use namespace
            atom_ns = {'atom': 'http://www.w3.org/2005/Atom'}
            entries = root.findall('atom:entry', atom_ns)

            for entry in entries[:5]:
                title_elem = entry.find('atom:title', atom_ns)
                link_elem = entry.find('atom:link', atom_ns)
                content_elem = entry.find('atom:content', atom_ns)  # Atom uses 'content' not 'description'

                if title_elem is not None and title_elem.text:
                    # Extract URL from link element's href attribute
                    url = ''
                    if link_elem is not None:
                        url = link_elem.get('href', '')

                    # Extract and clean description from content
                    description = ''
                    if content_elem is not None and content_elem.text:
                        import re
                        description = re.sub('<[^<]+?>', '', content_elem.text)  # Strip HTML
                        description = ' '.join(description.split())  # Normalize whitespace
                        if len(description) > 500:
                            description = description[:500] + '...'

                    community_topics.append({
                        'title': title_elem.text.strip(),
                        'source': 'ProductHunt',
                        'url': url,
                        'score': 0,
                        'comments': 0,
                        'description': description  # Product details
                    })

            ph_count = len([t for t in community_topics if t['source'] == 'ProductHunt'])
            safe_print(f"    ‚úì Found {ph_count} topics from ProductHunt (with descriptions)")

        except Exception as e:
            safe_print(f"    ‚ö†Ô∏è ProductHunt fetch failed: {mask_secrets(str(e))}")

        safe_print(f"\n  üéâ Total {len(community_topics)} community topics fetched!\n")

        return {'en': community_topics}  # All community sources are English

    def fetch_trending_from_rss(self) -> Dict[str, List[str]]:
        """Fetch trending topics from Google Trends RSS feeds grouped by language"""
        import xml.etree.ElementTree as ET

        rss_urls = {
            "KR": "https://trends.google.co.kr/trending/rss?geo=KR",
            "US": "https://trends.google.co.kr/trending/rss?geo=US",
            "JP": "https://trends.google.co.kr/trending/rss?geo=JP"
        }

        # Map region to language
        region_to_lang = {
            "KR": "ko",
            "US": "en",
            "JP": "ja"
        }

        # Group trends by language
        trends_by_lang = {"ko": [], "en": [], "ja": []}

        for geo, url in rss_urls.items():
            try:
                verify_ssl = certifi.where() if certifi else True
                response = requests.get(url, timeout=10, verify=verify_ssl)
                response.raise_for_status()

                # Parse XML
                root = ET.fromstring(response.content)

                # Find all items (trending topics)
                items = root.findall('.//item')

                lang = region_to_lang[geo]
                for item in items[:5]:  # Top 5 per region (15 total)
                    title_elem = item.find('title')
                    if title_elem is not None and title_elem.text:
                        trends_by_lang[lang].append(title_elem.text.strip())

                safe_print(f"  ‚úì Found {min(len(items), 5)} trends from {geo} ‚Üí {lang}")

            except requests.exceptions.Timeout:
                safe_print(f"  ‚ö†Ô∏è  RSS fetch timeout for {geo}: Request took too long")
                continue
            except requests.exceptions.HTTPError as e:
                safe_print(f"  ‚ö†Ô∏è  RSS HTTP error for {geo}: {e.response.status_code if e.response else 'unknown'}")
                continue
            except ET.ParseError as e:
                safe_print(f"  ‚ö†Ô∏è  RSS parse error for {geo}: Invalid XML format")
                safe_print(f"     Error: {str(e)}")
                continue
            except Exception as e:
                safe_print(f"  ‚ö†Ô∏è  RSS fetch error for {geo}: {mask_secrets(str(e))}")
                continue

        return trends_by_lang

    def fetch_trending_topics(self) -> Dict[str, str]:
        """Fetch trending topics using Google Trends RSS feeds, grouped by language"""
        safe_print(f"\n{'='*60}")
        safe_print(f"  üî• Fetching REAL-TIME trending topics from Google Trends RSS...")
        safe_print(f"{'='*60}\n")

        # Try RSS feeds first (most reliable method)
        trends_by_lang = self.fetch_trending_from_rss()

        # Check if we got any trends
        total_trends = sum(len(trends) for trends in trends_by_lang.values())

        if total_trends > 0:
            safe_print(f"\n  üéâ Total {total_trends} real-time trending topics from RSS!")
            safe_print(f"     EN: {len(trends_by_lang['en'])}, KO: {len(trends_by_lang['ko'])}, JA: {len(trends_by_lang['ja'])}\n")
        else:
            safe_print("  ‚ö†Ô∏è  RSS feeds failed. Falling back to pattern-based queries...\n")
            # Fallback to pattern queries (grouped by language)
            trends_by_lang = {
                "en": [
                    "account banned after update no response",
                    "service outage promised compensation denied",
                    "class action deadline passed too late",
                    "refund promised but denied suddenly",
                    "government support supposed to but denied",
                    "new policy suddenly stricter than announced",
                    "celebrity apology issued but backlash continues"
                ],
                "ko": [
                    "Ïï± ÏóÖÎç∞Ïù¥Ìä∏ ÌõÑ Í∞ëÏûêÍ∏∞ Î®πÌÜµ",
                    "ÏßëÎã®ÏÜåÏÜ° Ïã†Ï≤≠ ÎßàÍ∞ê ÎÜìÏπ®",
                    "Ï†ïÎ∂ÄÏßÄÏõê Ï°∞Í±¥ Î∞úÌëúÏôÄ Îã§Î¶Ñ",
                    "ÏÇ¨Í≥ºÎ¨∏ ÎÉàÏßÄÎßå ÎÖºÎûÄ Í≥ÑÏÜç",
                    "Î¶¨ÏΩú Î∞úÌëúÌñàÎäîÎç∞ ÌôòÎ∂à Í±∞Î∂Ä"
                ],
                "ja": [
                    "„Ç¢„Ç´„Ç¶„É≥„ÉàÂÅúÊ≠¢ ÁêÜÁî±Ë™¨Êòé„Å™„Åó",
                    "ËøîÈáëÁ¥ÑÊùü„Åó„Åü„Åå ÊãíÂê¶„Åï„Çå„Åü",
                    "ÊîøÂ∫úÊîØÊè¥ Á™ÅÁÑ∂ Êù°‰ª∂Âé≥„Åó„Åè",
                    "Ë¨ùÁΩ™ÊñáÂá∫„Åó„Åü„Åå ÁÇé‰∏äÁ∂ö„Åè",
                    "„É™„Ç≥„Éº„É´Áô∫Ë°® ËøîÈáëÂØæÂøú„Å™„Åó"
                ]
            }

        # Flatten for search queries (but keep language tracking)
        all_queries = []
        for lang, queries in trends_by_lang.items():
            for query in queries:
                all_queries.append((query, lang))

        # If no Brave Search API, skip search results
        if not self.brave_api_key:
            safe_print("  üö® CRITICAL WARNING: Brave Search API not configured")
            safe_print("  üìå References will NOT be generated for keywords!")
            safe_print("  üìå Set BRAVE_API_KEY environment variable")
            safe_print("  üìå OR: Add it as GitHub Secret for automated workflows\n")
            self.search_results = []

            # Format trends by language for prompt
            trends_formatted = {}
            for lang, queries in trends_by_lang.items():
                trends_formatted[lang] = "\n".join([f"Query: {q}" for q in queries[:10]])

            return trends_formatted

        all_results = []
        for query, query_lang in all_queries:
            try:
                # Brave Search API endpoint
                url = "https://api.search.brave.com/res/v1/web/search"
                headers = {
                    "Accept": "application/json",
                    "X-Subscription-Token": self.brave_api_key
                }
                params = {
                    "q": query,
                    "count": 3,  # Get top 3 results per query for better quality
                    "freshness": "pw"  # Past week (ÏµúÏã† Îâ¥Ïä§)
                }

                # Add delay to avoid rate limiting
                time.sleep(0.5)

                verify_ssl = certifi.where() if certifi else True
                response = requests.get(url, headers=headers, params=params, verify=verify_ssl)
                response.raise_for_status()

                data = response.json()

                # Brave API returns results in "web" -> "results" structure
                web_results = data.get("web", {}).get("results", [])

                if web_results:
                    # Detect intent signals for this query
                    signals = self.detect_intent_signals(query)

                    for item in web_results:
                        all_results.append({
                            "query": query,
                            "query_lang": query_lang,  # Track which language this query belongs to
                            "signals": signals,  # Add intent signals
                            "title": item.get("title", ""),
                            "snippet": item.get("description", ""),  # Brave uses "description" not "snippet"
                            "link": item.get("url", ""),  # Brave uses "url" not "link"
                            "source": item.get("url", "").split("/")[2] if item.get("url") else ""  # Extract domain
                        })

                safe_print(f"  ‚úì Fetched {len(web_results)} results for: {query}")

            except requests.exceptions.Timeout:
                safe_print(f"  ‚ö†Ô∏è  Timeout fetching results for '{query[:50]}...'")
                continue
            except requests.exceptions.HTTPError as e:
                status_code = e.response.status_code if e.response else 'unknown'
                safe_print(f"  ‚ö†Ô∏è  HTTP error ({status_code}) for '{query[:50]}...'")
                if status_code == 403:
                    safe_print(f"     ‚ö†Ô∏è  Brave API Access Forbidden - check API key")
                elif status_code == 429:
                    safe_print(f"     Rate limit exceeded (2000/month limit)")
                continue
            except json.JSONDecodeError:
                safe_print(f"  ‚ö†Ô∏è  Invalid JSON response for '{query[:50]}...'")
                continue
            except requests.exceptions.RequestException as e:
                safe_print(f"  ‚ö†Ô∏è  Network error for '{query[:50]}...': {mask_secrets(str(e))}")
                continue
            except Exception as e:
                safe_print(f"  ‚ö†Ô∏è  Unexpected error for '{query[:50]}...': {mask_secrets(str(e))}")
                continue

        safe_print(f"\n‚úÖ Total {len(all_results)} trending topics fetched\n")

        # Store results for reference extraction
        self.search_results = all_results

        # Format results for Claude, grouped by language
        trends_by_lang_formatted = {"en": [], "ko": [], "ja": []}
        for r in all_results:
            lang = r.get('query_lang', 'en')
            trends_by_lang_formatted[lang].append(
                f"Query: {r['query']}\nTitle: {r['title']}\nSnippet: {r['snippet']}\n"
            )

        # Convert to string format per language
        trends_formatted = {}
        for lang in ["en", "ko", "ja"]:
            trends_formatted[lang] = "\n\n".join(trends_by_lang_formatted[lang][:10])  # Top 10 per language

        return trends_formatted

    def filter_by_risk(self, candidates: List[Dict]) -> List[Dict]:
        """Filter out high-risk keywords automatically"""
        safe_candidates = []
        filtered_count = 0

        for kw in candidates:
            # Auto-reject high-risk
            if kw.get("risk_level") == "high_risk":
                filtered_count += 1
                safe_print(f"  üî¥ Filtered high-risk: {kw.get('keyword', 'unknown')}")
                continue

            # Flag caution items for manual review
            if kw.get("risk_level") == "caution":
                kw["needs_review"] = True
                safe_print(f"  üü° Caution flagged: {kw.get('keyword', 'unknown')}")

            safe_candidates.append(kw)

        if filtered_count > 0:
            safe_print(f"\n‚ö†Ô∏è  {filtered_count} high-risk keywords filtered out\n")

        return safe_candidates

    def fetch_evergreen_references(self, keyword: str, lang: str) -> List[Dict]:
        """Fetch references for evergreen keywords on-demand using Brave Search"""
        if not self.brave_api_key:
            safe_print(f"  ‚ö†Ô∏è  No Brave API key - skipping references for: {keyword}")
            return []

        try:
            url = "https://api.search.brave.com/res/v1/web/search"
            headers = {
                "Accept": "application/json",
                "X-Subscription-Token": self.brave_api_key
            }
            params = {
                "q": keyword,
                "count": 3,
                "freshness": "py"  # Past year (for evergreen content)
            }

            time.sleep(0.5)  # Rate limiting

            verify_ssl = certifi.where() if certifi else True
            response = requests.get(url, headers=headers, params=params, verify=verify_ssl)
            response.raise_for_status()

            data = response.json()
            web_results = data.get("web", {}).get("results", [])

            references = []
            seen_domains = set()

            for item in web_results:
                link = item.get("url", "")
                title = item.get("title", "")
                source = link.split("/")[2] if link else ""

                if link and source and source not in seen_domains:
                    references.append({
                        "title": title[:100],
                        "url": link,
                        "source": source
                    })
                    seen_domains.add(source)

                if len(references) >= 3:
                    break

            return references

        except Exception as e:
            safe_print(f"  ‚ö†Ô∏è  Failed to fetch references for '{keyword}': {mask_secrets(str(e))}")
            return []

    def extract_references(self, all_results: List[Dict], keyword: str, lang: str) -> List[Dict]:
        """Extract top 3 references for a keyword based on search results"""
        # Find relevant results for this keyword
        # Match by language and keyword similarity
        relevant = []

        for result in all_results:
            query = result.get("query", "").lower()
            # Simple matching: if keyword words appear in query
            keyword_words = set(keyword.lower().split())
            query_words = set(query.split())

            # Check language match (simple heuristic)
            is_relevant = len(keyword_words & query_words) > 0

            if is_relevant:
                relevant.append(result)

        # Take top 3 unique sources
        references = []
        seen_domains = set()

        for result in relevant[:10]:  # Check first 10 relevant results
            link = result.get("link", "")
            source = result.get("source", "")
            title = result.get("title", "")

            if link and source and source not in seen_domains:
                references.append({
                    "title": title[:100],  # Truncate long titles
                    "url": link,
                    "source": source
                })
                seen_domains.add(source)

            if len(references) >= 3:  # Get 3 references per keyword for AdSense quality
                break

        return references

    def load_evergreen_keywords(self) -> Dict[str, Dict[str, List[str]]]:
        """Load evergreen keywords from JSON file"""
        evergreen_path = Path("data/evergreen_keywords.json")
        if not evergreen_path.exists():
            safe_print("‚ö†Ô∏è  Evergreen keywords file not found, using empty pool")
            return {"tech": {"en": [], "ko": [], "ja": []}, "business": {"en": [], "ko": [], "ja": []}}

        with open(evergreen_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def generate_candidates(self, count: int = 15, keyword_type: str = "trend") -> List[Dict]:
        """Generate keyword candidates using Claude API

        Args:
            count: Number of keywords to generate
            keyword_type: "trend" or "evergreen"
        """
        safe_print(f"\n{'='*60}")
        safe_print(f"  üîç Generating {count} {keyword_type} keyword candidates...")
        safe_print(f"{'='*60}\n")

        # Calculate per-language count
        per_lang = count // 3  # Distribute evenly across 3 languages

        if keyword_type == "evergreen":
            # Load evergreen keywords pool
            evergreen_pool = self.load_evergreen_keywords()

            # Format evergreen keywords for prompt
            evergreen_en = "\n".join([f"- {kw}" for cat in evergreen_pool.values() for kw in cat.get("en", [])])
            evergreen_ko = "\n".join([f"- {kw}" for cat in evergreen_pool.values() for kw in cat.get("ko", [])])
            evergreen_ja = "\n".join([f"- {kw}" for cat in evergreen_pool.values() for kw in cat.get("ja", [])])

            # Generate prompt with evergreen data
            prompt = CURATION_PROMPT_EVERGREEN.format(
                evergreen_en=evergreen_en,
                evergreen_ko=evergreen_ko,
                evergreen_ja=evergreen_ja,
                count=count,
                per_lang=per_lang
            )

            # For evergreen, we don't need real-time search results
            self.search_results = []

        else:  # trend
            # Fetch trending topics from Google (store for reference extraction)
            self.search_results = []  # Store search results
            trends_by_lang = self.fetch_trending_topics()

            # Fetch community topics (HackerNews, Dev.to, Lobsters, ProductHunt)
            community_data = self.fetch_community_topics()
            community_topics_list = community_data.get('en', [])

            # Format community topics for prompt (with additional context)
            def format_community_topic(t):
                base = f"- [{t['source']}] {t['title']} (score: {t['score']}, comments: {t['comments']})\n  URL: {t['url']}"

                # Add HackerNews top comments if available
                if t.get('top_comments'):
                    comments_text = "\n  üí¨ Top developer comments:\n"
                    for i, comment in enumerate(t['top_comments'][:2], 1):  # Max 2 comments
                        comments_text += f"    {i}. {comment[:200]}...\n" if len(comment) > 200 else f"    {i}. {comment}\n"
                    base += comments_text

                # Add ProductHunt description if available
                if t.get('description'):
                    desc = t['description'][:300] + '...' if len(t['description']) > 300 else t['description']
                    base += f"\n  üìù Description: {desc}"

                return base

            community_topics_formatted = "\n".join([
                format_community_topic(t)
                for t in community_topics_list[:15]  # Top 15 community topics
            ]) or "No community topics available"

            # Calculate language-specific counts (EN 40%, KO 40%, JA 20%)
            en_count = int(count * 0.4)
            ko_count = int(count * 0.4)
            ja_count = count - en_count - ko_count  # Remainder goes to JA

            # Generate prompt with trending data (grouped by language)
            prompt = CURATION_PROMPT_WITH_TRENDS.format(
                trends_en=trends_by_lang.get('en', 'No English trends available'),
                trends_ko=trends_by_lang.get('ko', 'No Korean trends available'),
                trends_ja=trends_by_lang.get('ja', 'No Japanese trends available'),
                community_topics=community_topics_formatted,
                count=count,
                per_lang=per_lang,
                en_count=en_count,
                ko_count=ko_count,
                ja_count=ja_count
            )

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=16000,  # Increased for 30+ keywords
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )
        except Exception as e:
            safe_print(f"‚ùå ERROR: Claude API call failed")
            safe_print(f"   Error: {mask_secrets(str(e))}")
            safe_print(f"   This is a critical error - cannot continue without keyword candidates")
            sys.exit(1)

        if not response or not response.content:
            safe_print(f"‚ùå ERROR: Empty response from Claude API")
            safe_print(f"   This is a critical error - cannot continue without keyword candidates")
            sys.exit(1)

        # Parse JSON response
        content = response.content[0].text.strip()

        # Extract JSON from markdown code blocks if present
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        try:
            candidates = json.loads(content)
        except json.JSONDecodeError as e:
            safe_print(f"‚ùå ERROR: Failed to parse JSON response from Claude")
            safe_print(f"   Parse error: {str(e)}")
            safe_print(f"   Raw response (first 500 chars):\n{content[:500]}")
            safe_print(f"   This is a critical error - cannot continue with invalid JSON")
            sys.exit(1)

        safe_print(f"‚úÖ Generated {len(candidates)} candidates\n")

        # STEP 1: Remove duplicates (keep first occurrence, regardless of category)
        seen_keywords = {}
        dedup_candidates = []
        duplicates_removed = 0

        for candidate in candidates:
            keyword_lower = candidate.get('keyword', '').lower()
            if keyword_lower in seen_keywords:
                duplicates_removed += 1
                first_category = seen_keywords[keyword_lower]
                duplicate_category = candidate.get('category')
                safe_print(f"  üî¥ DUPLICATE REMOVED: '{candidate.get('keyword')}' (duplicate category: {duplicate_category}, already exists as: {first_category})")
            else:
                # Store the category of the first occurrence
                seen_keywords[keyword_lower] = candidate.get('category')
                dedup_candidates.append(candidate)

        if duplicates_removed > 0:
            safe_print(f"\n‚ö†Ô∏è  Removed {duplicates_removed} duplicate keywords from Claude's response")
            safe_print(f"    Policy: One keyword = one category (first occurrence wins)\n")

        # STEP 2: Auto-correct sports keywords category
        sports_keywords = ['vs', 'vs.', 'game', 'match', 'league', 'cup', 'tournament', 'championship',
                          'basketball', 'football', 'soccer', 'baseball', 'hockey', 'tennis', 'golf',
                          'nba', 'nfl', 'mlb', 'nhl', 'premier league', 'uefa', 'champions league',
                          'world cup', 'olympics', 'ufc', 'boxing', 'wrestling', 'mma',
                          'u23', 'u-23', 'u21', 'u-21', 'player', 'team', 'squad']

        corrected_count = 0
        for candidate in dedup_candidates:
            keyword_lower = candidate.get('keyword', '').lower()
            category = candidate.get('category', '')

            # Auto-detect sports keywords
            if category != 'sports':
                is_sports = any(sport_term in keyword_lower for sport_term in sports_keywords)
                if is_sports:
                    old_category = category
                    candidate['category'] = 'sports'
                    corrected_count += 1
                    safe_print(f"  ‚úÖ AUTO-CORRECTED: {candidate.get('keyword')} ({old_category} ‚Üí sports)")

        if corrected_count > 0:
            safe_print(f"\n‚úÖ Auto-corrected {corrected_count} sports keywords\n")

        # Apply risk filtering
        filtered_candidates = self.filter_by_risk(dedup_candidates)

        # Extract references for each candidate
        safe_print(f"üìö Extracting references for {len(filtered_candidates)} candidates...\n")
        keywords_with_refs = 0
        keywords_without_refs = 0

        for candidate in filtered_candidates:
            keyword = candidate.get("keyword", "")
            lang = candidate.get("language", "en")
            kw_type = candidate.get("keyword_type", "trend")

            # For evergreen keywords, fetch references on-demand
            if kw_type == "evergreen" and not self.search_results:
                references = self.fetch_evergreen_references(keyword, lang)
            else:
                references = self.extract_references(self.search_results, keyword, lang)

            candidate["references"] = references
            if references:
                safe_print(f"  ‚úì {len(references)} refs for: {keyword[:50]}...")
                keywords_with_refs += 1
            else:
                keywords_without_refs += 1

        safe_print("")

        # Validation warning
        if keywords_without_refs > 0:
            safe_print(f"‚ö†Ô∏è  WARNING: {keywords_without_refs}/{len(filtered_candidates)} keywords have NO references")
            safe_print(f"   This means generated posts will lack credible sources!")
            if not self.google_api_key or not self.google_cx:
                safe_print(f"   ROOT CAUSE: Google Custom Search API credentials not configured")
                safe_print(f"   FIX: Set GOOGLE_API_KEY and GOOGLE_CX environment variables\n")
        else:
            safe_print(f"‚úÖ All {keywords_with_refs} keywords have references!\n")

        return filtered_candidates

    def display_candidates(self, candidates: List[Dict]):
        """Display candidates with numbered list"""
        safe_print(f"{'='*60}")
        safe_print(f"  üìã Keyword Candidates")
        safe_print(f"{'='*60}\n")

        # Group by language
        by_lang = {"en": [], "ko": [], "ja": []}
        for c in candidates:
            lang = c.get("language", "en")
            by_lang[lang].append(c)

        idx = 1
        lang_names = {"en": "English", "ko": "Korean", "ja": "Japanese"}

        for lang in ["en", "ko", "ja"]:
            if by_lang[lang]:
                safe_print(f"\n[{lang_names[lang]}]")
                safe_print("-" * 60)

                for candidate in by_lang[lang]:
                    type_emoji = "üî•" if candidate.get("keyword_type") == "trend" else "üå≤"
                    comp_emoji = {
                        "low": "üü¢",
                        "medium": "üü°",
                        "high": "üî¥"
                    }.get(candidate.get("competition_level", "medium"), "‚ö™")

                    safe_print(f"\n{idx}. {type_emoji} {candidate['keyword']}")
                    safe_print(f"   Category: {candidate['category']} | Competition: {comp_emoji} {candidate.get('competition_level', 'N/A')}")
                    safe_print(f"   Intent: {candidate['search_intent']}")
                    safe_print(f"   Angle: {candidate['angle']}")
                    safe_print(f"   Why: {candidate.get('why_it_works', 'N/A')[:80]}...")

                    idx += 1

        safe_print(f"\n{'='*60}\n")

    def interactive_selection(self, candidates: List[Dict]) -> List[Dict]:
        """Interactive selection of keywords"""
        safe_print("Ïñ¥Îñ§ ÌÇ§ÏõåÎìúÎ•º ÌÅêÏóê Ï∂îÍ∞ÄÌï†ÍπåÏöî?")
        safe_print("Ïà´ÏûêÎ•º ÏâºÌëúÎ°ú Íµ¨Î∂ÑÌï¥ÏÑú ÏûÖÎ†•ÌïòÏÑ∏Ïöî (Ïòà: 1,3,5,7,10)")
        safe_print("ÎòêÎäî 'all'ÏùÑ ÏûÖÎ†•ÌïòÎ©¥ Ï†ÑÎ∂Ä Ï∂îÍ∞ÄÎê©ÎãàÎã§.")
        safe_print("'q'Î•º ÏûÖÎ†•ÌïòÎ©¥ Ï∑®ÏÜåÌï©ÎãàÎã§.\n")

        while True:
            user_input = input("ÏÑ†ÌÉù: ").strip()

            if user_input.lower() == 'q':
                safe_print("‚ùå Ï∑®ÏÜåÎêòÏóàÏäµÎãàÎã§.")
                return []

            if user_input.lower() == 'all':
                return candidates

            try:
                # Parse selected indices
                selected_indices = [int(x.strip()) for x in user_input.split(',')]

                # Validate indices
                if any(idx < 1 or idx > len(candidates) for idx in selected_indices):
                    safe_print(f"‚ö†Ô∏è  ÏûòÎ™ªÎêú Î≤àÌò∏ÏûÖÎãàÎã§. 1-{len(candidates)} Î≤îÏúÑÎ°ú ÏûÖÎ†•ÌïòÏÑ∏Ïöî.\n")
                    continue

                # Convert to 0-based index and return selected candidates
                selected = [candidates[idx - 1] for idx in selected_indices]
                return selected

            except ValueError:
                safe_print("‚ö†Ô∏è  ÏûòÎ™ªÎêú ÌòïÏãùÏûÖÎãàÎã§. Ïòà: 1,3,5\n")

    def _validate_keyword_language(self, keyword: str, language: str) -> bool:
        """Validate that keyword matches the specified language"""
        import unicodedata

        def has_hangul(text):
            """Check if text contains Korean characters"""
            return any('\uac00' <= char <= '\ud7a3' for char in text)

        def has_hiragana_katakana(text):
            """Check if text contains Japanese characters"""
            return any(
                ('\u3040' <= char <= '\u309f') or  # Hiragana
                ('\u30a0' <= char <= '\u30ff')     # Katakana
                for char in text
            )

        def has_kanji_only(text):
            """Check if text contains only Kanji/Chinese characters (could be Japanese)"""
            return any('\u4e00' <= char <= '\u9fff' for char in text)

        def has_vietnamese_chars(text):
            """Check if text contains Vietnamese diacritics"""
            vietnamese_chars = ['ƒë', 'ƒÉ', '√¢', '√™', '√¥', '∆°', '∆∞', '√°', '√†', '·∫£', '√£', '·∫°',
                               '·∫Ø', '·∫±', '·∫≥', '·∫µ', '·∫∑', '·∫•', '·∫ß', '·∫©', '·∫´', '·∫≠',
                               '√©', '√®', '·∫ª', '·∫Ω', '·∫π', '·∫ø', '·ªÅ', '·ªÉ', '·ªÖ', '·ªá',
                               '√≠', '√¨', '·ªâ', 'ƒ©', '·ªã', '√≥', '√≤', '·ªè', '√µ', '·ªç',
                               '·ªë', '·ªì', '·ªï', '·ªó', '·ªô', '·ªõ', '·ªù', '·ªü', '·ª°', '·ª£',
                               '√∫', '√π', '·ªß', '≈©', '·ª•', '·ª©', '·ª´', '·ª≠', '·ªØ', '·ª±',
                               '√Ω', '·ª≥', '·ª∑', '·ªπ', '·ªµ']
            return any(char in text.lower() for char in vietnamese_chars)

        def has_spanish_only_chars(text):
            """Check if text contains Spanish-only characters (√±, √°, √©, √≠, √≥, √∫, √º, ¬ø, ¬°)"""
            # Check for Spanish question/exclamation marks
            if '¬ø' in text or '¬°' in text:
                return True
            # Check for √±
            if '√±' in text.lower():
                return True
            return False

        # Validation rules
        if language == 'ko':
            # Korean must have Hangul
            if not has_hangul(keyword):
                return False
            # Korean cannot have Japanese characters
            if has_hiragana_katakana(keyword) or (has_kanji_only(keyword) and not has_hangul(keyword)):
                return False
            # Korean cannot have Vietnamese/Spanish
            if has_vietnamese_chars(keyword) or has_spanish_only_chars(keyword):
                return False
        elif language == 'ja':
            # Japanese must have Hiragana/Katakana or Kanji
            if not (has_hiragana_katakana(keyword) or has_kanji_only(keyword)):
                return False
            # Japanese cannot have Korean
            if has_hangul(keyword):
                return False
            # Japanese cannot have Vietnamese/Spanish
            if has_vietnamese_chars(keyword) or has_spanish_only_chars(keyword):
                return False
        elif language == 'en':
            # English cannot have Korean/Japanese
            if has_hangul(keyword) or has_hiragana_katakana(keyword):
                return False
            # English cannot have Vietnamese (common in trends)
            if has_vietnamese_chars(keyword):
                return False
            # English cannot have Spanish-only markers
            if has_spanish_only_chars(keyword):
                return False

        return True

    def add_to_queue(self, selected: List[Dict]):
        """Add selected keywords to topic queue with language and duplicate validation"""
        if not selected:
            safe_print("ÏÑ†ÌÉùÎêú ÌÇ§ÏõåÎìúÍ∞Ä ÏóÜÏäµÎãàÎã§.")
            return

        safe_print(f"\n{'='*60}")
        safe_print(f"  üíæ ÌÅêÏóê {len(selected)}Í∞ú ÌÇ§ÏõåÎìú Ï∂îÍ∞Ä Ï§ë...")
        safe_print(f"{'='*60}\n")

        # Get existing keywords for duplicate check (case-insensitive)
        existing_keywords = {t['keyword'].lower() for t in self.queue_data['topics']}

        # Get next ID
        existing_ids = [int(t['id'].split('-')[0]) for t in self.queue_data['topics'] if t['id'].split('-')[0].isdigit()]
        next_id = max(existing_ids) + 1 if existing_ids else 1

        added_count = 0
        rejected_count = 0
        for candidate in selected:
            # Validate keyword-language match
            keyword = candidate.get('keyword', '')
            language = candidate.get('language', 'en')

            # Check for duplicate keyword
            if keyword.lower() in existing_keywords:
                safe_print(f"  üî¥ REJECTED: Duplicate keyword")
                safe_print(f"     Keyword: {keyword}")
                safe_print(f"     Reason: Keyword already exists in queue")
                rejected_count += 1
                continue

            if not self._validate_keyword_language(keyword, language):
                safe_print(f"  üî¥ REJECTED: Keyword-language mismatch")
                safe_print(f"     Keyword: {keyword}")
                safe_print(f"     Language: {language}")
                safe_print(f"     Reason: Keyword contains characters from different language")
                rejected_count += 1
                continue
            # Generate topic ID
            topic_id = f"{next_id:03d}-{candidate['language']}-{candidate['category']}-{candidate['keyword'][:20].replace(' ', '-')}"

            # Create topic entry
            topic = {
                "id": topic_id,
                "keyword": candidate['keyword'],
                "category": candidate['category'],
                "lang": candidate['language'],
                "priority": candidate.get('priority', 7),
                "status": "pending",
                "created_at": datetime.now().isoformat(),
                "retry_count": 0,
                "keyword_type": candidate.get('keyword_type', 'evergreen'),
                "search_intent": candidate.get('search_intent', ''),
                "angle": candidate.get('angle', ''),
                "competition_level": candidate.get('competition_level', 'medium'),
                "references": candidate.get('references', [])
            }

            # Add expiry_days for trend keywords
            if topic['keyword_type'] == 'trend':
                topic['expiry_days'] = 3  # 3 days expiry for trending keywords
                # Add content_type hint for trends: prefer BREAKING
                topic['content_type_hint'] = 'BREAKING'
            elif topic['keyword_type'] == 'evergreen':
                # Add content_type hint for evergreen: prefer GUIDE
                topic['content_type_hint'] = 'GUIDE'
            else:
                # Mixed or undefined: let generate_posts.py decide
                topic['content_type_hint'] = 'MIXED'

            self.queue_data['topics'].append(topic)

            type_label = "üî• Trend" if topic['keyword_type'] == 'trend' else "üå≤ Evergreen"
            safe_print(f"  ‚úì Added: {type_label} | {candidate['keyword']}")

            added_count += 1
            next_id += 1

        # Save queue
        self._save_queue()

        safe_print(f"\n‚úÖ {added_count}Í∞ú ÌÇ§ÏõåÎìúÍ∞Ä ÌÅêÏóê Ï∂îÍ∞ÄÎêòÏóàÏäµÎãàÎã§!")
        if rejected_count > 0:
            safe_print(f"üî¥ {rejected_count}Í∞ú ÌÇ§ÏõåÎìúÍ∞Ä Ïñ∏Ïñ¥ Î∂àÏùºÏπòÎ°ú Í±∞Î∂ÄÎêòÏóàÏäµÎãàÎã§!")
        safe_print(f"üìä Total topics in queue: {len(self.queue_data['topics'])}")

        # Show statistics
        self._show_queue_stats()

    def _show_queue_stats(self):
        """Show queue statistics"""
        topics = self.queue_data['topics']

        # Count by status (safe for any status value)
        by_status = {"pending": 0, "in_progress": 0, "completed": 0, "failed": 0}
        for t in topics:
            status = t.get('status', 'pending')
            by_status[status] = by_status.get(status, 0) + 1

        # Count by type
        by_type = {"trend": 0, "evergreen": 0, "unknown": 0}
        for t in topics:
            ktype = t.get('keyword_type', 'unknown')
            by_type[ktype] = by_type.get(ktype, 0) + 1

        # Count by language
        by_lang = {"en": 0, "ko": 0, "ja": 0}
        for t in topics:
            lang = t.get('lang', 'en')
            by_lang[lang] = by_lang.get(lang, 0) + 1

        safe_print(f"\n{'='*60}")
        safe_print(f"  üìä Queue Statistics")
        safe_print(f"{'='*60}")
        safe_print(f"  Status: Pending={by_status['pending']}, In Progress={by_status['in_progress']}, Completed={by_status['completed']}")
        safe_print(f"  Type: üî• Trend={by_type['trend']}, üå≤ Evergreen={by_type['evergreen']}, Unknown={by_type['unknown']}")
        safe_print(f"  Language: EN={by_lang['en']}, KO={by_lang['ko']}, JA={by_lang['ja']}")
        safe_print(f"{'='*60}\n")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Keyword Curator for blog content")
    parser.add_argument('--count', type=int, default=15, help="Number of candidates to generate (default: 15)")
    parser.add_argument('--auto', action='store_true', help="Automatically add all candidates without interactive selection")
    parser.add_argument('--type', choices=['trend', 'evergreen', 'mixed'], default='trend',
                       help="Keyword type: trend (default), evergreen, or mixed")
    parser.add_argument('--evergreen-ratio', type=float, default=0.6,
                       help="Ratio of evergreen keywords in mixed mode (default: 0.6)")
    args = parser.parse_args()

    # Check API key
    if not os.environ.get("ANTHROPIC_API_KEY"):
        safe_print("Error: ANTHROPIC_API_KEY environment variable not set")
        sys.exit(1)

    # Initialize curator
    curator = KeywordCurator()

    # Generate candidates based on type
    if args.type == 'mixed':
        # Mixed mode: generate both trend and evergreen
        evergreen_count = int(args.count * args.evergreen_ratio)
        trend_count = args.count - evergreen_count

        safe_print(f"\nüìä Mixed Mode: {trend_count} trend + {evergreen_count} evergreen keywords\n")

        # Generate trend keywords
        if trend_count > 0:
            trend_candidates = curator.generate_candidates(count=trend_count, keyword_type="trend")
        else:
            trend_candidates = []

        # Generate evergreen keywords
        if evergreen_count > 0:
            evergreen_candidates = curator.generate_candidates(count=evergreen_count, keyword_type="evergreen")
        else:
            evergreen_candidates = []

        # Combine candidates
        candidates = trend_candidates + evergreen_candidates

    else:
        # Single type mode
        candidates = curator.generate_candidates(count=args.count, keyword_type=args.type)

    # Display candidates
    curator.display_candidates(candidates)

    # Selection
    if args.auto:
        # Auto mode: add all candidates
        safe_print("\nü§ñ Auto mode: Adding all candidates to queue...\n")
        selected = candidates
    else:
        # Interactive mode: ask user
        selected = curator.interactive_selection(candidates)

    # Add to queue
    if selected:
        curator.add_to_queue(selected)

    safe_print("\n‚ú® Done!\n")


if __name__ == "__main__":
    main()
