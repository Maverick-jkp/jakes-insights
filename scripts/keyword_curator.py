#!/usr/bin/env python3
"""
Keyword Curator - Semi-automated keyword research for blog content

Generates keyword candidates using Claude API based on KEYWORD_STRATEGY.md
Provides interactive selection interface for human filtering (5 minutes weekly)

Usage:
    python scripts/keyword_curator.py
    python scripts/keyword_curator.py --count 15
"""

import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import requests

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

try:
    import certifi
except ImportError:
    safe_print("Warning: certifi not installed - SSL verification may fail")
    certifi = None

# Add utils to path
sys.path.insert(0, str(Path(__file__).parent))
from utils.security import safe_print, mask_secrets

try:
    from anthropic import Anthropic
except ImportError:
    safe_print("Error: anthropic package not installed")
    safe_print("Install with: pip install anthropic")
    sys.exit(1)


CURATION_PROMPT_WITH_TRENDS = """ì—­í• :
ë„ˆëŠ” ê´‘ê³  ìˆ˜ìµ ìµœì í™”ë¥¼ ìœ„í•œ í‚¤ì›Œë“œ íë ˆì´í„°ë‹¤.
ì•„ë˜ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ê²€ìƒ‰ ê²°ê³¼ì™€ ì»¤ë®¤ë‹ˆí‹° í† í”½ì„ ë°”íƒ•ìœ¼ë¡œ **ê³ CPC, ê°ì • ë°˜ì‘í˜•** í‚¤ì›Œë“œë¥¼ ì œì•ˆí•˜ë¼.

ğŸ“Š **ì†ŒìŠ¤ ë¹„ì¤‘**: Google Trends 40% + Community 40% + Evergreen 20%
ğŸ“Š **ì–¸ì–´ ë¹„ì¤‘**: EN 50% ({en_count}ê°œ), KO 50% ({ko_count}ê°œ)

ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë°ì´í„° (ì–¸ì–´ë³„ë¡œ êµ¬ë¶„ë¨):

ğŸ‡ºğŸ‡¸ English (US) Trends:
{trends_en}

ğŸ‡°ğŸ‡· Korean (KR) Trends:
{trends_ko}

ğŸŒ **Community Topics (HackerNews, Dev.to, Lobsters, ProductHunt)**:
{community_topics}

**ì¤‘ìš”**: Community TopicsëŠ” ì˜ì–´ ì›ë¬¸ì´ì§€ë§Œ, í•œêµ­ ë…ìì—ê²Œë„ ìœ ìš©í•œ ë‚´ìš©ì´ë©´ KO ë²„ì „ìœ¼ë¡œë„ ì œì•ˆí•˜ë¼.
ì˜ˆ: "OpenAI releases new model" â†’ EN ì›ë¬¸ + KO "OpenAI ìƒˆ ëª¨ë¸ ì¶œì‹œ"

**ğŸ”´ ì¤‘ìš” ê·œì¹™: ì–¸ì–´-í‚¤ì›Œë“œ ë§¤ì¹­ (CRITICAL - ìœ„ë°˜ ì‹œ ì¦‰ì‹œ ê±°ë¶€)**
1. English (US) íŠ¸ë Œë“œì˜ Query â†’ language: "en"ìœ¼ë¡œë§Œ ì‚¬ìš©
2. Korean (KR) íŠ¸ë Œë“œì˜ Query â†’ language: "ko"ë¡œë§Œ ì‚¬ìš©
3. ìœ„ íŠ¸ë Œë“œ ë°ì´í„°ì˜ Queryë¥¼ ê·¸ëŒ€ë¡œ keywordë¡œ ì‚¬ìš©í•˜ë¼. ì ˆëŒ€ ì¬í•´ì„í•˜ê±°ë‚˜ ì¬ì‘ì„±í•˜ì§€ ë§ ê²ƒ.

**ğŸš¨ ì–¸ì–´ ë¬¸ì ê²€ì¦ ê·œì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜):**
- **ì˜ì–´(en) í‚¤ì›Œë“œ**: í•œê¸€(ê°€-í£) í¬í•¨ ê¸ˆì§€
  - ì˜¬ë°”ë¥¸ ì˜ˆ: "NBA", "Kobe Bryant", "quad cortex"
  - ì˜ëª»ëœ ì˜ˆ: "ë¶‰ì€ì‚¬ë§‰" (í•œê¸€ í¬í•¨)
- **í•œêµ­ì–´(ko) í‚¤ì›Œë“œ**: ë°˜ë“œì‹œ í•œê¸€(ê°€-í£) í¬í•¨ í•„ìš”
  - ì˜¬ë°”ë¥¸ ì˜ˆ: "ë¶‰ì€ì‚¬ë§‰", "ê¹€ì—°ì•„", "u23" (ì˜ë¬¸ ì•½ì–´ëŠ” í—ˆìš©)
  - ì˜ëª»ëœ ì˜ˆ: "red desert" (í•œê¸€ ì—†ìŒ)

ëª©í‘œ:
í•œêµ­ì–´ / ì˜ì–´ ê°ê°ì—ì„œ
**ë¶ˆì•ˆ, ë¶„ë…¸, ê¶ê¸ˆì¦**ì„ ìœ ë°œí•˜ëŠ” í‚¤ì›Œë“œë§Œ ì œì•ˆí•˜ë¼.

ê¸ˆì§€:
- ì¶”ìƒì ì¸ íŠ¸ë Œë“œ ìš”ì•½ ("AI íŠ¸ë Œë“œ", "ìƒˆë¡œìš´ ê¸°ìˆ ")
- êµìœ¡/ì •ë³´ì„± í‚¤ì›Œë“œ ("~í•˜ëŠ” ë°©ë²•", "~ë€ ë¬´ì—‡ì¸ê°€")
- ê¸ì •ì ì´ê³  í‰í™”ë¡œìš´ í‚¤ì›Œë“œ
- **Queryë¥¼ ì¬í•´ì„í•˜ê±°ë‚˜ ë‹¤ì‹œ ì“°ëŠ” ê²ƒ**
- **ê°™ì€ í‚¤ì›Œë“œë¥¼ ë‹¤ë¥¸ ì¹´í…Œê³ ë¦¬ë¡œ ì¤‘ë³µ ì œì•ˆí•˜ëŠ” ê²ƒ**

ì¶œë ¥ í˜•ì‹:
ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë¼.

[
  {{
    "keyword": "ìœ„ íŠ¸ë Œë“œ ë°ì´í„°ì˜ Queryë¥¼ ê·¸ëŒ€ë¡œ ë³µì‚¬ (ì¬í•´ì„ ê¸ˆì§€)",
    "raw_search_title": "ì‚¬ìš©ìê°€ êµ¬ê¸€ì— ê²€ìƒ‰í•  ë•Œ ì •í™•íˆ ì…ë ¥í•˜ëŠ” ê²€ìƒ‰ì–´ (keywordì™€ ë™ì¼í•˜ê²Œ)",
    "editorial_title": "ê¸°ì‚¬ ì œëª© í˜•ì‹ì˜ ë…ì ì¹œí™”ì  ì œëª©",
    "core_fear_question": "ì‚¬ìš©ìì˜ í•µì‹¬ ë‘ë ¤ì›€ì„ ë‹´ì€ ì§ˆë¬¸ í•œ ë¬¸ì¥",
    "language": "ko",
    "category": "tech",
    "search_intent": "ì‚¬ìš©ìê°€ ì§€ê¸ˆ ë‹¹ì¥ ê²€ìƒ‰í•˜ëŠ” ì´ìœ  (í–‰ë™í•˜ì§€ ì•Šìœ¼ë©´ ë¬´ì—‡ì„ ìƒëŠ”ì§€)",
    "angle": "ì´ í‚¤ì›Œë“œë¥¼ ë‹¤ë£° ë•Œì˜ ê´€ì ",
    "competition_level": "low",
    "why_it_works": "ì‚¬ìš©ìê°€ ì§€ê¸ˆ í–‰ë™í•˜ì§€ ì•Šìœ¼ë©´ ì˜êµ¬ì ìœ¼ë¡œ ë¬´ì—‡ì„ ìƒëŠ”ì§€ (ë§ˆê°/ê¸°íšŒ ì†ì‹¤ ì¤‘ì‹¬)",
    "purpose": "high competitionì¸ ê²½ìš°ì—ë§Œ: Traffic acquisition / Brand positioning / Viral content ì¤‘ í•˜ë‚˜",
    "keyword_type": "trend",
    "priority": 7,
    "risk_level": "safe",
    "name_policy": "no_real_names",
    "intent_signal": "STATE_CHANGE"
  }}
]

ì¤‘ìš”:
- keyword_typeì€ "trend"ë§Œ ì‚¬ìš© (ì´ í”„ë¡¬í”„íŠ¸ëŠ” íŠ¸ë Œë“œ ì „ìš©)
- categoryëŠ” **5ê°œ ì¹´í…Œê³ ë¦¬ë§Œ** ì‚¬ìš©: "tech", "business", "society", "entertainment", "sports"
- **ì¹´í…Œê³ ë¦¬ ë¶„ë°° ë¹„ìœ¨ (ì¤‘ìš”)**:
  * tech: 40% (ê°€ì¥ ë†’ì€ CPM, ìš°ì„ ìˆœìœ„ ìµœê³ )
  * business: 20%
  * society: 15%
  * sports: 15%
  * entertainment: 10%
- Tech ê´€ë ¨ í‚¤ì›Œë“œëŠ” ìµœëŒ€í•œ ë§ì´ ì„ íƒí•  ê²ƒ (AI, ML, cloud, programming, frameworks, devops ë“±)
- languageëŠ” "en", "ko" ì¤‘ í•˜ë‚˜ (ë¹„ìœ¨: EN 50%, KO 50%)
- competition_levelì€ "low", "medium", "high" ì¤‘ í•˜ë‚˜
- priorityëŠ” 1-10 ì‚¬ì´ì˜ ìˆ«ì (ë†’ì„ìˆ˜ë¡ ìš°ì„ ìˆœìœ„ ë†’ìŒ)
- risk_levelì€ "safe", "caution", "high_risk" ì¤‘ í•˜ë‚˜ (ê¸°ë³¸ê°’: "safe")
- name_policyëŠ” "no_real_names", "generic_only" ì¤‘ í•˜ë‚˜ (ê¸°ë³¸ê°’: "no_real_names")
- intent_signalì€ "STATE_CHANGE", "PROMISE_BROKEN", "SILENCE", "DEADLINE_LOST", "COMPARISON" ì¤‘ í•˜ë‚˜
- **ì¤‘ìš”**: ìœ„ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë°ì´í„°ì˜ Queryë¥¼ keyword í•„ë“œì— ê·¸ëŒ€ë¡œ ë³µì‚¬í•  ê²ƒ
- **keyword í•„ë“œëŠ” ì ˆëŒ€ ì¬ì‘ì„±í•˜ì§€ ë§ê³  Queryë¥¼ ì •í™•íˆ ê·¸ëŒ€ë¡œ ì‚¬ìš©**
- **ì¤‘ìš”**: 5ê°œ ì¹´í…Œê³ ë¦¬(tech, business, society, entertainment, sports)ë¥¼ ë°˜ë“œì‹œ ê³ ë¥´ê²Œ ë¶„ë°°í•  ê²ƒ

**ğŸ”´ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ê°€ì´ë“œ (5ê°œ ì¹´í…Œê³ ë¦¬):**
- **tech**: ê¸°ìˆ , IT, AI, ê²Œì„, ì•±, ì†Œí”„íŠ¸ì›¨ì–´, êµìœ¡ ê¸°ìˆ (EdTech)
- **business**: ê²½ì œ, ê¸°ì—…, ì£¼ì‹, ë¶€ë™ì‚°, ì°½ì—…, ê¸ˆìœµ, íˆ¬ì
- **society**: ì‚¬íšŒ ì´ìŠˆ, ì •ì¹˜, ì •ì±…, ê±´ê°•, ì—¬í–‰, ë¼ì´í”„ìŠ¤íƒ€ì¼
- **entertainment**: ì˜í™”, ë“œë¼ë§ˆ, ìŒì•…, ì˜ˆëŠ¥, ì—°ì˜ˆì¸ (ìŠ¤í¬ì¸  ì„ ìˆ˜ ì œì™¸)
- **sports**: ëª¨ë“  ìš´ë™ ê²½ê¸°, ì„ ìˆ˜, íŒ€

ì–¸ì–´ë³„ í†¤ ì°¨ì´:
- ğŸ‡ºğŸ‡¸ English: rights, compensation, legal leverage, lawsuits ì¤‘ì‹¬
- ğŸ‡°ğŸ‡· Korean: ë¶ˆê³µì •, ì¢Œì ˆ, ì†Œë¹„ì ë³´í˜¸, ì±…ì„ ì¶”ê¶ ì¤‘ì‹¬

**ğŸ”´ ì•ˆì „ ê°€ì´ë“œë¼ì¸:**
- ëª…ì˜ˆí›¼ì†/ë¹„ë‚œ/ë¹„ë°© í‘œí˜„ ê¸ˆì§€
- ì‚¬ì‹¤ ê¸°ë°˜ì˜ trending í‚¤ì›Œë“œëŠ” ì‹¤ëª… ì‚¬ìš© ê°€ëŠ¥

**ì¤‘ë³µ ë°©ì§€ ê·œì¹™:**
- Intent signals: STATE_CHANGE, PROMISE_BROKEN, SILENCE, DEADLINE_LOST, COMPARISON
- ê°™ì€ signalì„ ê°€ì§„ í‚¤ì›Œë“œëŠ” ì–¸ì–´ë‹¹ ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ

**ğŸš¨ ì–¸ì–´ë³„ í‚¤ì›Œë“œ ìƒì„± ê·œì¹™ (ì ˆëŒ€ ì¤€ìˆ˜):**
ë°˜ë“œì‹œ ì •í™•íˆ {count}ê°œì˜ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ë¼:
- ì˜ì–´(en): ì •í™•íˆ {en_count}ê°œ
- í•œêµ­ì–´(ko): ì •í™•íˆ {ko_count}ê°œ
- ì´í•©: ì •í™•íˆ {count}ê°œ

**ì–¸ì–´ë³„ íŠ¸ë Œë“œ ë°ì´í„° ì‚¬ìš© ê·œì¹™:**
- ğŸ‡ºğŸ‡¸ English (US) Trendsì—ì„œ {en_count}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ â†’ language: "en"
- ğŸ‡°ğŸ‡· Korean (KR) Trendsì—ì„œ {ko_count}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ â†’ language: "ko"

ê° ì–¸ì–´ ë‚´ì—ì„œ 5ê°œ ì¹´í…Œê³ ë¦¬ë¥¼ ìµœëŒ€í•œ ê· ë“±í•˜ê²Œ ë¶„ë°°í•  ê²ƒ."""


CURATION_PROMPT_EVERGREEN = """ì—­í• :
ë„ˆëŠ” **ì¥ê¸° íŠ¸ë˜í”½** í™•ë³´ë¥¼ ìœ„í•œ Evergreen í‚¤ì›Œë“œ íë ˆì´í„°ë‹¤.
ì•„ë˜ Evergreen í‚¤ì›Œë“œ í’€ì—ì„œ **ê²€ìƒ‰ëŸ‰ì´ ì§€ì†ë˜ëŠ”, êµìœ¡/ê°€ì´ë“œì„±** í‚¤ì›Œë“œë¥¼ ì œì•ˆí•˜ë¼.

Evergreen í‚¤ì›Œë“œ í’€ (ì–¸ì–´ë³„ë¡œ êµ¬ë¶„ë¨):

ğŸ‡ºğŸ‡¸ English Keywords:
{evergreen_en}

ğŸ‡°ğŸ‡· Korean Keywords:
{evergreen_ko}

**ğŸš« ì´ë¯¸ íì— ì¡´ì¬í•˜ëŠ” í‚¤ì›Œë“œ (ì ˆëŒ€ ì¤‘ë³µ ì œì•ˆ ê¸ˆì§€):**
{existing_keywords}

**ëª©í‘œ:**
- **ì§€ì†ì  ê²€ìƒ‰ ìˆ˜ìš”**: 1ë…„ í›„ì—ë„ ê²€ìƒ‰ë˜ëŠ” ì£¼ì œ
- **êµìœ¡/ê°€ì´ë“œì„±**: "how to", "guide", "ë°©ë²•", "ê°€ì´ë“œ" ë“±
- **ë‚®ì€ ê²½ìŸ**: low~medium competition ìœ„ì£¼
- **ì‹¤ìš©ì  ê°€ì¹˜**: ë…ìì—ê²Œ ì‹¤ì§ˆì  ë„ì›€ì´ ë˜ëŠ” ë‚´ìš©

**ê¸ˆì§€:**
- ì‹œì‚¬ì„± í† í”½ (ì†ë³´, ì‚¬ê±´ ì‚¬ê³ )
- ì‹¤ëª… ì¸ë¬¼ ê´€ë ¨ (ì—°ì˜ˆì¸, ì •ì¹˜ì¸)
- ë…¼ë€/ê°ì • ìê·¹í˜• í‚¤ì›Œë“œ
- ì¶”ìƒì  ì£¼ì œ ("AIì˜ ë¯¸ë˜", "ê¸°ìˆ  íŠ¸ë Œë“œ")
- **ìœ„ "ì´ë¯¸ íì— ì¡´ì¬í•˜ëŠ” í‚¤ì›Œë“œ" ëª©ë¡ê³¼ ë™ì¼í•˜ê±°ë‚˜ ìœ ì‚¬í•œ í‚¤ì›Œë“œ**

ì¶œë ¥ í˜•ì‹:
ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ë¼.

[
  {{
    "keyword": "ìœ„ Evergreen í‚¤ì›Œë“œ í’€ì—ì„œ ì„ íƒí•œ í‚¤ì›Œë“œ (ë˜ëŠ” ìœ ì‚¬ ë³€í˜•)",
    "raw_search_title": "ì‚¬ìš©ìê°€ êµ¬ê¸€ì— ê²€ìƒ‰í•  ë•Œ ì •í™•íˆ ì…ë ¥í•˜ëŠ” ê²€ìƒ‰ì–´",
    "editorial_title": "ê¸°ì‚¬ ì œëª© í˜•ì‹ì˜ ë…ì ì¹œí™”ì  ì œëª©",
    "core_question": "ì‚¬ìš©ìê°€ í•´ê²°í•˜ê³  ì‹¶ì€ í•µì‹¬ ì§ˆë¬¸ (êµìœ¡ì )",
    "language": "ko",
    "category": "tech",
    "search_intent": "ì‚¬ìš©ìê°€ ì´ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•˜ëŠ” ì‹¤ì§ˆì  ì´ìœ  (í•™ìŠµ, ë¬¸ì œ í•´ê²°, ì˜ì‚¬ ê²°ì • ë“±)",
    "angle": "ì´ í‚¤ì›Œë“œë¥¼ ë‹¤ë£° ë•Œì˜ ê´€ì  (êµìœ¡, ë¹„êµ, ê°€ì´ë“œ ë“±)",
    "competition_level": "low",
    "why_evergreen": "ì´ í‚¤ì›Œë“œê°€ ì¥ê¸°ê°„ ê²€ìƒ‰ë  ì´ìœ  (ì§€ì†ì  ìˆ˜ìš” ê·¼ê±°)",
    "keyword_type": "evergreen",
    "priority": 6,
    "risk_level": "safe",
    "name_policy": "no_real_names",
    "content_depth": "comprehensive"
  }}
]

ì¤‘ìš”:
- keyword_typeì€ "evergreen"ë§Œ ì‚¬ìš© (ì´ í”„ë¡¬í”„íŠ¸ëŠ” ì—ë²„ê·¸ë¦° ì „ìš©)
- categoryëŠ” **5ê°œ ì¹´í…Œê³ ë¦¬ë§Œ** ì‚¬ìš©: "tech", "business", "society", "entertainment", "sports"
- languageëŠ” "en", "ko" ì¤‘ í•˜ë‚˜ (ë¹„ìœ¨: EN 50%, KO 50%)
- competition_levelì€ "low", "medium"ë§Œ ì‚¬ìš© (high ê¸ˆì§€)
- priorityëŠ” 6-9 ì‚¬ì´ (Evergreenì€ ì¥ê¸° ê°€ì¹˜ê°€ ë†’ìœ¼ë¯€ë¡œ ìš°ì„ ìˆœìœ„ ìƒí–¥)
- risk_levelì€ ë¬´ì¡°ê±´ "safe"
- content_depthëŠ” "comprehensive"

**ğŸš¨ ì–¸ì–´ë³„ í‚¤ì›Œë“œ ìƒì„± ê·œì¹™:**
ë°˜ë“œì‹œ ì •í™•íˆ {count}ê°œì˜ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ë¼:
- ì˜ì–´(en): ì •í™•íˆ {en_count}ê°œ
- í•œêµ­ì–´(ko): ì •í™•íˆ {ko_count}ê°œ
- ì´í•©: ì •í™•íˆ {count}ê°œ

ê° ì–¸ì–´ ë‚´ì—ì„œ 5ê°œ ì¹´í…Œê³ ë¦¬ë¥¼ ê· ë“±í•˜ê²Œ ë¶„ë°°í•  ê²ƒ.
"""


class KeywordCurator:
    def __init__(self, api_key: str = None, google_api_key: str = None, google_cx: str = None):
        """Initialize keyword curator with Claude API and Google Custom Search"""
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            safe_print("âŒ ERROR: ANTHROPIC_API_KEY not found")
            safe_print("   Please set it as environment variable")
            safe_print("   Example: export ANTHROPIC_API_KEY='your-key-here'")
            raise ValueError("ANTHROPIC_API_KEY not found")

        # Brave Search API (replacing Google Custom Search)
        self.brave_api_key = os.environ.get("BRAVE_API_KEY")

        # Keep Google API keys for backward compatibility (deprecated)
        self.google_api_key = google_api_key or os.environ.get("GOOGLE_API_KEY")
        self.google_cx = google_cx or os.environ.get("GOOGLE_CX")

        if not self.brave_api_key:
            safe_print("âš ï¸  Brave Search API key not found")
            safe_print("   Set BRAVE_API_KEY environment variable")
            safe_print("   Falling back to Claude-only mode")
            if self.google_api_key and self.google_cx:
                safe_print("   Note: Google Custom Search API is deprecated for new users")

        try:
            self.client = Anthropic(api_key=self.api_key)
            self.model = "claude-sonnet-4-5-20250929"
            safe_print("  âœ“ Anthropic API client initialized successfully")
        except Exception as e:
            safe_print(f"âŒ ERROR: Failed to initialize Anthropic client")
            safe_print(f"   Error: {mask_secrets(str(e))}")
            raise

        # Load existing queue
        self.queue_path = Path("data/topics_queue.json")
        try:
            self.queue_data = self._load_queue()
            safe_print(f"  âœ“ Loaded topic queue: {len(self.queue_data.get('topics', []))} topics")
        except Exception as e:
            safe_print(f"âš ï¸  WARNING: Failed to load existing queue, starting fresh")
            safe_print(f"   Error: {str(e)}")
            self.queue_data = {"topics": []}

    def _load_queue(self) -> Dict:
        """Load existing topic queue"""
        if not self.queue_path.exists():
            return {"topics": []}

        with open(self.queue_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _save_queue(self):
        """Save updated topic queue"""
        try:
            # Ensure parent directory exists
            self.queue_path.parent.mkdir(parents=True, exist_ok=True)

            with open(self.queue_path, 'w', encoding='utf-8') as f:
                json.dump(self.queue_data, f, indent=2, ensure_ascii=False)
        except IOError as e:
            safe_print(f"âŒ ERROR: Failed to save queue to filesystem")
            safe_print(f"   Path: {self.queue_path}")
            safe_print(f"   Error: {str(e)}")
            raise
        except Exception as e:
            safe_print(f"âŒ ERROR: Unexpected error saving queue")
            safe_print(f"   Error: {str(e)}")
            raise

    def detect_intent_signals(self, query: str) -> list:
        """Detect intent signals from query for deduplication"""
        signals = []

        # State transition patterns
        if any(word in query.lower() for word in ["after", "ê°‘ìê¸°", "suddenly", "overnight"]):
            signals.append("STATE_CHANGE")

        # Promise broken patterns
        if any(word in query.lower() for word in ["promised", "supposed to", "ì•½ì†", "denied", "ê±°ë¶€"]):
            signals.append("PROMISE_BROKEN")

        # Silence patterns
        if any(word in query.lower() for word in ["no response", "ignored", "ë¬´ì‘ë‹µ", "ì¹¨ë¬µ"]):
            signals.append("SILENCE")

        # Deadline/time loss patterns
        if any(word in query.lower() for word in ["deadline", "too late", "ë§ˆê°", "ë†“ì¹¨"]):
            signals.append("DEADLINE_LOST")

        # Comparison/injustice patterns
        if any(word in query.lower() for word in ["others got", "only me", "ë‚˜ë§Œ"]):
            signals.append("COMPARISON")

        return signals if signals else ["GENERAL"]

    def fetch_community_topics(self) -> Dict[str, List[Dict]]:
        """Fetch trending topics from HackerNews, Reddit, and ProductHunt"""
        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸŒ Fetching topics from community sources...")
        safe_print(f"{'='*60}\n")

        community_topics = []
        verify_ssl = certifi.where() if certifi else True

        # 1. HackerNews - Top Stories + Top Comments (free, no auth)
        try:
            safe_print("  â†’ Fetching from HackerNews (with top comments)...")
            hn_top_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            response = requests.get(hn_top_url, timeout=10, verify=verify_ssl)
            response.raise_for_status()
            story_ids = response.json()[:10]  # Top 10 stories

            for story_id in story_ids[:5]:  # Fetch details for top 5
                try:
                    item_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                    item_resp = requests.get(item_url, timeout=5, verify=verify_ssl)
                    item_resp.raise_for_status()
                    item = item_resp.json()

                    if item and item.get('title'):
                        # Fetch top 3 comments for additional context
                        top_comments = []
                        comment_ids = item.get('kids', [])[:3]  # Top 3 comment IDs
                        for comment_id in comment_ids:
                            try:
                                comment_url = f"https://hacker-news.firebaseio.com/v0/item/{comment_id}.json"
                                comment_resp = requests.get(comment_url, timeout=3, verify=verify_ssl)
                                comment_resp.raise_for_status()
                                comment = comment_resp.json()
                                if comment and comment.get('text'):
                                    # Strip HTML tags and limit length
                                    import re
                                    clean_text = re.sub('<[^<]+?>', '', comment.get('text', ''))
                                    if len(clean_text) > 500:
                                        clean_text = clean_text[:500] + '...'
                                    top_comments.append(clean_text)
                            except Exception:
                                continue

                        community_topics.append({
                            'title': item['title'],
                            'source': 'HackerNews',
                            'url': item.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                            'score': item.get('score', 0),
                            'comments': item.get('descendants', 0),
                            'top_comments': top_comments  # NEW: Developer insights
                        })
                except Exception:
                    continue

            hn_count = len([t for t in community_topics if t['source'] == 'HackerNews'])
            safe_print(f"    âœ“ Found {hn_count} topics from HackerNews (with comments)")

        except Exception as e:
            safe_print(f"    âš ï¸ HackerNews fetch failed: {mask_secrets(str(e))}")

        # 2. Dev.to - Developer community (free, no auth needed)
        try:
            safe_print("  â†’ Fetching from Dev.to (top articles)...")
            devto_url = "https://dev.to/api/articles?top=1&per_page=5"
            headers = {'User-Agent': 'JakesTechInsights/1.0'}
            response = requests.get(devto_url, headers=headers, timeout=10, verify=verify_ssl)
            response.raise_for_status()
            data = response.json()

            for article in data[:5]:
                if article.get('title'):
                    community_topics.append({
                        'title': article['title'],
                        'source': 'Dev.to',
                        'url': article.get('url', ''),
                        'score': article.get('positive_reactions_count', 0),
                        'comments': article.get('comments_count', 0)
                    })

            devto_count = len([t for t in community_topics if t['source'] == 'Dev.to'])
            safe_print(f"    âœ“ Found {devto_count} topics from Dev.to")

        except Exception as e:
            safe_print(f"    âš ï¸ Dev.to fetch failed: {mask_secrets(str(e))}")

        # 3a. Lobsters - Tech community (free, no auth needed)
        try:
            safe_print("  â†’ Fetching from Lobsters (hottest)...")
            lobsters_url = "https://lobste.rs/hottest.json"
            response = requests.get(lobsters_url, headers={'User-Agent': 'JakesTechInsights/1.0'}, timeout=10, verify=verify_ssl)
            response.raise_for_status()
            data = response.json()

            for article in data[:5]:
                if article.get('title'):
                    community_topics.append({
                        'title': article['title'],
                        'source': 'Lobsters',
                        'url': article.get('url') or article.get('short_id_url', ''),
                        'score': article.get('score', 0),
                        'comments': article.get('comment_count', 0)
                    })

            lobsters_count = len([t for t in community_topics if t['source'] == 'Lobsters'])
            safe_print(f"    âœ“ Found {lobsters_count} topics from Lobsters")

        except Exception as e:
            safe_print(f"    âš ï¸ Lobsters fetch failed: {mask_secrets(str(e))}")

        # 3. ProductHunt - Using Atom feed with descriptions (no auth needed)
        try:
            safe_print("  â†’ Fetching from ProductHunt (with descriptions)...")
            import xml.etree.ElementTree as ET
            ph_feed_url = "https://www.producthunt.com/feed"
            response = requests.get(ph_feed_url, timeout=10, verify=verify_ssl)
            response.raise_for_status()

            root = ET.fromstring(response.content)
            # ProductHunt uses Atom format, not RSS - need to use namespace
            atom_ns = {'atom': 'http://www.w3.org/2005/Atom'}
            entries = root.findall('atom:entry', atom_ns)

            for entry in entries[:5]:
                title_elem = entry.find('atom:title', atom_ns)
                link_elem = entry.find('atom:link', atom_ns)
                content_elem = entry.find('atom:content', atom_ns)  # Atom uses 'content' not 'description'

                if title_elem is not None and title_elem.text:
                    # Extract URL from link element's href attribute
                    url = ''
                    if link_elem is not None:
                        url = link_elem.get('href', '')

                    # Extract and clean description from content
                    description = ''
                    if content_elem is not None and content_elem.text:
                        import re
                        description = re.sub('<[^<]+?>', '', content_elem.text)  # Strip HTML
                        description = ' '.join(description.split())  # Normalize whitespace
                        if len(description) > 500:
                            description = description[:500] + '...'

                    community_topics.append({
                        'title': title_elem.text.strip(),
                        'source': 'ProductHunt',
                        'url': url,
                        'score': 0,
                        'comments': 0,
                        'description': description  # Product details
                    })

            ph_count = len([t for t in community_topics if t['source'] == 'ProductHunt'])
            safe_print(f"    âœ“ Found {ph_count} topics from ProductHunt (with descriptions)")

        except Exception as e:
            safe_print(f"    âš ï¸ ProductHunt fetch failed: {mask_secrets(str(e))}")

        safe_print(f"\n  ğŸ‰ Total {len(community_topics)} community topics fetched!\n")

        return {'en': community_topics}  # All community sources are English

    def fetch_trending_from_rss(self) -> Dict[str, List[str]]:
        """Fetch trending topics from Google Trends RSS feeds grouped by language"""
        import xml.etree.ElementTree as ET

        rss_urls = {
            "KR": "https://trends.google.co.kr/trending/rss?geo=KR",
            "US": "https://trends.google.co.kr/trending/rss?geo=US"
        }

        # Map region to language
        region_to_lang = {
            "KR": "ko",
            "US": "en"
        }

        # Group trends by language
        trends_by_lang = {"ko": [], "en": []}

        for geo, url in rss_urls.items():
            try:
                verify_ssl = certifi.where() if certifi else True
                response = requests.get(url, timeout=10, verify=verify_ssl)
                response.raise_for_status()

                # Parse XML
                root = ET.fromstring(response.content)

                # Find all items (trending topics)
                items = root.findall('.//item')

                lang = region_to_lang[geo]
                for item in items[:5]:  # Top 5 per region (15 total)
                    title_elem = item.find('title')
                    if title_elem is not None and title_elem.text:
                        trends_by_lang[lang].append(title_elem.text.strip())

                safe_print(f"  âœ“ Found {min(len(items), 5)} trends from {geo} â†’ {lang}")

            except requests.exceptions.Timeout:
                safe_print(f"  âš ï¸  RSS fetch timeout for {geo}: Request took too long")
                continue
            except requests.exceptions.HTTPError as e:
                safe_print(f"  âš ï¸  RSS HTTP error for {geo}: {e.response.status_code if e.response else 'unknown'}")
                continue
            except ET.ParseError as e:
                safe_print(f"  âš ï¸  RSS parse error for {geo}: Invalid XML format")
                safe_print(f"     Error: {str(e)}")
                continue
            except Exception as e:
                safe_print(f"  âš ï¸  RSS fetch error for {geo}: {mask_secrets(str(e))}")
                continue

        return trends_by_lang

    def fetch_trending_topics(self) -> Dict[str, str]:
        """Fetch trending topics using Google Trends RSS feeds, grouped by language"""
        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸ”¥ Fetching REAL-TIME trending topics from Google Trends RSS...")
        safe_print(f"{'='*60}\n")

        # Try RSS feeds first (most reliable method)
        trends_by_lang = self.fetch_trending_from_rss()

        # Check if we got any trends
        total_trends = sum(len(trends) for trends in trends_by_lang.values())

        if total_trends > 0:
            safe_print(f"\n  ğŸ‰ Total {total_trends} real-time trending topics from RSS!")
            safe_print(f"     EN: {len(trends_by_lang['en'])}, KO: {len(trends_by_lang['ko'])}\n")
        else:
            safe_print("  âš ï¸  RSS feeds failed. Falling back to pattern-based queries...\n")
            # Fallback to pattern queries (grouped by language)
            trends_by_lang = {
                "en": [
                    "account banned after update no response",
                    "service outage promised compensation denied",
                    "class action deadline passed too late",
                    "refund promised but denied suddenly",
                    "government support supposed to but denied",
                    "new policy suddenly stricter than announced",
                    "celebrity apology issued but backlash continues"
                ],
                "ko": [
                    "ì•± ì—…ë°ì´íŠ¸ í›„ ê°‘ìê¸° ë¨¹í†µ",
                    "ì§‘ë‹¨ì†Œì†¡ ì‹ ì²­ ë§ˆê° ë†“ì¹¨",
                    "ì •ë¶€ì§€ì› ì¡°ê±´ ë°œí‘œì™€ ë‹¤ë¦„",
                    "ì‚¬ê³¼ë¬¸ ëƒˆì§€ë§Œ ë…¼ë€ ê³„ì†",
                    "ë¦¬ì½œ ë°œí‘œí–ˆëŠ”ë° í™˜ë¶ˆ ê±°ë¶€"
                ]
            }

        # Flatten for search queries (but keep language tracking)
        all_queries = []
        for lang, queries in trends_by_lang.items():
            for query in queries:
                all_queries.append((query, lang))

        # If no Brave Search API, skip search results
        if not self.brave_api_key:
            safe_print("  ğŸš¨ CRITICAL WARNING: Brave Search API not configured")
            safe_print("  ğŸ“Œ References will NOT be generated for keywords!")
            safe_print("  ğŸ“Œ Set BRAVE_API_KEY environment variable")
            safe_print("  ğŸ“Œ OR: Add it as GitHub Secret for automated workflows\n")
            self.search_results = []

            # Format trends by language for prompt
            trends_formatted = {}
            for lang, queries in trends_by_lang.items():
                trends_formatted[lang] = "\n".join([f"Query: {q}" for q in queries[:10]])

            return trends_formatted

        all_results = []
        for query, query_lang in all_queries:
            try:
                # Brave Search API endpoint
                url = "https://api.search.brave.com/res/v1/web/search"
                headers = {
                    "Accept": "application/json",
                    "X-Subscription-Token": self.brave_api_key
                }
                params = {
                    "q": query,
                    "count": 3,  # Get top 3 results per query for better quality
                    "freshness": "pw"  # Past week (ìµœì‹  ë‰´ìŠ¤)
                }

                # Add delay to avoid rate limiting
                time.sleep(0.5)

                verify_ssl = certifi.where() if certifi else True
                response = requests.get(url, headers=headers, params=params, verify=verify_ssl)
                response.raise_for_status()

                data = response.json()

                # Brave API returns results in "web" -> "results" structure
                web_results = data.get("web", {}).get("results", [])

                if web_results:
                    # Detect intent signals for this query
                    signals = self.detect_intent_signals(query)

                    for item in web_results:
                        all_results.append({
                            "query": query,
                            "query_lang": query_lang,  # Track which language this query belongs to
                            "signals": signals,  # Add intent signals
                            "title": item.get("title", ""),
                            "snippet": item.get("description", ""),  # Brave uses "description" not "snippet"
                            "link": item.get("url", ""),  # Brave uses "url" not "link"
                            "source": item.get("url", "").split("/")[2] if item.get("url") else ""  # Extract domain
                        })

                safe_print(f"  âœ“ Fetched {len(web_results)} results for: {query}")

            except requests.exceptions.Timeout:
                safe_print(f"  âš ï¸  Timeout fetching results for '{query[:50]}...'")
                continue
            except requests.exceptions.HTTPError as e:
                status_code = e.response.status_code if e.response else 'unknown'
                safe_print(f"  âš ï¸  HTTP error ({status_code}) for '{query[:50]}...'")
                if status_code == 403:
                    safe_print(f"     âš ï¸  Brave API Access Forbidden - check API key")
                elif status_code == 429:
                    safe_print(f"     Rate limit exceeded (2000/month limit)")
                continue
            except json.JSONDecodeError:
                safe_print(f"  âš ï¸  Invalid JSON response for '{query[:50]}...'")
                continue
            except requests.exceptions.RequestException as e:
                safe_print(f"  âš ï¸  Network error for '{query[:50]}...': {mask_secrets(str(e))}")
                continue
            except Exception as e:
                safe_print(f"  âš ï¸  Unexpected error for '{query[:50]}...': {mask_secrets(str(e))}")
                continue

        safe_print(f"\nâœ… Total {len(all_results)} trending topics fetched\n")

        # Store results for reference extraction
        self.search_results = all_results

        # Format results for Claude, grouped by language
        trends_by_lang_formatted = {"en": [], "ko": []}
        for r in all_results:
            lang = r.get('query_lang', 'en')
            if lang in trends_by_lang_formatted:
                trends_by_lang_formatted[lang].append(
                    f"Query: {r['query']}\nTitle: {r['title']}\nSnippet: {r['snippet']}\n"
                )

        # Convert to string format per language
        trends_formatted = {}
        for lang in ["en", "ko"]:
            trends_formatted[lang] = "\n\n".join(trends_by_lang_formatted[lang][:10])  # Top 10 per language

        return trends_formatted

    def filter_by_risk(self, candidates: List[Dict]) -> List[Dict]:
        """Filter out high-risk keywords automatically"""
        safe_candidates = []
        filtered_count = 0

        for kw in candidates:
            # Auto-reject high-risk
            if kw.get("risk_level") == "high_risk":
                filtered_count += 1
                safe_print(f"  ğŸ”´ Filtered high-risk: {kw.get('keyword', 'unknown')}")
                continue

            # Flag caution items for manual review
            if kw.get("risk_level") == "caution":
                kw["needs_review"] = True
                safe_print(f"  ğŸŸ¡ Caution flagged: {kw.get('keyword', 'unknown')}")

            safe_candidates.append(kw)

        if filtered_count > 0:
            safe_print(f"\nâš ï¸  {filtered_count} high-risk keywords filtered out\n")

        return safe_candidates

    def fetch_evergreen_references(self, keyword: str, lang: str) -> List[Dict]:
        """Fetch references for evergreen keywords on-demand using Brave Search"""
        if not self.brave_api_key:
            safe_print(f"  âš ï¸  No Brave API key - skipping references for: {keyword}")
            return []

        try:
            url = "https://api.search.brave.com/res/v1/web/search"
            headers = {
                "Accept": "application/json",
                "X-Subscription-Token": self.brave_api_key
            }
            params = {
                "q": keyword,
                "count": 3,
                "freshness": "py"  # Past year (for evergreen content)
            }

            time.sleep(0.5)  # Rate limiting

            verify_ssl = certifi.where() if certifi else True
            response = requests.get(url, headers=headers, params=params, verify=verify_ssl)
            response.raise_for_status()

            data = response.json()
            web_results = data.get("web", {}).get("results", [])

            references = []
            seen_domains = set()

            for item in web_results:
                link = item.get("url", "")
                title = item.get("title", "")
                source = link.split("/")[2] if link else ""

                if link and source and source not in seen_domains:
                    references.append({
                        "title": title[:100],
                        "url": link,
                        "source": source
                    })
                    seen_domains.add(source)

                if len(references) >= 3:
                    break

            return references

        except Exception as e:
            safe_print(f"  âš ï¸  Failed to fetch references for '{keyword}': {mask_secrets(str(e))}")
            return []

    def extract_references(self, all_results: List[Dict], keyword: str, lang: str) -> List[Dict]:
        """Extract top 3 references for a keyword based on search results"""
        # Find relevant results for this keyword
        # Match by language and keyword similarity
        relevant = []

        for result in all_results:
            query = result.get("query", "").lower()
            # Simple matching: if keyword words appear in query
            keyword_words = set(keyword.lower().split())
            query_words = set(query.split())

            # Check language match (simple heuristic)
            is_relevant = len(keyword_words & query_words) > 0

            if is_relevant:
                relevant.append(result)

        # Take top 3 unique sources
        references = []
        seen_domains = set()

        for result in relevant[:10]:  # Check first 10 relevant results
            link = result.get("link", "")
            source = result.get("source", "")
            title = result.get("title", "")

            if link and source and source not in seen_domains:
                references.append({
                    "title": title[:100],  # Truncate long titles
                    "url": link,
                    "source": source
                })
                seen_domains.add(source)

            if len(references) >= 3:  # Get 3 references per keyword for AdSense quality
                break

        return references

    def load_evergreen_keywords(self) -> Dict[str, Dict[str, List[str]]]:
        """Load evergreen keywords from JSON file"""
        evergreen_path = Path("data/evergreen_keywords.json")
        if not evergreen_path.exists():
            safe_print("âš ï¸  Evergreen keywords file not found, using empty pool")
            return {"tech": {"en": [], "ko": []}, "business": {"en": [], "ko": []}}

        with open(evergreen_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def generate_candidates(self, count: int = 15, keyword_type: str = "trend") -> List[Dict]:
        """Generate keyword candidates using Claude API

        Args:
            count: Number of keywords to generate
            keyword_type: "trend" or "evergreen"
        """
        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸ” Generating {count} {keyword_type} keyword candidates...")
        safe_print(f"{'='*60}\n")

        # Calculate per-language count (EN 50%, KO 50%)
        en_count = count // 2
        ko_count = count - en_count  # Remainder to KO

        if keyword_type == "evergreen":
            # Load evergreen keywords pool
            evergreen_pool = self.load_evergreen_keywords()

            # Format evergreen keywords for prompt
            evergreen_en = "\n".join([f"- {kw}" for cat in evergreen_pool.values() for kw in cat.get("en", [])])
            evergreen_ko = "\n".join([f"- {kw}" for cat in evergreen_pool.values() for kw in cat.get("ko", [])])

            # Collect existing keywords from queue to prevent duplicates
            existing_keywords = [t['keyword'] for t in self.queue_data.get('topics', [])]
            existing_keywords_text = "\n".join([f"- {kw}" for kw in existing_keywords[-100:]]) if existing_keywords else "ì—†ìŒ"

            # Generate prompt with evergreen data
            prompt = CURATION_PROMPT_EVERGREEN.format(
                evergreen_en=evergreen_en,
                evergreen_ko=evergreen_ko,
                count=count,
                en_count=en_count,
                ko_count=ko_count,
                existing_keywords=existing_keywords_text
            )

            # For evergreen, we don't need real-time search results
            self.search_results = []

        else:  # trend
            # Fetch trending topics from Google (store for reference extraction)
            self.search_results = []  # Store search results
            trends_by_lang = self.fetch_trending_topics()

            # Fetch community topics (HackerNews, Dev.to, Lobsters, ProductHunt)
            community_data = self.fetch_community_topics()
            community_topics_list = community_data.get('en', [])

            # Format community topics for prompt (with additional context)
            def format_community_topic(t):
                base = f"- [{t['source']}] {t['title']} (score: {t['score']}, comments: {t['comments']})\n  URL: {t['url']}"

                # Add HackerNews top comments if available
                if t.get('top_comments'):
                    comments_text = "\n  ğŸ’¬ Top developer comments:\n"
                    for i, comment in enumerate(t['top_comments'][:2], 1):  # Max 2 comments
                        comments_text += f"    {i}. {comment[:200]}...\n" if len(comment) > 200 else f"    {i}. {comment}\n"
                    base += comments_text

                # Add ProductHunt description if available
                if t.get('description'):
                    desc = t['description'][:300] + '...' if len(t['description']) > 300 else t['description']
                    base += f"\n  ğŸ“ Description: {desc}"

                return base

            community_topics_formatted = "\n".join([
                format_community_topic(t)
                for t in community_topics_list[:15]  # Top 15 community topics
            ]) or "No community topics available"

            # Generate prompt with trending data (grouped by language)
            prompt = CURATION_PROMPT_WITH_TRENDS.format(
                trends_en=trends_by_lang.get('en', 'No English trends available'),
                trends_ko=trends_by_lang.get('ko', 'No Korean trends available'),
                community_topics=community_topics_formatted,
                count=count,
                en_count=en_count,
                ko_count=ko_count
            )

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=16000,  # Increased for 30+ keywords
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )
        except Exception as e:
            safe_print(f"âŒ ERROR: Claude API call failed")
            safe_print(f"   Error: {mask_secrets(str(e))}")
            safe_print(f"   This is a critical error - cannot continue without keyword candidates")
            sys.exit(1)

        if not response or not response.content:
            safe_print(f"âŒ ERROR: Empty response from Claude API")
            safe_print(f"   This is a critical error - cannot continue without keyword candidates")
            sys.exit(1)

        # Parse JSON response
        content = response.content[0].text.strip()

        # Extract JSON from markdown code blocks if present
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()

        try:
            candidates = json.loads(content)
        except json.JSONDecodeError as e:
            safe_print(f"âŒ ERROR: Failed to parse JSON response from Claude")
            safe_print(f"   Parse error: {str(e)}")
            safe_print(f"   Raw response (first 500 chars):\n{content[:500]}")
            safe_print(f"   This is a critical error - cannot continue with invalid JSON")
            sys.exit(1)

        safe_print(f"âœ… Generated {len(candidates)} candidates\n")

        # STEP 1: Remove duplicates (keep first occurrence, regardless of category)
        seen_keywords = {}
        dedup_candidates = []
        duplicates_removed = 0

        for candidate in candidates:
            keyword_lower = candidate.get('keyword', '').lower()
            if keyword_lower in seen_keywords:
                duplicates_removed += 1
                first_category = seen_keywords[keyword_lower]
                duplicate_category = candidate.get('category')
                safe_print(f"  ğŸ”´ DUPLICATE REMOVED: '{candidate.get('keyword')}' (duplicate category: {duplicate_category}, already exists as: {first_category})")
            else:
                # Store the category of the first occurrence
                seen_keywords[keyword_lower] = candidate.get('category')
                dedup_candidates.append(candidate)

        if duplicates_removed > 0:
            safe_print(f"\nâš ï¸  Removed {duplicates_removed} duplicate keywords from Claude's response")
            safe_print(f"    Policy: One keyword = one category (first occurrence wins)\n")

        # STEP 2: Auto-correct sports keywords category
        sports_keywords = ['vs', 'vs.', 'game', 'match', 'league', 'cup', 'tournament', 'championship',
                          'basketball', 'football', 'soccer', 'baseball', 'hockey', 'tennis', 'golf',
                          'nba', 'nfl', 'mlb', 'nhl', 'premier league', 'uefa', 'champions league',
                          'world cup', 'olympics', 'ufc', 'boxing', 'wrestling', 'mma',
                          'u23', 'u-23', 'u21', 'u-21', 'player', 'team', 'squad']

        corrected_count = 0
        for candidate in dedup_candidates:
            keyword_lower = candidate.get('keyword', '').lower()
            category = candidate.get('category', '')

            # Auto-detect sports keywords
            if category != 'sports':
                is_sports = any(sport_term in keyword_lower for sport_term in sports_keywords)
                if is_sports:
                    old_category = category
                    candidate['category'] = 'sports'
                    corrected_count += 1
                    safe_print(f"  âœ… AUTO-CORRECTED: {candidate.get('keyword')} ({old_category} â†’ sports)")

        if corrected_count > 0:
            safe_print(f"\nâœ… Auto-corrected {corrected_count} sports keywords\n")

        # Apply risk filtering
        filtered_candidates = self.filter_by_risk(dedup_candidates)

        # Extract references for each candidate
        safe_print(f"ğŸ“š Extracting references for {len(filtered_candidates)} candidates...\n")
        keywords_with_refs = 0
        keywords_without_refs = 0

        for candidate in filtered_candidates:
            keyword = candidate.get("keyword", "")
            lang = candidate.get("language", "en")
            kw_type = candidate.get("keyword_type", "trend")

            # For evergreen keywords, fetch references on-demand
            if kw_type == "evergreen" and not self.search_results:
                references = self.fetch_evergreen_references(keyword, lang)
            else:
                references = self.extract_references(self.search_results, keyword, lang)

            candidate["references"] = references
            if references:
                safe_print(f"  âœ“ {len(references)} refs for: {keyword[:50]}...")
                keywords_with_refs += 1
            else:
                keywords_without_refs += 1

        safe_print("")

        # Validation warning
        if keywords_without_refs > 0:
            safe_print(f"âš ï¸  WARNING: {keywords_without_refs}/{len(filtered_candidates)} keywords have NO references")
            safe_print(f"   This means generated posts will lack credible sources!")
            if not self.google_api_key or not self.google_cx:
                safe_print(f"   ROOT CAUSE: Google Custom Search API credentials not configured")
                safe_print(f"   FIX: Set GOOGLE_API_KEY and GOOGLE_CX environment variables\n")
        else:
            safe_print(f"âœ… All {keywords_with_refs} keywords have references!\n")

        return filtered_candidates

    def display_candidates(self, candidates: List[Dict]):
        """Display candidates with numbered list"""
        safe_print(f"{'='*60}")
        safe_print(f"  ğŸ“‹ Keyword Candidates")
        safe_print(f"{'='*60}\n")

        # Group by language
        by_lang = {"en": [], "ko": []}
        for c in candidates:
            lang = c.get("language", "en")
            if lang in by_lang:
                by_lang[lang].append(c)

        idx = 1
        lang_names = {"en": "English", "ko": "Korean"}

        for lang in ["en", "ko"]:
            if by_lang[lang]:
                safe_print(f"\n[{lang_names[lang]}]")
                safe_print("-" * 60)

                for candidate in by_lang[lang]:
                    type_emoji = "ğŸ”¥" if candidate.get("keyword_type") == "trend" else "ğŸŒ²"
                    comp_emoji = {
                        "low": "ğŸŸ¢",
                        "medium": "ğŸŸ¡",
                        "high": "ğŸ”´"
                    }.get(candidate.get("competition_level", "medium"), "âšª")

                    safe_print(f"\n{idx}. {type_emoji} {candidate['keyword']}")
                    safe_print(f"   Category: {candidate['category']} | Competition: {comp_emoji} {candidate.get('competition_level', 'N/A')}")
                    safe_print(f"   Intent: {candidate['search_intent']}")
                    safe_print(f"   Angle: {candidate['angle']}")
                    safe_print(f"   Why: {candidate.get('why_it_works', 'N/A')[:80]}...")

                    idx += 1

        safe_print(f"\n{'='*60}\n")

    def interactive_selection(self, candidates: List[Dict]) -> List[Dict]:
        """Interactive selection of keywords"""
        safe_print("ì–´ë–¤ í‚¤ì›Œë“œë¥¼ íì— ì¶”ê°€í• ê¹Œìš”?")
        safe_print("ìˆ«ìë¥¼ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 1,3,5,7,10)")
        safe_print("ë˜ëŠ” 'all'ì„ ì…ë ¥í•˜ë©´ ì „ë¶€ ì¶”ê°€ë©ë‹ˆë‹¤.")
        safe_print("'q'ë¥¼ ì…ë ¥í•˜ë©´ ì·¨ì†Œí•©ë‹ˆë‹¤.\n")

        while True:
            user_input = input("ì„ íƒ: ").strip()

            if user_input.lower() == 'q':
                safe_print("âŒ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return []

            if user_input.lower() == 'all':
                return candidates

            try:
                # Parse selected indices
                selected_indices = [int(x.strip()) for x in user_input.split(',')]

                # Validate indices
                if any(idx < 1 or idx > len(candidates) for idx in selected_indices):
                    safe_print(f"âš ï¸  ì˜ëª»ëœ ë²ˆí˜¸ì…ë‹ˆë‹¤. 1-{len(candidates)} ë²”ìœ„ë¡œ ì…ë ¥í•˜ì„¸ìš”.\n")
                    continue

                # Convert to 0-based index and return selected candidates
                selected = [candidates[idx - 1] for idx in selected_indices]
                return selected

            except ValueError:
                safe_print("âš ï¸  ì˜ëª»ëœ í˜•ì‹ì…ë‹ˆë‹¤. ì˜ˆ: 1,3,5\n")

    def _validate_keyword_language(self, keyword: str, language: str) -> bool:
        """Validate that keyword matches the specified language"""

        def has_hangul(text):
            """Check if text contains Korean characters"""
            return any('\uac00' <= char <= '\ud7a3' for char in text)

        def has_hiragana_katakana(text):
            """Check if text contains Japanese characters"""
            return any(
                ('\u3040' <= char <= '\u309f') or  # Hiragana
                ('\u30a0' <= char <= '\u30ff')     # Katakana
                for char in text
            )

        # Validation rules
        if language == 'ko':
            # Korean must have Hangul
            if not has_hangul(keyword):
                return False
            # Korean cannot have Japanese characters
            if has_hiragana_katakana(keyword):
                return False
        elif language == 'en':
            # English cannot have Korean/Japanese
            if has_hangul(keyword) or has_hiragana_katakana(keyword):
                return False

        return True

    def add_to_queue(self, selected: List[Dict]):
        """Add selected keywords to topic queue with language and duplicate validation"""
        if not selected:
            safe_print("ì„ íƒëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸ’¾ íì— {len(selected)}ê°œ í‚¤ì›Œë“œ ì¶”ê°€ ì¤‘...")
        safe_print(f"{'='*60}\n")

        # Get existing keywords for duplicate check (case-insensitive)
        existing_keywords = {t['keyword'].lower() for t in self.queue_data['topics']}

        # Get next ID
        existing_ids = [int(t['id'].split('-')[0]) for t in self.queue_data['topics'] if t['id'].split('-')[0].isdigit()]
        next_id = max(existing_ids) + 1 if existing_ids else 1

        added_count = 0
        rejected_count = 0
        for candidate in selected:
            # Validate keyword-language match
            keyword = candidate.get('keyword', '')
            language = candidate.get('language', 'en')

            # Check for duplicate keyword
            if keyword.lower() in existing_keywords:
                safe_print(f"  ğŸ”´ REJECTED: Duplicate keyword")
                safe_print(f"     Keyword: {keyword}")
                safe_print(f"     Reason: Keyword already exists in queue")
                rejected_count += 1
                continue

            if not self._validate_keyword_language(keyword, language):
                safe_print(f"  ğŸ”´ REJECTED: Keyword-language mismatch")
                safe_print(f"     Keyword: {keyword}")
                safe_print(f"     Language: {language}")
                safe_print(f"     Reason: Keyword contains characters from different language")
                rejected_count += 1
                continue
            # Generate topic ID
            topic_id = f"{next_id:03d}-{candidate['language']}-{candidate['category']}-{candidate['keyword'][:20].replace(' ', '-')}"

            # Create topic entry
            topic = {
                "id": topic_id,
                "keyword": candidate['keyword'],
                "category": candidate['category'],
                "lang": candidate['language'],
                "priority": candidate.get('priority', 7),
                "status": "pending",
                "created_at": datetime.now().isoformat(),
                "retry_count": 0,
                "keyword_type": candidate.get('keyword_type', 'evergreen'),
                "search_intent": candidate.get('search_intent', ''),
                "angle": candidate.get('angle', ''),
                "competition_level": candidate.get('competition_level', 'medium'),
                "references": candidate.get('references', [])
            }

            # Add expiry_days for trend keywords
            if topic['keyword_type'] == 'trend':
                topic['expiry_days'] = 3  # 3 days expiry for trending keywords
                # Add content_type hint for trends: prefer BREAKING
                topic['content_type_hint'] = 'BREAKING'
            elif topic['keyword_type'] == 'evergreen':
                # Add content_type hint for evergreen: prefer GUIDE
                topic['content_type_hint'] = 'GUIDE'
            else:
                # Mixed or undefined: let generate_posts.py decide
                topic['content_type_hint'] = 'MIXED'

            self.queue_data['topics'].append(topic)

            type_label = "ğŸ”¥ Trend" if topic['keyword_type'] == 'trend' else "ğŸŒ² Evergreen"
            safe_print(f"  âœ“ Added: {type_label} | {candidate['keyword']}")

            added_count += 1
            next_id += 1

        # Save queue
        self._save_queue()

        safe_print(f"\nâœ… {added_count}ê°œ í‚¤ì›Œë“œê°€ íì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
        if rejected_count > 0:
            safe_print(f"ğŸ”´ {rejected_count}ê°œ í‚¤ì›Œë“œê°€ ì–¸ì–´ ë¶ˆì¼ì¹˜ë¡œ ê±°ë¶€ë˜ì—ˆìŠµë‹ˆë‹¤!")
        safe_print(f"ğŸ“Š Total topics in queue: {len(self.queue_data['topics'])}")

        # Show statistics
        self._show_queue_stats()

    def _show_queue_stats(self):
        """Show queue statistics"""
        topics = self.queue_data['topics']

        # Count by status (safe for any status value)
        by_status = {"pending": 0, "in_progress": 0, "completed": 0, "failed": 0}
        for t in topics:
            status = t.get('status', 'pending')
            by_status[status] = by_status.get(status, 0) + 1

        # Count by type
        by_type = {"trend": 0, "evergreen": 0, "unknown": 0}
        for t in topics:
            ktype = t.get('keyword_type', 'unknown')
            by_type[ktype] = by_type.get(ktype, 0) + 1

        # Count by language
        by_lang = {"en": 0, "ko": 0}
        for t in topics:
            lang = t.get('lang', 'en')
            if lang in by_lang:
                by_lang[lang] = by_lang.get(lang, 0) + 1

        safe_print(f"\n{'='*60}")
        safe_print(f"  ğŸ“Š Queue Statistics")
        safe_print(f"{'='*60}")
        safe_print(f"  Status: Pending={by_status['pending']}, In Progress={by_status['in_progress']}, Completed={by_status['completed']}")
        safe_print(f"  Type: ğŸ”¥ Trend={by_type['trend']}, ğŸŒ² Evergreen={by_type['evergreen']}, Unknown={by_type['unknown']}")
        safe_print(f"  Language: EN={by_lang['en']}, KO={by_lang['ko']}")
        safe_print(f"{'='*60}\n")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Keyword Curator for blog content")
    parser.add_argument('--count', type=int, default=15, help="Number of candidates to generate (default: 15)")
    parser.add_argument('--auto', action='store_true', help="Automatically add all candidates without interactive selection")
    parser.add_argument('--type', choices=['trend', 'evergreen', 'mixed'], default='trend',
                       help="Keyword type: trend (default), evergreen, or mixed")
    parser.add_argument('--evergreen-ratio', type=float, default=0.2,
                       help="Ratio of evergreen keywords in mixed mode (default: 0.2)")
    args = parser.parse_args()

    # Check API key
    if not os.environ.get("ANTHROPIC_API_KEY"):
        safe_print("Error: ANTHROPIC_API_KEY environment variable not set")
        sys.exit(1)

    # Initialize curator
    curator = KeywordCurator()

    # Generate candidates based on type
    if args.type == 'mixed':
        # Mixed mode: Google Trends 40% + Community 40% + Evergreen 20%
        # trend candidates (80%) include both Google Trends and Community sources
        # The prompt instructs Claude to split trend candidates 50/50 between Trends and Community
        evergreen_count = int(args.count * args.evergreen_ratio)
        trend_count = args.count - evergreen_count

        safe_print(f"\nğŸ“Š Mixed Mode: {trend_count} trend (Trends+Community) + {evergreen_count} evergreen keywords")
        safe_print(f"   Source ratio: Google Trends ~{trend_count//2} + Community ~{trend_count - trend_count//2} + Evergreen {evergreen_count}\n")

        # Generate trend keywords
        if trend_count > 0:
            trend_candidates = curator.generate_candidates(count=trend_count, keyword_type="trend")
        else:
            trend_candidates = []

        # Generate evergreen keywords
        if evergreen_count > 0:
            evergreen_candidates = curator.generate_candidates(count=evergreen_count, keyword_type="evergreen")
        else:
            evergreen_candidates = []

        # Combine candidates
        candidates = trend_candidates + evergreen_candidates

    else:
        # Single type mode
        candidates = curator.generate_candidates(count=args.count, keyword_type=args.type)

    # Display candidates
    curator.display_candidates(candidates)

    # Selection
    if args.auto:
        # Auto mode: add all candidates
        safe_print("\nğŸ¤– Auto mode: Adding all candidates to queue...\n")
        selected = candidates
    else:
        # Interactive mode: ask user
        selected = curator.interactive_selection(candidates)

    # Add to queue
    if selected:
        curator.add_to_queue(selected)

    safe_print("\nâœ¨ Done!\n")


if __name__ == "__main__":
    main()
