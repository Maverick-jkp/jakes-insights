#!/usr/bin/env python3
"""
Content Generation Script

Generates blog posts using Claude API with two-stage process:
1. Draft Agent: Creates initial content
2. Editor Agent: Refines and improves the draft

Usage:
    python generate_posts.py --count 3
    python generate_posts.py --topic-id 001-en-tech-ai-coding
"""

import os
import sys
import json
import argparse
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

# Add scripts to path
sys.path.insert(0, str(Path(__file__).parent))

from topic_queue import reserve_topics, mark_completed, mark_failed
from utils.security import safe_print, mask_secrets
from utils.content_classifier import ContentClassifier
from affiliate_config import (
    detect_product_mentions,
    generate_affiliate_link,
    create_affiliate_box,
    get_affiliate_disclosure,
    should_add_affiliate_links
)
from internal_linker import InternalLinker
from ab_test_manager import ABTestManager
from prompts import get_tutorial_prompt, get_analysis_prompt, get_news_prompt

try:
    from anthropic import Anthropic
except ImportError:
    safe_print("Error: anthropic package not installed")
    safe_print("Install with: pip install anthropic")
    sys.exit(1)

try:
    import requests
except ImportError:
    safe_print("Error: requests package not installed")
    safe_print("Install with: pip install requests")
    sys.exit(1)

try:
    import certifi
except ImportError:
    safe_print("Warning: certifi not installed - SSL verification may fail on Windows")
    safe_print("Install with: pip install certifi")
    certifi = None


# System prompts for different languages
SYSTEM_PROMPTS = {
    "en": f"""You are a professional writer for Jake's Tech Insights blog.

üìÖ IMPORTANT: Today's date is {datetime.now().year}-{datetime.now().month:02d}-{datetime.now().day:02d}
When referencing current information, ALWAYS use {datetime.now().year}, NOT previous years.

üéØ Goal: 800-1,100 words of concise, high-impact content (AdSense optimized)

[EDITORIAL POLICY - READ FIRST]
This is an in-depth tech analysis publication covering:
- Technology trends, SaaS analysis, data-driven industry reports, developer tools
- Reader value and analytical depth are PRIMARY goals
- Data-backed claims and evidence-based analysis ARE required

Your role:
- Provide evidence-based analysis with specific data points
- Structure content as analytical reports, not news recaps
- Include comparison data, market context, and actionable conclusions
- Every claim should reference a specific source, study, or dataset
- Assume all provided topics are already approved for publication
- Your job is to deliver clear, data-driven analysis

Output requirements:
- Include data-backed conclusions and practical recommendations
- Provide context: why this matters NOW and what comes next
- Support every claim with evidence or reasoning
- Be specific: use numbers, dates, and named sources

Every topic must clearly answer:
- What does the data show?
- Why does this matter for the reader?
- What should they do or watch for next?

[Length Guide - Brevity is Key!]
- Total: 800-1,100 words (optimized completion rate)
- Each ## section: 120-180 words (core insights only)
- Intro: 80-100 words (strong hook)
- Conclusion: 60-80 words (clear CTA)
- **Finish completely**: No mid-sentence cutoffs

[Key Takeaways Block - MANDATORY]
After the introduction and before the first ## heading, ALWAYS include:
> **Key Takeaways**
> - 3-5 bullet points summarizing key insights
> - Each must be a complete, declarative sentence with specific data
> - Written as standalone statements (quotable by AI search engines)

[Content Principles]
1. First paragraph: Hook with the core finding or data point (1-2 sentences)
2. Structure: Context ‚Üí Data Analysis ‚Üí Comparison ‚Üí Practical Implications ‚Üí Conclusion
3. Tone: Medium/Substack style - conversational, personal, direct
4. SEO: Keyword "{{keyword}}" naturally 4-6 times
5. Sections: 3-4 ## headings (scannable)
6. End: Clear CTA - question or next step

[Writing Style (Required!)]
- Use "you" and "I" sparingly for conversational tone
- Short direct sentences without filler phrases
- Natural transitions without overused connectors
- Avoid AI tell-tale phrases: "Here's the thing", "Sound familiar?", "Look", "Let me explain"
- Strong sentence starters: State facts directly, use specific data points
- Include recent dates, statistics, and concrete examples (2025-2026 data preferred)

[Style - Completion Optimized]
- Active voice, short sentences (1-2 lines)
- Core value only (cut fluff)
- Specific numbers/examples (1-2 selective)
- Bullet points for scannability
- End with punch: "Here's the bottom line."

[Absolutely Avoid]
- Redundancy: repeating same points ‚ùå
- AI tells: "certainly", "it's important to note", "moreover", "furthermore"
- Academic tone: formal, distant language
- Abstract buzzwords: "revolutionary", "game-changer", "cutting-edge"
- Excessive emojis, unnecessary case studies
- Aggro triggers: "shock", "expose", "truth revealed", "jaw-dropping", "unbelievable"

[Headline Patterns - Analytical (Use ONLY these patterns)]
A. Comparison: "[X] vs [Y]: What the Data Shows in [year]"
B. Deep Dive: "Why [topic] Matters: [specific data point]"
C. Market Analysis: "The State of [topic] in [year]: Key Findings"
D. Practical Guide: "How [topic] Changes [outcome]: Data-Driven Analysis"

‚ö†Ô∏è Core: Complete 800-1,100 word article. Plenty of headroom in 12,000 tokens!""",

    "ko": f"""ÎãπÏã†ÏùÄ Jake's Tech Insights Î∏îÎ°úÍ∑∏Ïùò Ï†ÑÎ¨∏ ÏûëÍ∞ÄÏûÖÎãàÎã§.

üìÖ Ï§ëÏöî: Ïò§Îäò ÎÇ†ÏßúÎäî {datetime.now().year}ÎÖÑ {datetime.now().month}Ïõî {datetime.now().day}ÏùºÏûÖÎãàÎã§
ÌòÑÏû¨ Ï†ïÎ≥¥Î•º Ïñ∏Í∏âÌï† Îïå Î∞òÎìúÏãú {datetime.now().year}ÎÖÑÏùÑ ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî. Í≥ºÍ±∞ Ïó∞ÎèÑ ÏÇ¨Ïö© Í∏àÏßÄ.

üéØ ÌïµÏã¨ Î™©Ìëú: 800-1,100 Îã®Ïñ¥Ïùò Í∞ÑÍ≤∞ÌïòÍ≥† ÏûÑÌå©Ìä∏ ÏûàÎäî Í∏Ä ÏûëÏÑ± (Ïï†ÎìúÏÑºÏä§ ÏµúÏ†ÅÌôî)

[Ìé∏Ïßë Î∞©Ïπ® - Î∞òÎìúÏãú ÏàôÏßÄ]
Ïù¥ ÏÇ¨Ïù¥Ìä∏Îäî Ïã¨Ï∏µ Í∏∞Ïà† Î∂ÑÏÑù ÎØ∏ÎîîÏñ¥ÏûÖÎãàÎã§:
- Í∏∞Ïà† Ìä∏Î†åÎìú, SaaS Î∂ÑÏÑù, Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Î¶¨Ìè¨Ìä∏, Í∞úÎ∞úÏûê ÎèÑÍµ¨
- ÎèÖÏûê Í∞ÄÏπòÏôÄ Î∂ÑÏÑùÏùò ÍπäÏù¥Í∞Ä ÌïµÏã¨ Î™©Ìëú
- Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Ï£ºÏû•Í≥º Í∑ºÍ±∞ ÏûàÎäî Î∂ÑÏÑùÏù¥ ÌïÑÏàò

ÎãπÏã†Ïùò Ïó≠Ìï†:
- Íµ¨Ï≤¥Ï†Å Îç∞Ïù¥ÌÑ∞ÏôÄ Í∑ºÍ±∞Î•º Ìè¨Ìï®Ìïú Î∂ÑÏÑù Ï†úÍ≥µ
- Îâ¥Ïä§ ÏöîÏïΩÏù¥ ÏïÑÎãå Î∂ÑÏÑù Î¶¨Ìè¨Ìä∏ ÌòïÌÉúÎ°ú Íµ¨ÏÑ±
- ÎπÑÍµê Îç∞Ïù¥ÌÑ∞, ÏãúÏû• Îß•ÎùΩ, Ïã§Ïö©Ï†Å Í≤∞Î°† Ìè¨Ìï®
- Î™®Îì† Ï£ºÏû•Ïóê Ï∂úÏ≤ò, Ïó∞Íµ¨, Îç∞Ïù¥ÌÑ∞ÏÖã Ï∞∏Ï°∞ ÌïÑÏàò
- Î™®Îì† ÌÜ†ÌîΩÏùÄ Ïù¥ÎØ∏ Ìé∏ÏßëÌåÄÏù¥ ÏäπÏù∏Ìïú Í≤ÉÏúºÎ°ú Í∞ÑÏ£ºÌïòÏÑ∏Ïöî

Ï∂úÎ†• ÏöîÍµ¨ÏÇ¨Ìï≠:
- Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Í≤∞Î°†Í≥º Ïã§Ïö©Ï†Å Í∂åÏû• ÏÇ¨Ìï≠ Ìè¨Ìï®
- Îß•ÎùΩ Ï†úÍ≥µ: Ïôú ÏßÄÍ∏à Ï§ëÏöîÌïúÏßÄ, Îã§ÏùåÏóê Î¨¥ÏóáÏù¥ Ïò¨ÏßÄ
- Î™®Îì† Ï£ºÏû•Ïóê Í∑ºÍ±∞ÎÇò ÎÖºÎ¶¨ Îí∑Î∞õÏπ®
- Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú: Ïà´Ïûê, ÎÇ†Ïßú, Ïã§Î™Ö Ï∂úÏ≤ò ÏÇ¨Ïö©

Î™®Îì† ÌÜ†ÌîΩÏùÄ Î™ÖÌôïÌûà ÎãµÌï¥Ïïº Ìï©ÎãàÎã§:
- Îç∞Ïù¥ÌÑ∞Í∞Ä Î¨¥ÏóáÏùÑ Î≥¥Ïó¨Ï£ºÎäîÍ∞Ä?
- ÎèÖÏûêÏóêÍ≤å Ïôú Ï§ëÏöîÌïúÍ∞Ä?
- Îã§ÏùåÏóê Î¨¥ÏóáÏùÑ Ï£ºÏãúÌï¥Ïïº ÌïòÎäîÍ∞Ä?

[Í∏∏Ïù¥ Í∞ÄÏù¥Îìú - Í∞ÑÍ≤∞Ìï®Ïù¥ ÌïµÏã¨!]
- Ï†ÑÏ≤¥ Í∏Ä: 800-1,100 Îã®Ïñ¥ (ÏôÑÎèÖÎ•† ÏµúÏ†ÅÌôî)
- Í∞Å ## ÏÑπÏÖò: 120-180 Îã®Ïñ¥ (ÌïµÏã¨Îßå Ï†ÑÎã¨)
- ÎèÑÏûÖÎ∂Ä: 80-100 Îã®Ïñ¥ (Í∞ïÎ†•Ìïú ÌõÑÌÇπ)
- Í≤∞Î°†: 60-80 Îã®Ïñ¥ (Î™ÖÌôïÌïú CTA)
- **ÎßàÏßÄÎßâ Î¨∏Ïû•ÍπåÏßÄ Î∞òÎìúÏãú ÏôÑÏÑ±**: ÎÅäÍπÄ ÏóÜÏù¥ ÏôÑÍ≤∞ÌïòÏÑ∏Ïöî

[ÌïµÏã¨ ÏöîÏïΩ Î∏îÎ°ù - ÌïÑÏàò]
ÎèÑÏûÖÎ∂Ä Ïù¥ÌõÑ, Ï≤´ ## Ìó§Îî© Ï†ÑÏóê Î∞òÎìúÏãú Ìè¨Ìï®:
> **ÌïµÏã¨ ÏöîÏïΩ**
> - 3-5Í∞ú ÌïµÏã¨ Ïù∏ÏÇ¨Ïù¥Ìä∏Î•º Î∂àÎ¶øÏúºÎ°ú ÏöîÏïΩ
> - Í∞Å Ìè¨Ïù∏Ìä∏Îäî Íµ¨Ï≤¥Ï†Å Îç∞Ïù¥ÌÑ∞Î•º Ìè¨Ìï®Ìïú ÏôÑÍ≤∞Îêú Î¨∏Ïû•
> - ÏÑ†Ïñ∏Ï†Å Î¨∏Ïû•ÏúºÎ°ú ÏûëÏÑ± (AI Í≤ÄÏÉâ ÏóîÏßÑÏù¥ Ïù∏Ïö© Í∞ÄÎä•Ìïú ÌòïÌÉú)

[ÏΩòÌÖêÏ∏† ÏõêÏπô]
1. Ï≤´ Î¨∏Îã®: ÌïµÏã¨ Î∞úÍ≤¨Ïù¥ÎÇò Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏Î°ú ÌõÑÌÇπ (1-2Î¨∏Ïû•)
2. Íµ¨Ï°∞: Îß•ÎùΩ ‚Üí Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù ‚Üí ÎπÑÍµê ‚Üí Ïã§Ïö©Ï†Å ÏãúÏÇ¨Ï†ê ‚Üí Í≤∞Î°†
3. ÌÜ§: ÌÜ†Ïä§(Toss) Ïä§ÌÉÄÏùº - Ï†ÑÎ¨∏Ï†ÅÏù¥ÏßÄÎßå Ìé∏ÏïàÌïú ÏπúÍµ¨ Í∞ôÏùÄ ÎäêÎÇå
4. SEO: ÌÇ§ÏõåÎìú "{{keyword}}"Î•º ÏûêÏó∞Ïä§ÎüΩÍ≤å 4-6Ìöå Ìè¨Ìï®
5. ÏÑπÏÖò: 3-4Í∞ú ## Ìó§Îî© (Í∞Å ÏÑπÏÖòÏùÄ ÏùΩÍ∏∞ ÏâΩÍ≤å)
6. ÎÅù: Î™ÖÌôïÌïú CTA - ÏßàÎ¨∏Ïù¥ÎÇò Îã§Ïùå Îã®Í≥Ñ Ï†úÏïà

[ÌÜ†Ïä§ Ïä§ÌÉÄÏùº ÎßêÌà¨ (ÌïÑÏàò!)]
- "~Ìï¥Ïöî" Î∞òÎßê Ï°¥ÎåìÎßê ÏÇ¨Ïö© (ÏäµÎãàÎã§/Ìï©ÎãàÎã§ ‚ùå)
- "Ïñ¥Îñ§Í∞ÄÏöî?", "ÌïúÎ≤à Î≥ºÍπåÏöî?", "Í∂ÅÍ∏àÌïòÏßÄ ÏïäÏúºÏÑ∏Ïöî?" Í∞ôÏùÄ ÏπúÍ∑ºÌïú ÏßàÎ¨∏
- "ÏÇ¨Ïã§", "Ïã§Ï†úÎ°ú", "Í∑∏Îü∞Îç∞", "Ï∞∏Í≥†Î°ú" Í∞ôÏùÄ ÏûêÏó∞Ïä§Îü¨Ïö¥ Ï†ëÏÜçÏÇ¨
- Ïà´ÏûêÎ•º ÏπúÍ∑ºÌïòÍ≤å: "10Í∞ú ‚Üí Ïó¥ Í∞ú", "50% ‚Üí Ï†àÎ∞ò", "3Î∞∞ ‚Üí ÏÑ∏ Î∞∞"
- ÏßßÍ≥† Í∞ïÎ†¨Ìïú Î¨∏Ïû•: "ÎÜÄÎûçÏ£†?", "ÎßûÏïÑÏöî.", "Ïù¥Í≤å ÌïµÏã¨Ïù¥ÏóêÏöî."

[Ïä§ÌÉÄÏùº - ÏôÑÎèÖÎ•† ÏµúÏ†ÅÌôî]
- Îä•ÎèôÌÉú ÏúÑÏ£º, ÏßßÏùÄ Î¨∏Ïû• (1-2Ï§Ñ)
- ÌïµÏã¨Îßå Ï†ÑÎã¨ (Î∂àÌïÑÏöîÌïú ÏÑ§Î™Ö Ï†úÍ±∞)
- Íµ¨Ï≤¥Ï†Å Ïà´Ïûê/ÏòàÏãú (1-2Í∞úÎßå ÏÑ†ÌÉùÏ†ÅÏúºÎ°ú)
- Î∂àÎ¶ø Ìè¨Ïù∏Ìä∏ Ï†ÅÍ∑π ÌôúÏö© (Ïä§Ï∫î Í∞ÄÎä•ÌïòÍ≤å)
- Î¨∏Îã® ÎÅù Í∞ïÏ°∞: "Ïôú Í∑∏Îü¥ÍπåÏöî?", "Ïù¥Í≤å ÌïµÏã¨Ïù¥ÏóêÏöî."

[Ï†àÎåÄ Í∏àÏßÄ]
- Ï§ëÏñ∏Î∂ÄÏñ∏: Í∞ôÏùÄ ÎÇ¥Ïö© Î∞òÎ≥µ ‚ùå
- AI Ìã∞: "Î¨ºÎ°†", "~Ìï† Ïàò ÏûàÏäµÎãàÎã§", "~ÌïòÎäî Í≤ÉÏù¥ Ï§ëÏöîÌï©ÎãàÎã§"
- Îî±Îî±Ìïú Î¨∏Ï≤¥: "~ÏäµÎãàÎã§/~Ìï©ÎãàÎã§" (Ìï¥ÏöîÏ≤¥Îßå!)
- Ï∂îÏÉÅÏ†Å ÌëúÌòÑ: "ÌòÅÏã†Ï†Å", "Í≤åÏûÑÏ≤¥Ïù∏Ï†Ä", "Ï£ºÎ™©Ìï† ÎßåÌïú"
- Í≥ºÎèÑÌïú Ïù¥Î™®ÏßÄ, Î∂àÌïÑÏöîÌïú ÏÇ¨Î°Ä ÎÇòÏó¥
- Ïñ¥Í∑∏Î°ú Îã®Ïñ¥: "Ï∂©Í≤©", "Ìè≠Î°ú", "Ïã§Ï≤¥", "ÏßÑÏã§", "ÏÜåÎ¶Ñ", "Ï∂©Í≤©Ï†Å", "ÏôÑÎ≤Ω Ï†ïÎ¶¨", "Ìïú Î≤àÏóê Ïù¥Ìï¥"

[Ìó§ÎìúÎùºÏù∏ Ìå®ÌÑ¥ - Î∂ÑÏÑùÌòï (Ïù¥ Ìå®ÌÑ¥Îßå ÏÇ¨Ïö©)]
A. ÎπÑÍµê: "[X] vs [Y]: Îç∞Ïù¥ÌÑ∞Í∞Ä Î≥¥Ïó¨Ï£ºÎäî [year]ÎÖÑ ÌòÑÌô©"
B. Ïã¨Ï∏µ Î∂ÑÏÑù: "[Ï£ºÏ†ú]Í∞Ä Ï§ëÏöîÌïú Ïù¥Ïú†: [Íµ¨Ï≤¥Ï†Å Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏]"
C. ÏãúÏû• Î∂ÑÏÑù: "[year]ÎÖÑ [Ï£ºÏ†ú] ÌòÑÌô©: ÌïµÏã¨ Î∞úÍ≤¨"
D. Ïã§Ï†Ñ Í∞ÄÏù¥Îìú: "[Ï£ºÏ†ú]Í∞Ä [Í≤∞Í≥º]Î•º Î∞îÍæ∏Îäî Î∞©Î≤ï: Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò Î∂ÑÏÑù"

‚ö†Ô∏è ÌïµÏã¨: 800-1,100 Îã®Ïñ¥Î°ú ÏôÑÍ≤∞Îêú Í∏ÄÏùÑ ÏûëÏÑ±ÌïòÏÑ∏Ïöî. 12,000 ÌÜ†ÌÅ∞ ÎÇ¥ÏóêÏÑú Ïó¨Ïú†ÏûàÍ≤å!"""
}


class ContentGenerator:
    def __init__(self, api_key: Optional[str] = None, unsplash_key: Optional[str] = None):
        """Initialize content generator with Claude API and Unsplash API"""
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            safe_print("‚ùå ERROR: ANTHROPIC_API_KEY not found")
            safe_print("   Please set it as environment variable or pass to constructor")
            safe_print("   Example: export ANTHROPIC_API_KEY='your-key-here'")
            raise ValueError(
                "ANTHROPIC_API_KEY not found. Set it as environment variable or pass to constructor."
            )

        # Initialize with Prompt Caching beta header
        try:
            self.client = Anthropic(
                api_key=self.api_key,
                default_headers={
                    "anthropic-beta": "prompt-caching-2024-07-31"
                }
            )
            self.model = "claude-sonnet-4-5-20250929"
            safe_print("  ‚úì Anthropic API client initialized successfully")
        except Exception as e:
            safe_print(f"‚ùå ERROR: Failed to initialize Anthropic client: {mask_secrets(str(e))}")
            raise

        # Unsplash API (optional)
        self.unsplash_key = unsplash_key or os.environ.get("UNSPLASH_ACCESS_KEY")
        if self.unsplash_key:
            safe_print("  üñºÔ∏è  Unsplash API enabled")
        else:
            safe_print("  ‚ö†Ô∏è  Unsplash API key not found (images will be skipped)")
            safe_print("     Set UNSPLASH_ACCESS_KEY environment variable to enable")

        # Initialize A/B Test Manager
        self.ab_test_manager = ABTestManager()
        safe_print("  üß™ A/B Test Manager initialized")

    def generate_draft(self, topic: Dict) -> tuple[str, str]:
        """Generate initial draft using Draft Agent with Prompt Caching

        Returns:
            tuple: (draft_content, content_type)
        """
        keyword = topic['keyword']
        lang = topic['lang']
        category = topic['category']
        references = topic.get('references', [])  # Get references from topic

        # Classify content type
        classifier = ContentClassifier()
        keywords = [keyword]  # Use topic keyword
        content_type = classifier.classify(keyword, keywords, category)

        safe_print(f"  üéØ Content type: {content_type}")
        safe_print(f"  üìù Generating draft for: {keyword}")

        # Get type-specific prompt
        if content_type == 'tutorial':
            user_prompt = get_tutorial_prompt(keyword, keywords, lang)
        elif content_type == 'analysis':
            user_prompt = get_analysis_prompt(keyword, keywords, lang)
        else:  # news
            user_prompt = get_news_prompt(keyword, keywords, lang)

        # Append references if available
        if references and len(references) > 0:
            user_prompt += "\n\n## References (Use for factual accuracy)\n\n"
            for i, ref in enumerate(references[:3], 1):  # Use top 3
                user_prompt += f"{i}. {ref['title']}\n   URL: {ref['url']}\n"
                if ref.get('snippet'):
                    user_prompt += f"   Summary: {ref['snippet']}\n"
                user_prompt += "\n"
            user_prompt += "Use these references for factual information, but DO NOT plagiarize. Write in your own words.\n"

        system_prompt = SYSTEM_PROMPTS[lang].format(keyword=keyword)

        # Use Prompt Caching: cache the system prompt
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=12000,
                system=[
                    {
                        "type": "text",
                        "text": system_prompt,
                        "cache_control": {"type": "ephemeral"}
                    }
                ],
                messages=[{
                    "role": "user",
                    "content": user_prompt
                }]
            )
        except Exception as e:
            error_msg = mask_secrets(str(e))
            safe_print(f"  ‚ùå ERROR: API call failed during draft generation")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {error_msg}")
            raise

        if not response or not response.content:
            safe_print(f"  ‚ùå ERROR: Empty response from API")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            raise ValueError("Empty response from Claude API")

        draft = response.content[0].text

        # Log cache performance
        usage = response.usage
        cache_read = getattr(usage, 'cache_read_input_tokens', 0)
        cache_create = getattr(usage, 'cache_creation_input_tokens', 0)

        # Always show cache status
        if cache_read > 0:
            safe_print(f"  üíæ Cache HIT: {cache_read} tokens saved!")
        elif cache_create > 0:
            safe_print(f"  üíæ Cache created: {cache_create} tokens")
        else:
            safe_print(f"  ‚ÑπÔ∏è  No caching (usage: input={usage.input_tokens}, output={usage.output_tokens})")

        safe_print(f"  ‚úì Draft generated ({len(draft)} chars)")
        return draft, content_type

    def edit_draft(self, draft: str, topic: Dict, content_type: str = 'analysis') -> str:
        """Refine draft using Editor Agent with Prompt Caching

        Args:
            draft: Draft content to edit
            topic: Topic dictionary
            content_type: Type of content (tutorial/analysis/news)

        Returns:
            str: Edited content
        """
        lang = topic['lang']

        safe_print(f"  ‚úèÔ∏è  Editing draft...")

        if not draft or len(draft.strip()) == 0:
            safe_print(f"  ‚ö†Ô∏è  WARNING: Empty draft provided for editing")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            raise ValueError("Cannot edit empty draft")

        editor_prompt = self._get_editor_prompt(lang, content_type)

        # Use Prompt Caching: cache the editor instructions
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=12000,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": editor_prompt,
                                "cache_control": {"type": "ephemeral"}
                            },
                            {
                                "type": "text",
                                "text": f"\n\n---\n\n{draft}"
                            }
                        ]
                    }
                ]
            )
        except Exception as e:
            error_msg = mask_secrets(str(e))
            safe_print(f"  ‚ùå ERROR: API call failed during draft editing")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            safe_print(f"     Draft length: {len(draft)} chars")
            safe_print(f"     Error: {error_msg}")
            raise

        if not response or not response.content:
            safe_print(f"  ‚ùå ERROR: Empty response from editing API")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            raise ValueError("Empty response from Claude API during editing")

        edited = response.content[0].text

        # Log cache performance
        usage = response.usage
        cache_read = getattr(usage, 'cache_read_input_tokens', 0)
        cache_create = getattr(usage, 'cache_creation_input_tokens', 0)

        # Always show cache status
        if cache_read > 0:
            safe_print(f"  üíæ Cache HIT: {cache_read} tokens saved!")
        elif cache_create > 0:
            safe_print(f"  üíæ Cache created: {cache_create} tokens")
        else:
            safe_print(f"  ‚ÑπÔ∏è  No caching (usage: input={usage.input_tokens}, output={usage.output_tokens})")

        safe_print(f"  ‚úì Draft edited ({len(edited)} chars)")
        return edited

    def _get_draft_prompt(self, keyword: str, category: str, lang: str, references: List[Dict] = None) -> str:
        """Get draft generation prompt based on language"""
        # Get current date in KST
        from datetime import datetime, timezone, timedelta
        kst = timezone(timedelta(hours=9))
        today = datetime.now(kst)
        current_date = today.strftime("%YÎÖÑ %mÏõî %dÏùº")  # Korean format
        current_date_en = today.strftime("%B %d, %Y")  # English format
        current_year = today.year

        # Format references for prompt
        refs_section = ""
        if references and len(references) > 0:
            refs_list = "\n".join([
                f"- [{ref.get('title', 'Source')}]({ref.get('url', '')}) - {ref.get('source', '')}"
                for ref in references[:3]
            ])
            refs_section = f"\n\nüìö USE THESE REFERENCES:\n{refs_list}\n"

        prompts = {
            "en": f"""üìÖ TODAY'S DATE: {current_date_en}
‚ö†Ô∏è IMPORTANT: You are writing this article as of TODAY ({current_date_en}). All information must be current as of {current_year}. Do NOT use outdated information from 2024 or earlier years.

Write a comprehensive blog post about: {keyword}{refs_section}

Category: {category}

‚è±Ô∏è Reading Time Target: 4-5 minutes
- Write 3-4 main sections (## headings)
- Each section: 1-2 minutes to read, one key point
- Short paragraphs (2-4 sentences each)
- End with a thought-provoking question

üéØ HOOKING STRATEGY (Critical!):
1. **Opening Hook** (First 2-3 sentences):
   - Start with a PROBLEM SITUATION that readers face
   - Use empathy: "You adopted X, but employees don't use it..."
   - Include specific failure stat: "60% of X projects fail because..."
   - NOT generic intro like "X is becoming popular..."

2. **Real Success/Failure Cases**:
   - Include 1-2 SPECIFIC company/person examples
   - "A shopping mall tried X for everything and failed, but when they focused on Y..."
   - Show what DOESN'T work, not just what works
   - Avoid abstract: "Many companies..." ‚Üí Use: "One e-commerce startup..."

3. **Limitations & Pitfalls**:
   - Dedicate 1 section to "When X Actually Hurts"
   - "In these 3 situations, X is counterproductive..."
   - This makes content feel authentic and trustworthy

4. **Data-Driven**:
   - Include 2-3 specific statistics (even if approximate)
   - "2024 survey shows 60% failure rate..."
   - "Companies saw 35% productivity increase..."

Content Guidelines:
- Target audience: Decision-makers seeking practical advice
- Focus on "What to avoid" as much as "What to do"
- Concrete examples over abstract concepts
- Mention current trends (2025-2026)
- Be concise and impactful - avoid unnecessary explanations

üìö REFERENCES SECTION:
- If references were provided above in the prompt, you MUST add a "## References" section at the end
- Use those EXACT URLs - do not modify or create new ones
- Format: `- [Source Title](URL) - Organization/Publisher`
- Example:
  ## References
  - [The State of AI in 2025](https://example.com/ai-report) - McKinsey & Company
  - [Remote Work Statistics 2025](https://example.com/remote) - Buffer
- **IMPORTANT**: If NO references were provided above, DO NOT add a References section at all

**This section is REQUIRED for all posts - even Entertainment/Society topics!**

Write the complete blog post now (body only, no title or metadata):""",

            "ko": f"""üìÖ Ïò§Îäò ÎÇ†Ïßú: {current_date}
‚ö†Ô∏è Ï§ëÏöî: Ïù¥ Í∏ÄÏùÄ Ïò§Îäò({current_date}) Í∏∞Ï§ÄÏúºÎ°ú ÏûëÏÑ±Ìï©ÎãàÎã§. Î™®Îì† Ï†ïÎ≥¥Îäî {current_year}ÎÖÑ ÌòÑÏû¨Î•º Í∏∞Ï§ÄÏúºÎ°ú Ìï¥Ïïº Ìï©ÎãàÎã§. 2024ÎÖÑ Ïù¥ÌïòÏùò Ïò§ÎûòÎêú Ï†ïÎ≥¥Î•º ÏÇ¨Ïö©ÌïòÏßÄ ÎßàÏÑ∏Ïöî.

Îã§Ïùå Ï£ºÏ†úÎ°ú Ìè¨Í¥ÑÏ†ÅÏù∏ Î∏îÎ°úÍ∑∏ Í∏ÄÏùÑ ÏûëÏÑ±ÌïòÏÑ∏Ïöî: {keyword}{refs_section}

Ïπ¥ÌÖåÍ≥†Î¶¨: {category}

‚è±Ô∏è ÏùΩÍ∏∞ ÏãúÍ∞Ñ Î™©Ìëú: 4-5Î∂Ñ
- 3-4Í∞úÏùò Ï£ºÏöî ÏÑπÏÖò (## Ìó§Îî©) ÏûëÏÑ±
- Í∞Å ÏÑπÏÖò: 1-2Î∂Ñ ÏùΩÍ∏∞ Î∂ÑÎüâ, ÌïòÎÇòÏùò ÌïµÏã¨ Ìè¨Ïù∏Ìä∏
- ÏßßÏùÄ Î¨∏Îã® ÏÇ¨Ïö© (2-4 Î¨∏Ïû•Ïî©)
- ÏÉùÍ∞ÅÏùÑ ÏûêÍ∑πÌïòÎäî ÏßàÎ¨∏ÏúºÎ°ú ÎßàÎ¨¥Î¶¨

üéØ ÌõÑÌÇπ Ï†ÑÎûµ (ÌïÑÏàò!):
1. **Ïò§ÌîÑÎãù ÌõÑÌÇπ** (Ï≤´ 2-3Î¨∏Ïû•):
   - ÎèÖÏûêÍ∞Ä ÏßÅÎ©¥Ìïú Î¨∏Ï†ú ÏÉÅÌô©ÏúºÎ°ú ÏãúÏûë
   - Í≥µÍ∞ê Ïú†ÎèÑ: "ÌöåÏÇ¨ÏóêÏÑú XÎ•º ÎèÑÏûÖÌñàÎäîÎç∞ ÏßÅÏõêÎì§Ïù¥ Ïì∞ÏßÄ ÏïäÍ≥†..."
   - Íµ¨Ï≤¥Ï†Å Ïã§Ìå® ÌÜµÍ≥Ñ Ìè¨Ìï®: "X ÌîÑÎ°úÏ†ùÌä∏Ïùò 60%Í∞Ä Ïã§Ìå®ÌïòÎäî Ïù¥Ïú†Îäî..."
   - ÏùºÎ∞òÏ†Å ÏãúÏûë Í∏àÏßÄ: "XÍ∞Ä Ïù∏Í∏∞Î•º ÎÅåÍ≥† ÏûàÏäµÎãàÎã§..." ‚ùå

2. **Ïã§Ï†ú ÏÑ±Í≥µ/Ïã§Ìå® ÏÇ¨Î°Ä**:
   - Íµ¨Ï≤¥Ï†ÅÏù∏ ÌöåÏÇ¨/ÏÇ¨Îûå ÏÇ¨Î°Ä 1-2Í∞ú Ìè¨Ìï®
   - "Ìïú ÏáºÌïëÎ™∞ÏùÄ XÎ•º Î™®Îì† Í≤ÉÏóê Ï†ÅÏö©ÌñàÎã§Í∞Ä Ïã§Ìå®ÌñàÏßÄÎßå, YÏóêÎßå ÏßëÏ§ëÌïòÎãàÍπå..."
   - Ïïà ÎêòÎäî Í≤ÉÎèÑ Î≥¥Ïó¨Ï£ºÍ∏∞ (ÏÑ±Í≥µÎßå ÎßêÌïòÏßÄ ÎßêÍ∏∞)
   - Ï∂îÏÉÅÏ†Å ÌëúÌòÑ Í∏àÏßÄ: "ÎßéÏùÄ Í∏∞ÏóÖÎì§..." ‚Üí "Ìïú Ïä§ÌÉÄÌä∏ÏóÖÏùÄ..." ‚úÖ

3. **ÌïúÍ≥ÑÏ†êÍ≥º Ìï®Ï†ï**:
   - "XÍ∞Ä Ïò§ÌûàÎ†§ Ïó≠Ìö®Í≥ºÏù∏ Í≤ΩÏö∞" ÏÑπÏÖò 1Í∞ú Ìï†Ïï†
   - "Ïù¥ 3Í∞ÄÏßÄ ÏÉÅÌô©ÏóêÏÑúÎäî XÍ∞Ä ÎπÑÌö®Ïú®Ï†Å..."
   - Ïù¥Í≤ÉÏù¥ ÏßÑÏ†ïÏÑ±Í≥º Ïã†Î¢∞Î•º ÎßåÎì¶

4. **Îç∞Ïù¥ÌÑ∞ Í∏∞Î∞ò**:
   - Íµ¨Ï≤¥Ï†Å ÌÜµÍ≥Ñ 2-3Í∞ú Ìè¨Ìï® (ÎåÄÎûµÏ†ÅÏù¥Ïñ¥ÎèÑ OK)
   - "2024ÎÖÑ Ï°∞ÏÇ¨Ïóê Îî∞Î•¥Î©¥ 60% Ïã§Ìå®Ïú®..."
   - "Í∏∞ÏóÖÎì§Ïù¥ 35% ÏÉùÏÇ∞ÏÑ± Ï¶ùÍ∞Ä Í≤ΩÌóò..."

ÏΩòÌÖêÏ∏† Í∞ÄÏù¥ÎìúÎùºÏù∏:
- ÎåÄÏÉÅ ÎèÖÏûê: Ïã§Ïö©Ï†Å Ï°∞Ïñ∏ÏùÑ Ï∞æÎäî ÏùòÏÇ¨Í≤∞Ï†ïÏûê
- "ÌîºÌï¥Ïïº Ìï† Í≤É"ÏùÑ "Ìï¥Ïïº Ìï† Í≤É"ÎßåÌÅº Í∞ïÏ°∞
- Ï∂îÏÉÅÏ†Å Í∞úÎÖêÎ≥¥Îã§ Íµ¨Ï≤¥Ï†Å ÏòàÏãú
- ÌòÑÏû¨ Ìä∏Î†åÎìú Ïñ∏Í∏â (2025-2026ÎÖÑ)
- Í∞ÑÍ≤∞ÌïòÍ≥† ÏûÑÌå©Ìä∏ ÏûàÍ≤å - Î∂àÌïÑÏöîÌïú ÏÑ§Î™Ö Ï†úÍ±∞

üìö Ï∞∏Í≥†ÏûêÎ£å ÏÑπÏÖò:
- ÏúÑ ÌîÑÎ°¨ÌîÑÌä∏Ïóê Ï∞∏Í≥†ÏûêÎ£åÍ∞Ä Ï†úÍ≥µÎêú Í≤ΩÏö∞, Î∞òÎìúÏãú Í∏Ä ÎßàÏßÄÎßâÏóê "## Ï∞∏Í≥†ÏûêÎ£å" ÏÑπÏÖò Ï∂îÍ∞Ä
- Ï†úÍ≥µÎêú URLÏùÑ Ï†ïÌôïÌûà ÏÇ¨Ïö© - ÏàòÏ†ïÌïòÍ±∞ÎÇò ÏÉàÎ°ú ÎßåÎì§ÏßÄ Îßê Í≤É
- ÌòïÏãù: `- [Ï∂úÏ≤ò Ï†úÎ™©](URL) - Ï°∞ÏßÅ/Ï∂úÌåêÏÇ¨`
- ÏòàÏãú:
  ## Ï∞∏Í≥†ÏûêÎ£å
  - [2025 AI ÌòÑÌô© Î≥¥Í≥†ÏÑú](https://example.com/ai-report) - Îß•ÌÇ®ÏßÄÏï§Ïª¥ÌçºÎãà
  - [ÏõêÍ≤© Í∑ºÎ¨¥ ÌÜµÍ≥Ñ 2025](https://example.com/remote) - Buffer
- **Ï§ëÏöî**: ÏúÑÏóê Ï∞∏Í≥†ÏûêÎ£åÍ∞Ä Ï†úÍ≥µÎêòÏßÄ ÏïäÏïòÎã§Î©¥, Ï∞∏Í≥†ÏûêÎ£å ÏÑπÏÖòÏùÑ Ï†àÎåÄ Ï∂îÍ∞ÄÌïòÏßÄ ÎßàÏÑ∏Ïöî

ÏßÄÍ∏à Î∞îÎ°ú ÏôÑÏ†ÑÌïú Î∏îÎ°úÍ∑∏ Í∏ÄÏùÑ ÏûëÏÑ±ÌïòÏÑ∏Ïöî (Î≥∏Î¨∏Îßå, Ï†úÎ™©Ïù¥ÎÇò Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï†úÏô∏):"""
        }

        return prompts[lang]

    def _get_editor_prompt(self, lang: str, content_type: str = 'analysis') -> str:
        """Get editor prompt based on language and content type

        Args:
            lang: Language code (en/ko)
            content_type: Content type (tutorial/analysis/news)

        Returns:
            str: Editor prompt with type-specific length targets
        """
        # Get type-specific word count targets
        classifier = ContentClassifier()
        config = classifier.get_config(content_type, lang)
        min_count, max_count = config['word_count']

        # Format length requirements based on language
        if lang == 'ko':
            count_unit = 'Í∏ÄÏûê'
            length_req = f"""üìè Í∏∏Ïù¥ ÏöîÍµ¨ÏÇ¨Ìï≠ (CRITICAL - Î∞òÎìúÏãú Ï§ÄÏàò):
üéØ Î™©Ìëú Î≤îÏúÑ: {min_count:,}-{max_count:,}{count_unit}

**Ï†àÎåÄ Í∑úÏπô**:
- Ï¥àÏïàÏù¥ {int(min_count*0.8):,}{count_unit} ÎØ∏Îßå: ÏòàÏãú/ÏÑ§Î™Ö Ï∂îÍ∞ÄÎ°ú ÏµúÏÜå {min_count:,}{count_unit} Ïù¥ÏÉÅ ÌôïÏû•
- Ï¥àÏïàÏù¥ {min_count:,}-{max_count:,}{count_unit}: Í∏∏Ïù¥ Ï†àÎåÄ Ïú†ÏßÄ (Ïù¥ÏÉÅÏ†Å Î≤îÏúÑ - ÏïïÏ∂ï Í∏àÏßÄ!)
- Ï¥àÏïàÏù¥ {int(max_count*1.3):,}{count_unit} Ïù¥ÏÉÅ: Ï§ëÎ≥µÎßå Ï†úÍ±∞ÌïòÏó¨ {max_count:,}{count_unit} Í∑ºÏ≤òÎ°ú Ï°∞Ï†ï

‚ö†Ô∏è  Í≤ΩÍ≥†: Ïù¥ÏÉÅÏ†Å Î≤îÏúÑ({min_count:,}-{max_count:,}{count_unit})Ïóê ÏûàÏúºÎ©¥ Ï†àÎåÄ Ï§ÑÏù¥ÏßÄ ÎßàÏÑ∏Ïöî!"""
        else:
            length_req = f"""üìè Length Requirements (CRITICAL - Must Follow):
üéØ Target Range: {min_count:,}-{max_count:,} words

**Absolute Rules**:
- If draft is under {int(min_count*0.8):,} words: EXPAND with examples/explanations to reach at least {min_count:,} words
- If draft is {min_count:,}-{max_count:,} words: MAINTAIN exact length (ideal range - DO NOT compress!)
- If draft is over {int(max_count*1.3):,} words: Remove only redundancy to reach near {max_count:,} words

‚ö†Ô∏è  Warning: If draft is in ideal range ({min_count:,}-{max_count:,} words), DO NOT shorten it!"""

        prompts = {
            "en": f"""You are an expert editor. Transform this into Medium-style content with authentic human touch:

{length_req}

üéØ CRITICAL ENHANCEMENTS:
1. **Strengthen Opening Hook**:
   - If opening is generic, rewrite to start with problem/pain point
   - Add empathy: "You've been there, right?"
   - Make it personal and relatable

2. **Add Authenticity Markers** (NO personal anecdotes):
   - Use authoritative references: "Industry reports show...", "According to recent data..."
   - Add failure acknowledgment: "This approach can fail when..."
   - Show balanced perspective: "This isn't always the answer..."
   - AVOID: "In my experience...", "I spoke with...", "I thought..." (credibility issues on anonymous blogs)

3. **Enhance Examples**:
   - Make vague examples specific: "Many companies" ‚Üí "One fintech startup" or "A Silicon Valley tech company"
   - Add concrete details: numbers, outcomes, timelines
   - Include what went WRONG, not just success stories
   - AVOID: "I worked with", "I spoke to" ‚Üí Use: "Case studies show", "Reports indicate"

4. **Balance Perspective**:
   - Ensure there's a "When this doesn't work" section
   - Add nuance: "This works IF...", "But in these cases..."
   - Avoid absolute claims: "always", "never", "guaranteed"

Tasks:
1. **Medium style conversion**: Add "you/I", conversational tone
2. **Eliminate all AI tells**: "certainly", "moreover", "it's important to note"
3. **Natural connectors**: "Look", "Here's why", "The truth is"
4. **Break fourth wall**: "You might be thinking...", "Sound familiar?"
5. **Punchy sentences**: "Here's the thing.", "Let me explain.", "Stop it."
6. **Smooth transitions**: "Now", "Here's where it gets interesting"
7. Keep all factual information intact
8. **Complete ending**: Finish conclusion fully
9. **Preserve Key Takeaways block**: Do not remove or restructure the blockquote Key Takeaways. Improve wording only.

Return improved version (body only, no title):""",

            "ko": f"""ÎãπÏã†ÏùÄ Ï†ÑÎ¨∏ ÏóêÎîîÌÑ∞ÏûÖÎãàÎã§. Ïù¥ Î∏îÎ°úÍ∑∏ Í∏ÄÏùÑ ÏßÑÏßú ÏÇ¨ÎûåÏù¥ Ïì¥ Í≤É Í∞ôÏùÄ ÌÜ†Ïä§ Ïä§ÌÉÄÏùºÎ°ú Í∞úÏÑ†ÌïòÏÑ∏Ïöî:

{length_req}

üéØ ÌïµÏã¨ Í∞úÏÑ†ÏÇ¨Ìï≠:
1. **Ïò§ÌîÑÎãù Í∞ïÌôî**:
   - ÏùºÎ∞òÏ†Å ÏãúÏûëÏù¥Î©¥ Î¨∏Ï†ú/Í≥†ÎØº ÏÉÅÌô©ÏúºÎ°ú Ïû¨ÏûëÏÑ±
   - Í≥µÍ∞ê Ï∂îÍ∞Ä: "Ïù¥Îü∞ Í≤ΩÌóò ÏûàÏúºÏãúÏ£†?"
   - Í∞úÏù∏Ï†ÅÏù¥Í≥† Í≥µÍ∞ê Í∞ÄÎä•ÌïòÍ≤å

2. **Ï†ïÎ≥¥ Î∞ÄÎèÑ ÏµúÏö∞ÏÑ†** (ÌïúÍµ≠ ÎèÖÏûê = Îπ†Î•∏ Ï†ïÎ≥¥ ÏÑ†Ìò∏):
   - ÌïµÏã¨ Ï†ïÎ≥¥ Î®ºÏ†Ä: ÏàòÏπò, Îã®Í≥Ñ, Î∞©Î≤ï
   - Ïã§Ïö© Ï†ïÎ≥¥ Ï¶âÏãú Ï†úÍ≥µ: "Í≥ÑÏÇ∞Î≤ï: 1) ~ 2) ~"
   - "ÏùòÏô∏Î°ú...", "ÎÜÄÎûçÍ≤åÎèÑ..." Í∞ôÏùÄ ÏûêÏó∞Ïä§Îü¨Ïö¥ ÌëúÌòÑ
   - ÌïúÍ≥Ñ Ïñ∏Í∏â: "Ìï≠ÏÉÅ ÎãµÏùÄ ÏïÑÎãàÏóêÏöî..."

3. **ÏòàÏãú Íµ¨Ï≤¥Ìôî** (Í∞úÏù∏ Í≤ΩÌóò Î∞∞Ï†ú):
   - Ï∂îÏÉÅÏ†Å ÏòàÏãúÎ•º Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú: "ÎßéÏùÄ ÌöåÏÇ¨Îì§" ‚Üí "Ìïú ÌïÄÌÖåÌÅ¨ Ïä§ÌÉÄÌä∏ÏóÖÏùÄ" ÎòêÎäî "ÌÜ†Ïä§Ïùò Í≤ΩÏö∞"
   - Íµ¨Ï≤¥Ï†Å ÎîîÌÖåÏùº: Ïà´Ïûê, Í≤∞Í≥º, ÌÉÄÏûÑÎùºÏù∏
   - Ïã§Ìå®Ìïú Í≤ÉÎèÑ Ìè¨Ìï®: ÏÑ±Í≥µÎßå ÎßêÌïòÏßÄ ÎßêÍ∏∞
   - ÌîºÌï† Í≤É: "Ï†ú Í≤ΩÌóòÏÉÅ", "Ï†úÍ∞Ä Î¥§ÏùÑ Îïå" ‚Üí ÎåÄÏã†: "ÏÇ¨Î°Ä Ïó∞Íµ¨Ïóê Îî∞Î•¥Î©¥", "Îç∞Ïù¥ÌÑ∞Îäî Î≥¥Ïó¨Ï§çÎãàÎã§"

4. **Í∑†ÌòïÏû°Ìûå Í¥ÄÏ†ê**:
   - "Ïù¥Îü∞ Í≤ΩÏö∞Ïóî Ïïà ÌÜµÌï¥Ïöî" ÏÑπÏÖò ÌôïÏù∏/Ï∂îÍ∞Ä
   - ÎâòÏïôÏä§: "Ïù¥Í≤å ÌÜµÌïòÎ†§Î©¥...", "ÌïòÏßÄÎßå Ïù¥Îü∞ Í≤ΩÏö∞Ïóî..."
   - Ï†àÎåÄÏ†Å ÌëúÌòÑ ÌîºÌïòÍ∏∞: "Ìï≠ÏÉÅ", "Ï†àÎåÄ", "Î¨¥Ï°∞Í±¥"

ÏûëÏóÖ:
1. **ÌÜ†Ïä§ ÎßêÌà¨Î°ú Î≥ÄÌôò**: "~ÏäµÎãàÎã§" ‚Üí "~Ìï¥Ïöî", ÏπúÍ∑ºÌïú ÏßàÎ¨∏Ìòï Ï∂îÍ∞Ä
2. AI ÎäêÎÇå ÏôÑÏ†Ñ Ï†úÍ±∞: "Î¨ºÎ°†", "~Ìï† Ïàò ÏûàÏäµÎãàÎã§", "Ï§ëÏöîÌï©ÎãàÎã§" Î™®Îëê ÏÇ≠Ï†ú
3. ÏûêÏó∞Ïä§Îü¨Ïö¥ Ï†ëÏÜçÏÇ¨: "ÏÇ¨Ïã§", "Ïã§Ï†úÎ°ú", "Í∑∏Îü∞Îç∞", "Ï∞∏Í≥†Î°ú"
4. Ïà´ÏûêÎ•º ÏπúÍ∑ºÌïòÍ≤å: "50% ‚Üí Ï†àÎ∞ò", "3Î∞∞ ‚Üí ÏÑ∏ Î∞∞"
5. ÏßßÍ≥† Í∞ïÎ†¨Ìïú Î¨∏Ïû• Ï∂îÍ∞Ä: "ÎÜÄÎûçÏ£†?", "ÎßûÏïÑÏöî.", "Ïù¥Í≤å ÌïµÏã¨Ïù¥ÏóêÏöî."
6. ÏÑπÏÖò Í∞Ñ Îß§ÎÅÑÎü¨Ïö¥ Ï†ÑÌôò: "Ïûê, Ïù¥Ï†ú ~", "Í∑∏Îüº ~"
7. Î™®Îì† ÏÇ¨Ïã§ Ï†ïÎ≥¥Îäî Í∑∏ÎåÄÎ°ú Ïú†ÏßÄ
8. **ÎßàÏßÄÎßâ Î¨∏Ïû•ÍπåÏßÄ ÏôÑÍ≤∞**: Í≤∞Î°†ÏùÑ Î∞òÎìúÏãú ÏôÑÏÑ±
9. **ÌïµÏã¨ ÏöîÏïΩ Î∏îÎ°ù Ïú†ÏßÄ**: Key Takeaways Î∏îÎ°ùÏùÑ Ï†úÍ±∞ÌïòÍ±∞ÎÇò Íµ¨Ï°∞ Î≥ÄÍ≤ΩÌïòÏßÄ Îßê Í≤É. Î¨∏Íµ¨Îßå Í∞úÏÑ†.

Í∞úÏÑ†Îêú Î≤ÑÏ†ÑÏùÑ Î∞òÌôòÌïòÏÑ∏Ïöî (Î≥∏Î¨∏Îßå, Ï†úÎ™© Ï†úÏô∏):"""
        }

        return prompts[lang]

    def generate_title(self, content: str, keyword: str, lang: str, references: List[Dict] = None) -> str:
        """Generate SEO-friendly title based on actual content and references"""
        # Get current year in KST
        from datetime import datetime, timezone, timedelta
        kst = timezone(timedelta(hours=9))
        current_year = datetime.now(kst).year

        # Extract strategic samples from content for better context
        # Take beginning (intro), middle (main content), and end (conclusion)
        content_length = len(content)
        if content_length <= 1200:
            content_preview = content
        else:
            # Get first 500, middle 400, last 300 chars
            beginning = content[:500]
            middle_start = content_length // 2 - 200
            middle = content[middle_start:middle_start + 400]
            ending = content[-300:]
            content_preview = f"{beginning}\n\n[...middle section...]\n{middle}\n\n[...conclusion...]\n{ending}"

        # Format references if available
        refs_context = ""
        if references and len(references) > 0:
            refs_list = "\n".join([
                f"- {ref.get('title', 'Source')}"
                for ref in references[:3]
            ])
            refs_context = f"\n\nREFERENCE TOPICS:\n{refs_list}\n"

        prompts = {
            "en": f"Generate a factual, SEO-friendly blog title (50-60 chars) for this post about '{keyword}'.\n\nCONTENT SAMPLES (beginning, middle, end):\n{content_preview}{refs_context}\n\nCRITICAL RULES - VIOLATION WILL FAIL:\n1. Title MUST describe what the content ACTUALLY discusses (not what sounds catchy)\n2. NO exaggeration, speculation, or clickbait (e.g., \"confirmed\", \"revealed\", \"secret\")\n3. If content is about \"how to watch\", title must say \"how to watch\" (not \"rankings\")\n4. If content discusses problems/issues, title must reflect that (not promise solutions)\n5. ONLY use facts explicitly stated in the content samples\n6. Do NOT promise specific numbers/data unless clearly stated in content\n7. Include keyword '{keyword}' naturally\n8. Current year is {current_year}\n9. Return ONLY the title",
            "ko": f"'{keyword}'Ïóê ÎåÄÌïú ÏÇ¨Ïã§Ï†ÅÏù¥Í≥† SEO ÏπúÌôîÏ†ÅÏù∏ Ï†úÎ™©ÏùÑ ÏÉùÏÑ±ÌïòÏÑ∏Ïöî (50-60Ïûê).\n\nÎ≥∏Î¨∏ ÏÉòÌîå (ÏãúÏûë, Ï§ëÍ∞Ñ, ÎÅù):\n{content_preview}{refs_context}\n\nÌïµÏã¨ Í∑úÏπô - ÏúÑÎ∞ò Ïãú Ïã§Ìå®:\n1. Ï†úÎ™©ÏùÄ Î≥∏Î¨∏Ïù¥ Ïã§Ï†úÎ°ú Îã§Î£®Îäî ÎÇ¥Ïö©ÏùÑ ÏÑ§Î™ÖÌï¥Ïïº Ìï® (Îß§Î†•Ï†ÅÏúºÎ°ú Îì§Î¶¨Îäî Í≤ÉÏù¥ ÏïÑÎãò)\n2. Í≥ºÏû•, Ï∂îÏ∏°, ÌÅ¥Î¶≠Î≤†Ïù¥Ìä∏ Í∏àÏßÄ (Ïòà: \"ÌôïÏ†ï\", \"Ìè≠Î°ú\", \"Ï∂©Í≤©\")\n3. Î≥∏Î¨∏Ïù¥ \"ÏãúÏ≤≠ Î∞©Î≤ï\"Ïóê ÎåÄÌïú Í≤ÉÏù¥Î©¥ Ï†úÎ™©ÎèÑ \"ÏãúÏ≤≠ Î∞©Î≤ï\"Ïù¥Ïñ¥Ïïº Ìï® (\"Îû≠ÌÇπ\" ÏïÑÎãò)\n4. Î≥∏Î¨∏Ïù¥ Î¨∏Ï†úÏ†êÏùÑ ÎÖºÏùòÌïòÎ©¥ Ï†úÎ™©ÎèÑ Í∑∏Í≤ÉÏùÑ Î∞òÏòÅÌï¥Ïïº Ìï® (Ìï¥Í≤∞Ï±Ö ÏïΩÏÜç Í∏àÏßÄ)\n5. Î≥∏Î¨∏ ÏÉòÌîåÏóê Î™ÖÏãúÏ†ÅÏúºÎ°ú Ïñ∏Í∏âÎêú ÏÇ¨Ïã§Îßå ÏÇ¨Ïö©\n6. Î≥∏Î¨∏Ïóê Î™ÖÌôïÌûà ÎÇòÏôÄÏûàÏßÄ ÏïäÏúºÎ©¥ Íµ¨Ï≤¥Ï†Å Ïà´Ïûê/Îç∞Ïù¥ÌÑ∞ ÏïΩÏÜç Í∏àÏßÄ\n7. '{keyword}' ÌÇ§ÏõåÎìúÎ•º ÏûêÏó∞Ïä§ÎüΩÍ≤å Ìè¨Ìï®\n8. ÌòÑÏû¨ Ïó∞ÎèÑÎäî {current_year}ÎÖÑ\n9. Ï†úÎ™©Îßå Î∞òÌôò"
        }

        response = self.client.messages.create(
            model=self.model,
            max_tokens=100,
            messages=[{
                "role": "user",
                "content": prompts[lang]
            }]
        )

        generated_title = response.content[0].text.strip().strip('"').strip("'")

        # Validate title-content alignment (STRICT check for critical mismatches only)
        validation_prompts = {
            "en": f"Does this title accurately match what the content ACTUALLY discusses?\n\nTITLE: {generated_title}\n\nCONTENT: {content_preview}\n\nCheck for CRITICAL mismatches ONLY:\n- Title promises specific data (e.g., \"$800\", \"75%\") but content doesn't provide it\n- Title says \"how to watch\" but content discusses problems/history instead\n- Title says \"confirmed\" but content is speculation/rumors\n- Title topic completely different from content topic\n\nIGNORE minor issues like:\n- Year mentions (2026 is acceptable for future-looking content)\n- Slight emphasis differences\n- Language mixing (Korean keyword in English title is OK)\n\nAnswer 'yes' if title reasonably matches content. Answer 'no' ONLY for critical mismatches. If no, explain in max 15 words.",
            "ko": f"Ïù¥ Ï†úÎ™©Ïù¥ Î≥∏Î¨∏Ïù¥ Ïã§Ï†úÎ°ú ÎÖºÏùòÌïòÎäî ÎÇ¥Ïö©Í≥º Ï†ïÌôïÌûà ÏùºÏπòÌï©ÎãàÍπå?\n\nÏ†úÎ™©: {generated_title}\n\nÎ≥∏Î¨∏: {content_preview}\n\nÏπòÎ™ÖÏ†Å Î∂àÏùºÏπòÎßå ÌôïÏù∏:\n- Ï†úÎ™©Ïù¥ Íµ¨Ï≤¥Ï†Å Îç∞Ïù¥ÌÑ∞(Ïòà: \"75%\", \"$800\")Î•º ÏïΩÏÜçÌïòÏßÄÎßå Î≥∏Î¨∏Ïóê ÏóÜÏùå\n- Ï†úÎ™©ÏùÄ \"ÏãúÏ≤≠ Î∞©Î≤ï\"Ïù∏Îç∞ Î≥∏Î¨∏ÏùÄ Î¨∏Ï†úÏ†ê/Ïó≠ÏÇ¨Î•º ÎÖºÏùò\n- Ï†úÎ™©ÏùÄ \"ÌôïÏ†ï\"Ïù∏Îç∞ Î≥∏Î¨∏ÏùÄ Ï∂îÏ∏°/ÏÜåÎ¨∏\n- Ï†úÎ™© Ï£ºÏ†úÏôÄ Î≥∏Î¨∏ Ï£ºÏ†úÍ∞Ä ÏôÑÏ†ÑÌûà Îã§Î¶Ñ\n\nÎ¨¥ÏãúÌï† ÏÇ¨ÏÜåÌïú Î¨∏Ï†ú:\n- Ïó∞ÎèÑ Ïñ∏Í∏â (ÎØ∏Îûò ÏßÄÌñ•Ï†Å ÏΩòÌÖêÏ∏†Ïóê 2026 ÏÇ¨Ïö© Í∞ÄÎä•)\n- ÏïΩÍ∞ÑÏùò Í∞ïÏ°∞ Ï∞®Ïù¥\n- Ïñ∏Ïñ¥ ÌòºÏö© (ÏòÅÏñ¥ Ï†úÎ™©Ïóê ÌïúÍµ≠Ïñ¥ ÌÇ§ÏõåÎìú ÏÇ¨Ïö© Í∞ÄÎä•)\n\nÏ†úÎ™©Ïù¥ Î≥∏Î¨∏Í≥º Ìï©Î¶¨Ï†ÅÏúºÎ°ú ÏùºÏπòÌïòÎ©¥ 'Ïòà'. ÏπòÎ™ÖÏ†Å Î∂àÏùºÏπòÎßå 'ÏïÑÎãàÏò§'. ÏïÑÎãàÏò§ÎùºÎ©¥ 15Îã®Ïñ¥ Ïù¥ÎÇ¥ ÏÑ§Î™Ö."
        }

        validation_response = self.client.messages.create(
            model=self.model,
            max_tokens=50,
            messages=[{
                "role": "user",
                "content": validation_prompts[lang]
            }]
        )

        validation_result = validation_response.content[0].text.strip().lower()

        # If validation fails, regenerate title with strict instructions
        if not validation_result.startswith('yes') and not validation_result.startswith('Ïòà'):
            safe_print(f"  ‚ö†Ô∏è  Title-content mismatch detected: {validation_result}")
            safe_print(f"     Original title: {generated_title}")
            safe_print(f"  üîÑ Regenerating title with strict content alignment...")

            # Regenerate with stricter prompt
            regenerate_prompts = {
                "en": f"Generate a title that EXACTLY matches what this content discusses. Do NOT promise specifics that aren't in the content. Do NOT use words like 'confirmed', 'breaking', or future dates unless explicitly stated.\n\nContent preview:\n{content_preview}\n\nKeyword to include: {keyword}\n\nTitle (60-70 chars):",
                "ko": f"Î≥∏Î¨∏Ïù¥ Ïã§Ï†úÎ°ú Îã§Î£®Îäî ÎÇ¥Ïö©Í≥º Ï†ïÌôïÌûà ÏùºÏπòÌïòÎäî Ï†úÎ™©ÏùÑ ÏÉùÏÑ±ÌïòÏÑ∏Ïöî. Î≥∏Î¨∏Ïóê ÏóÜÎäî Íµ¨Ï≤¥Ï†Å ÎÇ¥Ïö©ÏùÑ ÏïΩÏÜçÌïòÏßÄ ÎßàÏÑ∏Ïöî. 'ÌôïÏ†ï', 'ÏÜçÎ≥¥', ÎØ∏Îûò ÎÇ†ÏßúÎäî Î≥∏Î¨∏Ïóê Î™ÖÏãúÎêòÏßÄ ÏïäÏúºÎ©¥ ÏÇ¨Ïö©ÌïòÏßÄ ÎßàÏÑ∏Ïöî.\n\nÎ≥∏Î¨∏ ÎØ∏Î¶¨Î≥¥Í∏∞:\n{content_preview}\n\nÌè¨Ìï®Ìï† ÌÇ§ÏõåÎìú: {keyword}\n\nÏ†úÎ™© (40-50Ïûê):"
            }

            regenerate_response = self.client.messages.create(
                model=self.model,
                max_tokens=100,
                messages=[{
                    "role": "user",
                    "content": regenerate_prompts[lang]
                }]
            )

            generated_title = regenerate_response.content[0].text.strip().strip('"').strip("'")
            safe_print(f"  ‚úì Regenerated title: {generated_title}")

        return generated_title

    def generate_description(self, content: str, keyword: str, lang: str) -> str:
        """Generate meta description optimized for SEO (120-160 chars)"""
        prompts = {
            "en": f"Generate a compelling meta description for a blog post about '{keyword}'.\n\nREQUIREMENTS:\n- Length: EXACTLY 120-160 characters (strict)\n- Include keyword naturally\n- Action-oriented and engaging\n- NO quotes, NO marketing fluff\n\nReturn ONLY the description, nothing else.",
            "ko": f"'{keyword}'Ïóê ÎåÄÌïú Î∏îÎ°úÍ∑∏ Í∏ÄÏùò Î©îÌÉÄ ÏÑ§Î™ÖÏùÑ ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.\n\nÏöîÍµ¨ÏÇ¨Ìï≠:\n- Í∏∏Ïù¥: Ï†ïÌôïÌûà 120-160Ïûê (ÏóÑÍ≤©)\n- ÌÇ§ÏõåÎìú ÏûêÏó∞Ïä§ÎüΩÍ≤å Ìè¨Ìï®\n- ÌñâÎèô ÏßÄÌñ•Ï†ÅÏù¥Í≥† Îß§Î†•Ï†ÅÏúºÎ°ú\n- Îî∞Ïò¥Ìëú ÏóÜÏù¥, ÎßàÏºÄÌåÖ Î¨∏Íµ¨ Í∏àÏßÄ\n\nÏÑ§Î™ÖÎßå Î∞òÌôòÌïòÏÑ∏Ïöî."
        }

        response = self.client.messages.create(
            model=self.model,
            max_tokens=100,
            messages=[{
                "role": "user",
                "content": prompts[lang]
            }]
        )

        return response.content[0].text.strip().strip('"').strip("'")

    def generate_faq(self, content: str, keyword: str, lang: str) -> list:
        """Generate 3-5 FAQ pairs based on article content for FAQPage schema"""
        prompts = {
            "en": f"""Based on this blog post about '{keyword}', generate 3-5 FAQ pairs that someone might search for.

RULES:
- Questions must be natural search queries (how people actually type in Google)
- Answers must be 2-3 sentences, factual, directly answering the question
- Include the keyword '{keyword}' naturally in at least 2 questions
- Each answer should be self-contained (understandable without reading the article)

Return ONLY a JSON array:
[
  {{"question": "...", "answer": "..."}},
  {{"question": "...", "answer": "..."}}
]

Article content:
{content[:3000]}""",
            "ko": f"""Ïù¥ '{keyword}' Î∏îÎ°úÍ∑∏ Í∏ÄÏùÑ Í∏∞Î∞òÏúºÎ°ú 3-5Í∞ú FAQ ÏåçÏùÑ ÏÉùÏÑ±ÌïòÏÑ∏Ïöî.

Í∑úÏπô:
- ÏßàÎ¨∏ÏùÄ ÏûêÏó∞Ïä§Îü¨Ïö¥ Í≤ÄÏÉâ ÏøºÎ¶¨Ïó¨Ïïº Ìï® (Ïã§Ï†ú Íµ¨Í∏ÄÏóê ÌÉÄÏù¥ÌïëÌïòÎäî ÌòïÌÉú)
- ÎãµÎ≥ÄÏùÄ 2-3Î¨∏Ïû•, ÏÇ¨Ïã§Ï†Å, ÏßàÎ¨∏Ïóê ÏßÅÏ†ë ÎãµÎ≥Ä
- ÏµúÏÜå 2Í∞ú ÏßàÎ¨∏Ïóê '{keyword}' ÌÇ§ÏõåÎìú ÏûêÏó∞Ïä§ÎüΩÍ≤å Ìè¨Ìï®
- Í∞Å ÎãµÎ≥ÄÏùÄ ÎèÖÎ¶ΩÏ†ÅÏúºÎ°ú Ïù¥Ìï¥ Í∞ÄÎä•Ìï¥Ïïº Ìï®

JSON Î∞∞Ïó¥Îßå Î∞òÌôò:
[
  {{"question": "...", "answer": "..."}},
  {{"question": "...", "answer": "..."}}
]

Í∏Ä ÎÇ¥Ïö©:
{content[:3000]}"""
        }

        response = self.client.messages.create(
            model=self.model,
            max_tokens=1500,
            messages=[{"role": "user", "content": prompts[lang]}]
        )

        text = response.content[0].text
        json_match = re.search(r'\[[\s\S]*\]', text)
        if json_match:
            faq_items = json.loads(json_match.group())
            return faq_items[:5]
        return []

    def extract_technologies(self, content: str, keyword: str) -> list:
        """Extract technology names mentioned in content for taxonomy"""
        known_techs = [
            'Python', 'JavaScript', 'TypeScript', 'React', 'Next.js', 'Vue.js',
            'Angular', 'Node.js', 'Django', 'FastAPI', 'Flask', 'Docker',
            'Kubernetes', 'AWS', 'Azure', 'GCP', 'PostgreSQL', 'MongoDB',
            'Redis', 'GraphQL', 'REST API', 'Claude', 'GPT', 'OpenAI',
            'Anthropic', 'LangChain', 'Vercel', 'Terraform', 'GitHub Actions',
            'Linux', 'Rust', 'Go', 'Swift', 'Kotlin', 'Java', 'C++',
            'WebAssembly', 'Cloudflare', 'Figma', 'Tailwind CSS', 'Hugo',
            'Webpack', 'Vite', 'Supabase', 'Firebase', 'Stripe', 'Notion',
            'Slack', 'VS Code', 'ChatGPT', 'Gemini', 'Copilot', 'Cursor',
            'TensorFlow', 'PyTorch', 'Stable Diffusion', 'Midjourney',
            'Ollama', 'Hugging Face', 'Mistral', 'Llama', 'DALL-E', 'Sora'
        ]

        found = []
        content_lower = content.lower()
        for tech in known_techs:
            if tech.lower() in content_lower:
                found.append(tech)

        return found[:5]

    def translate_to_english(self, text: str) -> str:
        """Translate non-English keywords to English for Unsplash search"""
        # Simple keyword translations for common tech/business/lifestyle terms
        translations = {
            # Korean
            'Ï±óÎ¥á': 'chatbot', 'AI': 'artificial intelligence', 'ÎèÑÏûÖ': 'implementation',
            'Ïã§Ìå®': 'failure', 'Ïù¥Ïú†': 'reasons', 'ÎÖ∏ÏΩîÎìú': 'no-code', 'Ìà¥': 'tool',
            'ÌïúÍ≥ÑÏ†ê': 'limitations', 'Ïû¨ÌÉùÍ∑ºÎ¨¥': 'remote work', 'ÌïòÏù¥Î∏åÎ¶¨Îìú': 'hybrid',
            'Í∑ºÎ¨¥': 'work', 'Ìö®Ïú®ÏÑ±': 'efficiency', 'MZÏÑ∏ÎåÄ': 'gen z', 'Í¥ÄÎ¶¨': 'management',
            'Î∞©Î≤ï': 'method', 'ÏÇ¨Î°Ä': 'case', 'ÎØ∏ÎãàÎ©Ä': 'minimal', 'ÎùºÏù¥ÌîÑ': 'lifestyle',
            'Ï§ëÎã®': 'quit', 'ÏÉùÏÇ∞ÏÑ±': 'productivity', 'ÌåÅ': 'tips',
        }

        # Split and translate each word
        words = text.split()
        translated_words = []
        for word in words:
            # Try exact match first
            if word in translations:
                translated_words.append(translations[word])
            else:
                # Check if word contains any translatable substring
                found = False
                for kr, en in translations.items():
                    if kr in word:
                        translated_words.append(en)
                        found = True
                        break
                if not found:
                    # Keep as-is if ASCII (likely already English)
                    try:
                        word.encode('ascii')
                        translated_words.append(word)
                    except UnicodeEncodeError:
                        pass  # Skip non-ASCII untranslatable words

        return ' '.join(translated_words) if translated_words else 'technology'

    def fetch_featured_image(self, keyword: str, category: str) -> Optional[Dict]:
        """Fetch featured image from Unsplash API"""
        if not self.unsplash_key:
            return None

        try:
            # Clean keyword for better Unsplash search
            # Remove years (2020-2030) to avoid year-specific images
            import re
            clean_keyword = re.sub(r'20[2-3][0-9]ÎÖÑ?', '', keyword)  # Match years + optional ÎÖÑ (Korean year)
            # Remove common prefixes/suffixes that reduce search quality
            clean_keyword = re.sub(r'„Äê.*?„Äë', '', clean_keyword)  # Remove „Äêbrackets„Äë
            clean_keyword = re.sub(r'\[.*?\]', '', clean_keyword)  # Remove [brackets]
            clean_keyword = clean_keyword.strip()

            # Translation dictionary for meaningful keywords
            keyword_translations = {
                # Korean - AI/Jobs/Employment
                'AI': 'artificial intelligence',
                'Ïù∏Í≥µÏßÄÎä•': 'artificial intelligence',
                'ÎåÄÏ≤¥': 'replacement automation',
                'ÏùºÏûêÎ¶¨': 'job employment work',
                'Ïã§ÏóÖ': 'unemployment jobless',
                'ÏßÅÏóÖ': 'occupation career profession',
                'Ï∑®ÏóÖ': 'employment hiring recruitment',
                'ÏûêÎèôÌôî': 'automation robot',
                'Í∏∞Ïà†': 'technology tech',
                'ÎîîÏßÄÌÑ∏': 'digital technology',
                'Î°úÎ¥á': 'robot automation',
                'ÎØ∏Îûò': 'future',
                'Î≥ÄÌôî': 'change transformation',
                'ÏúÑÌóò': 'risk danger',
                # Korean - Finance/Business
                'ÎÇòÎùºÏÇ¨ÎûëÏπ¥Îìú': 'patriot card credit card',
                'Ïπ¥Îìú': 'card credit',
                'Ïó∞Î†π': 'age limit',
                'Ï†úÌïú': 'restriction limit',
                'Ï†ÑÏÑ∏': 'housing lease deposit',
                'Î≥¥Ï¶ùÍ∏à': 'deposit guarantee',
                'Î∞∞Îã¨': 'delivery food',
                'ÏàòÏàòÎ£å': 'fee commission',
                'ÏûêÏòÅÏóÖ': 'small business owner',
                'ÌèêÏóÖ': 'business closure bankruptcy',
                'ÏßÄÏõêÍ∏à': 'subsidy support fund',
                'Ï†ïÎ∂Ä': 'government policy',
                'Ïã†Ï≤≠': 'application registration',
                'ÌòúÌÉù': 'benefit advantage',
                # Korean - Entertainment/Society
                'ÏÇ¨Í≥ºÎ¨∏': 'apology statement',
                'Ìå¨': 'fan supporter',
                'Îì±ÎèåÎ¶º': 'backlash criticism',
                'Ïä§ÎßàÌä∏Ìè∞': 'smartphone mobile',
                'Í±¥Í∞ï': 'health wellness'
            }

            # Extract meaningful keywords from title
            title_words = clean_keyword.split()
            translated_keywords = []

            # Try to find and translate key phrases
            for ko_word, en_translation in keyword_translations.items():
                if ko_word in clean_keyword:
                    translated_keywords.append(en_translation)

            # If no translation found, extract meaningful words (skip common noise and non-ASCII)
            if not translated_keywords:
                noise_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for']
                for word in title_words[:3]:  # Take first 3 words
                    # Filter out non-ASCII words to prevent non-English queries
                    try:
                        word.encode('ascii')
                        is_ascii = True
                    except UnicodeEncodeError:
                        is_ascii = False

                    if is_ascii and len(word) > 2 and word.lower() not in noise_words:
                        translated_keywords.append(word)

            # Add category context (more specific than before)
            category_context = {
                'tech': ['technology', 'digital innovation', 'tech workspace', 'coding', 'software'],
                'business': ['business', 'professional office', 'meeting', 'strategy', 'corporate'],
                'finance': ['finance', 'money investment', 'stock market', 'banking', 'wealth'],
                'society': ['society', 'community people', 'social gathering', 'urban life', 'culture'],
                'entertainment': ['entertainment', 'cinema theater', 'music concert', 'art gallery', 'performance'],
                'lifestyle': ['lifestyle', 'daily life', 'home interior', 'wellness', 'travel'],
                'sports': ['sports', 'athletic training', 'stadium', 'competition', 'fitness'],
                'education': ['education', 'learning classroom', 'study', 'books', 'school']
            }

            # Build flexible, contextual query with fallback strategies
            if translated_keywords:
                base_keywords = ' '.join(translated_keywords[:2])
                # Primary search: specific keywords + main category term
                context_list = category_context.get(category, ['technology'])
                context = context_list[0]
                query = f"{base_keywords} {context}".strip()
                # Store fallback queries
                fallback_queries = [f"{base_keywords} {ctx}" for ctx in context_list[1:3]]
                fallback_queries.append(context_list[0])  # Pure category as last resort
            else:
                # Fallback to category-specific queries if no English keywords found
                context_list = category_context.get(category, ['technology'])
                query = context_list[0]
                fallback_queries = context_list[1:3]

            # Unsplash API endpoint
            url = "https://api.unsplash.com/search/photos"
            headers = {
                "Authorization": f"Client-ID {self.unsplash_key}"
            }
            params = {
                "query": query,
                "per_page": 30,  # Increased from 5 to 30 for larger image pool
                "orientation": "landscape"
            }

            safe_print(f"  üîç Searching Unsplash for: {query}")

            # Use certifi for SSL verification (Windows compatibility)
            verify_ssl = certifi.where() if certifi else True
            response = requests.get(url, headers=headers, params=params, timeout=10, verify=verify_ssl)
            response.raise_for_status()

            data = response.json()

            # Load used images tracking file
            used_images_file = Path(__file__).parent.parent / "data" / "used_images.json"
            used_images_meta_file = Path(__file__).parent.parent / "data" / "used_images_metadata.json"

            # Load metadata (tracks when each image was used)
            used_images_meta = {}
            if used_images_meta_file.exists():
                try:
                    with open(used_images_meta_file, 'r') as f:
                        used_images_meta = json.load(f)
                except:
                    pass

            # Clean up images older than 30 days
            from datetime import datetime, timedelta
            current_time = datetime.now().timestamp()
            cutoff_time = (datetime.now() - timedelta(days=30)).timestamp()

            cleaned_meta = {}
            for img_id, timestamp in used_images_meta.items():
                # Skip corrupted entries (non-numeric timestamps)
                if not isinstance(timestamp, (int, float)):
                    continue
                if timestamp > cutoff_time:
                    cleaned_meta[img_id] = timestamp

            # Update set of used images (only keep recent ones)
            used_images = set(cleaned_meta.keys())

            # Save cleaned metadata
            if cleaned_meta != used_images_meta:
                used_images_meta_file.parent.mkdir(parents=True, exist_ok=True)
                with open(used_images_meta_file, 'w') as f:
                    json.dump(cleaned_meta, f, indent=2)
                if len(used_images_meta) > len(cleaned_meta):
                    safe_print(f"  üóëÔ∏è  Cleaned up {len(used_images_meta) - len(cleaned_meta)} images older than 30 days")

            used_images_meta = cleaned_meta

            # Find first unused image from results
            photo = None
            if data.get('results'):
                for result in data['results']:
                    image_id = result['id']
                    if image_id not in used_images:
                        photo = result
                        used_images.add(image_id)
                        used_images_meta[image_id] = current_time
                        break
            else:
                safe_print(f"  ‚ö†Ô∏è  No images found for '{query}'")

            # If no results or all images are used, try fallback queries
            if photo is None and fallback_queries:
                for fallback_query in fallback_queries:
                    safe_print(f"  ‚ö†Ô∏è  No unused images for '{query}', trying: {fallback_query}")
                    params['query'] = fallback_query

                    response = requests.get(url, headers=headers, params=params, timeout=10, verify=verify_ssl)
                    response.raise_for_status()
                    data = response.json()

                    if data.get('results'):
                        for result in data['results']:
                            image_id = result['id']
                            if image_id not in used_images:
                                photo = result
                                used_images.add(image_id)
                                used_images_meta[image_id] = current_time
                                safe_print(f"  ‚úì Found unused image with fallback: {fallback_query}")
                                break

                    # If found, stop trying fallbacks
                    if photo is not None:
                        break

                # If still no unused image found, return None (use placeholder)
                if photo is None:
                    safe_print(f"  ‚ùå No unused images available after trying all fallbacks for category '{category}'")
                    return None

            # Save used images (legacy file for backward compatibility)
            used_images_file.parent.mkdir(parents=True, exist_ok=True)
            with open(used_images_file, 'w') as f:
                json.dump(list(used_images), f)

            # Save metadata with timestamps
            with open(used_images_meta_file, 'w') as f:
                json.dump(used_images_meta, f, indent=2)

            image_info = {
                'url': photo['urls']['regular'],
                'download_url': photo['links']['download_location'],
                'photographer': photo['user']['name'],
                'photographer_url': photo['user']['links']['html'],
                'unsplash_url': photo['links']['html'],
                'image_id': photo['id']
            }

            safe_print(f"  ‚úì Found image by {image_info['photographer']}")
            return image_info

        except requests.exceptions.Timeout as e:
            safe_print(f"  ‚ö†Ô∏è  Unsplash API timeout: Request took too long")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None
        except requests.exceptions.HTTPError as e:
            safe_print(f"  ‚ö†Ô∏è  Unsplash API HTTP error: {e.response.status_code if e.response else 'unknown'}")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None
        except requests.exceptions.RequestException as e:
            safe_print(f"  ‚ö†Ô∏è  Unsplash API network error")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None
        except json.JSONDecodeError as e:
            safe_print(f"  ‚ö†Ô∏è  Unsplash API response parsing failed")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: Invalid JSON response")
            return None
        except Exception as e:
            safe_print(f"  ‚ö†Ô∏è  Image fetch failed with unexpected error")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None

    def download_image(self, image_info: Dict, keyword: str) -> Optional[str]:
        """Download optimized image to static/images/ directory with hash-based duplicate detection"""
        if not image_info:
            return None

        try:
            # Create images directory
            images_dir = Path("static/images")
            images_dir.mkdir(parents=True, exist_ok=True)

            # Generate filename
            slug = keyword.lower()
            # Allow Unicode characters (CJK) in filenames
            slug = ''.join(c if c.isalnum() or c.isspace() or ord(c) > 127 else '' for c in slug)
            slug = slug.replace(' ', '-')[:30]
            # Use KST for image filename
            from datetime import timezone, timedelta
            kst = timezone(timedelta(hours=9))
            date_str = datetime.now(kst).strftime("%Y%m%d")
            filename = f"{date_str}-{slug}.jpg"
            filepath = images_dir / filename

            # Trigger Unsplash download tracking (required by API terms)
            if image_info.get('download_url'):
                verify_ssl = certifi.where() if certifi else True
                requests.get(
                    image_info['download_url'],
                    headers={"Authorization": f"Client-ID {self.unsplash_key}"},
                    timeout=5,
                    verify=verify_ssl
                )

            # Download optimized image (1200px width, quality 85)
            # Use Unsplash's regular URL which already includes optimization
            download_url = image_info.get('url', '')
            # Add additional optimization parameters
            if '?' in download_url:
                optimized_url = f"{download_url}&w=1200&q=85&fm=jpg"
            else:
                optimized_url = f"{download_url}?w=1200&q=85&fm=jpg"

            safe_print(f"  üì• Downloading optimized image (1200px, q85)...")
            # Use certifi for SSL verification (Windows compatibility)
            verify_ssl = certifi.where() if certifi else True
            response = requests.get(optimized_url, timeout=15, verify=verify_ssl)
            response.raise_for_status()

            # Calculate MD5 hash of downloaded content
            import hashlib
            content_hash = hashlib.md5(response.content).hexdigest()

            # Check for duplicate images by hash
            duplicate_found = False
            for existing_file in images_dir.glob("*.jpg"):
                if existing_file.exists():
                    try:
                        with open(existing_file, 'rb') as f:
                            existing_hash = hashlib.md5(f.read()).hexdigest()
                        if existing_hash == content_hash:
                            safe_print(f"  ‚ö†Ô∏è  Duplicate image detected (same content as {existing_file.name})")
                            duplicate_found = True
                            break
                    except:
                        pass

            # If duplicate found, skip saving but still return the new filename
            # (The image will be saved with a new name to maintain unique URLs)
            if duplicate_found:
                safe_print(f"  ‚ÑπÔ∏è  Saving with new filename to maintain unique URL: {filename}")

            # Save image (even if duplicate, to maintain URL uniqueness per post)
            with open(filepath, 'wb') as f:
                f.write(response.content)

            size_kb = len(response.content) / 1024
            safe_print(f"  ‚úì Image saved: {filepath} ({size_kb:.1f} KB)")

            # Log hash for debugging
            safe_print(f"  üîë Image hash: {content_hash[:8]}...")

            # Return relative path for Hugo
            return f"/images/{filename}"

        except requests.exceptions.Timeout as e:
            safe_print(f"  ‚ö†Ô∏è  Image download timeout")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     URL: {optimized_url[:80]}...")
            return None
        except requests.exceptions.HTTPError as e:
            safe_print(f"  ‚ö†Ô∏è  Image download HTTP error: {e.response.status_code if e.response else 'unknown'}")
            safe_print(f"     Keyword: {keyword}")
            return None
        except IOError as e:
            safe_print(f"  ‚ö†Ô∏è  File system error during image save")
            safe_print(f"     Path: {filepath}")
            safe_print(f"     Error: {str(e)}")
            return None
        except Exception as e:
            safe_print(f"  ‚ö†Ô∏è  Image download failed with unexpected error")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None

    def save_post(self, topic: Dict, title: str, description: str, content: str, image_path: Optional[str] = None, image_credit: Optional[Dict] = None, faq_items: Optional[list] = None, technologies: Optional[list] = None) -> Path:
        """Save post to Hugo content directory"""
        lang = topic['lang']
        category = topic['category']
        keyword = topic['keyword']

        # Generate filename from keyword
        slug = keyword.lower()
        # Remove special characters, keep alphanumeric and spaces
        slug = ''.join(c if c.isalnum() or c.isspace() else '' for c in slug)
        slug = slug.replace(' ', '-')[:50]

        # Create directory
        content_dir = Path(f"content/{lang}/{category}")
        content_dir.mkdir(parents=True, exist_ok=True)

        # Generate filename with date in KST
        from datetime import timezone, timedelta
        kst = timezone(timedelta(hours=9))
        date_str = datetime.now(kst).strftime("%Y-%m-%d")
        filename = f"{date_str}-{slug}.md"
        filepath = content_dir / filename

        # A/B Testing Integration (50% chance)
        import random
        ab_test_id = None
        ab_variant = None

        if self.ab_test_manager.should_run_test("title_style"):
            # Generate post ID for consistent assignment
            post_id = f"{date_str}-{slug}"

            # Assign variant
            ab_variant = self.ab_test_manager.assign_variant(post_id, "title_style")

            if ab_variant:
                ab_test_id = "title_style"

                # Generate title variants
                title_variants = self.ab_test_manager.generate_title_variants(title, lang)

                # Apply variant title
                if ab_variant in title_variants:
                    original_title = title
                    title = title_variants[ab_variant]
                    safe_print(f"  üß™ A/B Test: title_style (Variant {ab_variant})")
                    safe_print(f"     Original: {original_title}")
                    safe_print(f"     Modified: {title}")

        # Hugo frontmatter with required image field
        # Use placeholder if no Unsplash image available
        if not image_path:
            # Use category-based placeholder
            image_path = f"/images/placeholder-{category}.jpg"

        # Use KST timezone for date
        from datetime import timezone, timedelta
        kst = timezone(timedelta(hours=9))
        now_kst = datetime.now(kst)

        # Escape nested quotes in YAML frontmatter
        safe_title = title.replace('"', "'")
        safe_description = description.replace('"', "'")

        # Build frontmatter with optional A/B test metadata
        frontmatter_lines = [
            "---",
            f'title: "{safe_title}"',
            f'date: {now_kst.strftime("%Y-%m-%dT%H:%M:%S%z")}',
            "draft: false",
            'author: "Jake Park"',
            f'categories: ["{category}"]',
            f'tags: {json.dumps(keyword.split()[:3])}',
            f'description: "{safe_description}"',
            f'image: "{image_path}"'
        ]

        # Add A/B test metadata if applicable
        if ab_test_id and ab_variant:
            frontmatter_lines.append(f'ab_test_id: "{ab_test_id}"')
            frontmatter_lines.append(f'ab_variant: "{ab_variant}"')

        # Add technologies taxonomy for tech posts
        if technologies and category == 'tech':
            frontmatter_lines.append(f'technologies: {json.dumps(technologies)}')

        # Add FAQ items for FAQPage schema
        if faq_items:
            frontmatter_lines.append("faq:")
            for item in faq_items:
                safe_q = item.get('question', '').replace('"', "'")
                safe_a = item.get('answer', '').replace('"', "'")
                frontmatter_lines.append(f'  - question: "{safe_q}"')
                frontmatter_lines.append(f'    answer: "{safe_a}"')

        frontmatter_lines.extend(["---", ""])

        frontmatter = "\n".join(frontmatter_lines) + "\n"

        # Hero image removed - PaperMod theme renders frontmatter image: as cover
        hero_image = ""

        # Add image credit at the end of content if available
        credit_line = ""
        if image_credit:
            credit_line = f"\n\n---\n\n*Photo by [{image_credit['photographer']}]({image_credit['photographer_url']}) on [Unsplash]({image_credit['unsplash_url']})*\n"

        # Validate References section and remove if it contains fake URLs
        def has_fake_reference_url(url: str) -> bool:
            """Check if URL is a fake reference"""
            fake_patterns = [
                r'example\.com',
                r'example\.org',
                r'\.gov/[a-z-]+-202[0-9]',
                r'\.org/[a-z-]+-survey',
                r'\.gov/[a-z-]+-compliance',
                r'\.gov/[a-z-]+-report',
            ]
            for pattern in fake_patterns:
                if re.search(pattern, url, re.IGNORECASE):
                    return True
            return False

        # Check if References section exists - if not, just skip it (don't add fake references)
        ref_headers = {
            'en': '## References',
            'ko': '## Ï∞∏Í≥†ÏûêÎ£å'
        }
        ref_header = ref_headers.get(lang, '## References')

        # First, normalize any non-standard reference formats to standard format
        # Remove bold "**References:**" format if exists (common Claude output)
        bold_ref_patterns = [
            (r'\*\*References?:\*\*\n', ''),  # **References:**
            (r'\*\*Ï∞∏Í≥†ÏûêÎ£å:\*\*\n', '')  # **Ï∞∏Í≥†ÏûêÎ£å:**
        ]
        for pattern, replacement in bold_ref_patterns:
            content = re.sub(pattern, replacement, content)

        # Extract References section if exists
        has_references = ref_header in content or '## Reference' in content or '## Ï∞∏Í≥†' in content

        if has_references:
            # Extract URLs from References section using regex
            # Pattern: [text](url) or bare URLs
            url_pattern = r'https?://[^\s\)\]<>"]+'  
            urls_in_content = re.findall(url_pattern, content)

            # Check if any URLs are fake
            fake_urls = [url for url in urls_in_content if has_fake_reference_url(url)]

            if fake_urls:
                safe_print(f"  ‚ö†Ô∏è  Fake reference URLs detected: {len(fake_urls)} found")
                safe_print(f"      Examples: {fake_urls[:3]}")

                # Remove References section entirely
                # Match from any References header to the next ## header or end of content
                ref_pattern = r'\n## (?:References?|Ï∞∏Í≥†ÏûêÎ£å)\n.*?(?=\n## |\Z)'
                content = re.sub(ref_pattern, '', content, flags=re.DOTALL)
                safe_print(f"  üóëÔ∏è  Removed References section with fake URLs")
                has_references = False  # Mark as no valid references
            else:
                safe_print(f"  ‚úÖ References section validated ({len(urls_in_content)} URLs)")

        # If no valid References section exists, add from queue
        if not has_references and topic.get('references'):
            references = topic['references']
            safe_print(f"  ‚ÑπÔ∏è  No References section in content, adding from queue ({len(references)} refs)")

            # Build References section
            ref_section = f"\n\n{ref_header}\n\n"
            for i, ref in enumerate(references, 1):
                ref_section += f"{i}. [{ref['title']}]({ref['url']})\n"

            # Append to content
            content = content.rstrip() + ref_section
            safe_print(f"  ‚úÖ Added {len(references)} references from queue")
        elif not has_references:
            safe_print(f"  ‚ÑπÔ∏è  No references available (neither in content nor queue)")

        # Add affiliate links if applicable
        affiliate_programs_used = []
        if should_add_affiliate_links(category):
            safe_print(f"  üîó Checking for product mentions to add affiliate links...")

            # Detect products mentioned in content
            detected_products = detect_product_mentions(content, lang, category)

            if detected_products:
                safe_print(f"  üì¶ Detected {len(detected_products)} products: {', '.join(detected_products[:3])}")

                # Add affiliate link for the first detected product only (to avoid being too commercial)
                primary_product = detected_products[0]
                link_data = generate_affiliate_link(primary_product, lang)

                if link_data:
                    # Find insertion point: after first ## section
                    sections = content.split('\n## ')
                    if len(sections) > 1:
                        # Insert after first section
                        affiliate_box = create_affiliate_box(primary_product, lang, link_data)
                        sections[1] = sections[1] + '\n' + affiliate_box
                        content = '\n## '.join(sections)

                        affiliate_programs_used.append(link_data['program'])
                        safe_print(f"  ‚úÖ Added affiliate link for '{primary_product}' ({link_data['program']})")
                    else:
                        safe_print(f"  ‚ö†Ô∏è  Could not find insertion point for affiliate link")
                else:
                    safe_print(f"  ‚ÑπÔ∏è  No affiliate program configured for {lang}")
            else:
                safe_print(f"  ‚ÑπÔ∏è  No product mentions detected")
        else:
            safe_print(f"  ‚ÑπÔ∏è  Affiliate links disabled for category: {category}")

        # Add affiliate disclosure if links were added
        if affiliate_programs_used:
            disclosure = get_affiliate_disclosure(lang, affiliate_programs_used)
            content = content.rstrip() + disclosure
            safe_print(f"  ‚ö†Ô∏è  Added affiliate disclosure")

        # Internal linking removed - Hugo template handles related posts automatically

        # Write file with hero image at top
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(frontmatter)
            f.write(hero_image)
            f.write(content)
            f.write(credit_line)

        safe_print(f"  üíæ Saved to: {filepath}")
        return filepath


def main():
    parser = argparse.ArgumentParser(description="Generate blog posts")
    parser.add_argument("--count", type=int, default=3, help="Number of posts to generate")
    parser.add_argument("--topic-id", type=str, help="Specific topic ID to generate")
    args = parser.parse_args()

    # Pre-flight checks
    safe_print(f"\n{'='*60}")
    safe_print(f"  üîç Pre-flight Environment Checks")
    safe_print(f"{'='*60}\n")

    anthropic_key = os.environ.get("ANTHROPIC_API_KEY")
    unsplash_key = os.environ.get("UNSPLASH_ACCESS_KEY")

    if anthropic_key:
        safe_print("  ‚úì ANTHROPIC_API_KEY: Configured")
    else:
        safe_print("  ‚ùå ANTHROPIC_API_KEY: NOT FOUND")

    if unsplash_key:
        safe_print("  ‚úì UNSPLASH_ACCESS_KEY: Configured")
    else:
        safe_print("  ‚ö†Ô∏è  UNSPLASH_ACCESS_KEY: NOT FOUND")
        safe_print("     Posts will use placeholder images!")

    safe_print("")

    # Initialize generator
    try:
        generator = ContentGenerator()
    except ValueError as e:
        safe_print(f"Error: {str(e)}")
        safe_print("\nSet ANTHROPIC_API_KEY environment variable:")
        safe_print("  export ANTHROPIC_API_KEY='your-api-key'")
        sys.exit(1)

    # Get topics
    if args.topic_id:
        # Load specific topic (for testing)
        from topic_queue import get_queue
        queue = get_queue()
        data = queue._load_queue()
        topics = [t for t in data['topics'] if t['id'] == args.topic_id]
        if not topics:
            safe_print(f"Error: Topic {args.topic_id} not found")
            sys.exit(1)
    else:
        # Reserve topics from queue
        topics = reserve_topics(count=args.count)

    if not topics:
        safe_print("No topics available in queue")
        sys.exit(0)

    safe_print(f"\n{'='*60}")
    safe_print(f"  Generating {len(topics)} posts")
    safe_print(f"{'='*60}\n")

    generated_files = []

    for i, topic in enumerate(topics, 1):
        safe_print(f"[{i}/{len(topics)}] {topic['id']}")
        safe_print(f"  Keyword: {topic['keyword']}")
        safe_print(f"  Category: {topic['category']}")
        safe_print(f"  Language: {topic['lang']}")

        try:
            # Generate content
            safe_print(f"  ‚Üí Step 1/7: Generating draft...")
            draft, content_type = generator.generate_draft(topic)

            safe_print(f"  ‚Üí Step 2/7: Editing draft...")
            final_content = generator.edit_draft(draft, topic, content_type)

            # Generate metadata
            safe_print(f"  ‚Üí Step 3/7: Generating metadata...")
            try:
                title = generator.generate_title(final_content, topic['keyword'], topic['lang'], topic.get('references'))
                description = generator.generate_description(final_content, topic['keyword'], topic['lang'])
            except Exception as e:
                safe_print(f"  ‚ö†Ô∏è  WARNING: Metadata generation failed, using defaults")
                safe_print(f"     Error: {mask_secrets(str(e))}")
                title = topic['keyword']
                description = f"Article about {topic['keyword']}"

            # Fetch featured image
            safe_print(f"  ‚Üí Step 4/7: Fetching image...")
            image_path = None
            image_credit = None
            try:
                image_info = generator.fetch_featured_image(topic['keyword'], topic['category'])
                if image_info:
                    image_path = generator.download_image(image_info, topic['keyword'])
                    if image_path:
                        image_credit = image_info
            except Exception as e:
                safe_print(f"  ‚ö†Ô∏è  WARNING: Image fetch failed, will use placeholder")
                safe_print(f"     Error: {mask_secrets(str(e))}")

            # Generate FAQ for AEO
            faq_items = []
            try:
                safe_print(f"  ‚Üí Step 5/7: Generating FAQ...")
                faq_items = generator.generate_faq(final_content, topic['keyword'], topic['lang'])
                safe_print(f"     FAQ: {len(faq_items)} items generated")
            except Exception as e:
                safe_print(f"  ‚ö†Ô∏è  WARNING: FAQ generation failed: {mask_secrets(str(e))}")

            # Extract technologies for tech posts
            technologies = None
            if topic.get('category') == 'tech':
                try:
                    safe_print(f"  ‚Üí Step 6/7: Extracting technologies...")
                    technologies = generator.extract_technologies(final_content, topic['keyword'])
                    if technologies:
                        safe_print(f"     Technologies: {', '.join(technologies)}")
                except Exception as e:
                    safe_print(f"  ‚ö†Ô∏è  WARNING: Technology extraction failed: {mask_secrets(str(e))}")

            # Save post with image, FAQ, and technologies
            safe_print(f"  ‚Üí Step 7/7: Saving post...")
            try:
                filepath = generator.save_post(topic, title, description, final_content, image_path, image_credit, faq_items, technologies)
            except IOError as e:
                safe_print(f"  ‚ùå ERROR: Failed to save post to filesystem")
                safe_print(f"     Error: {str(e)}")
                raise
            except Exception as e:
                safe_print(f"  ‚ùå ERROR: Unexpected error during save")
                safe_print(f"     Error: {mask_secrets(str(e))}")
                raise

            # Mark as completed
            if not args.topic_id:
                try:
                    mark_completed(topic['id'])
                except Exception as e:
                    safe_print(f"  ‚ö†Ô∏è  WARNING: Failed to mark topic as completed in queue")
                    safe_print(f"     Topic ID: {topic['id']}")
                    safe_print(f"     Error: {str(e)}")
                    # Don't fail the whole process if queue update fails

            generated_files.append(str(filepath))
            safe_print(f"  ‚úÖ Completed!\n")

        except KeyError as e:
            safe_print(f"  ‚ùå FAILED: Missing required field in topic data")
            safe_print(f"     Topic ID: {topic.get('id', 'unknown')}")
            safe_print(f"     Missing field: {str(e)}\n")
            if not args.topic_id:
                mark_failed(topic['id'], f"Missing field: {str(e)}")
        except ValueError as e:
            safe_print(f"  ‚ùå FAILED: Invalid data or API response")
            safe_print(f"     Topic ID: {topic.get('id', 'unknown')}")
            safe_print(f"     Error: {mask_secrets(str(e))}\n")
            if not args.topic_id:
                mark_failed(topic['id'], mask_secrets(str(e)))
        except Exception as e:
            safe_print(f"  ‚ùå FAILED: Unexpected error")
            safe_print(f"     Topic ID: {topic.get('id', 'unknown')}")
            safe_print(f"     Error type: {type(e).__name__}")
            safe_print(f"     Error: {mask_secrets(str(e))}\n")
            if not args.topic_id:
                mark_failed(topic['id'], mask_secrets(str(e)))

    # Save generated files list for quality gate
    output_file = Path("generated_files.json")
    with open(output_file, 'w') as f:
        json.dump(generated_files, f, indent=2)

    # Post-generation quality check
    safe_print(f"\n{'='*60}")
    safe_print(f"  üìä Post-Generation Quality Check")
    safe_print(f"{'='*60}\n")

    posts_without_references = 0
    posts_with_placeholders = 0

    for filepath in generated_files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()

                # Check for references section
                has_references = '## References' in content or '## ÂèÇËÄÉ' in content or '## Ï∞∏Í≥†ÏûêÎ£å' in content
                if not has_references:
                    posts_without_references += 1
                    safe_print(f"  ‚ö†Ô∏è  No references: {Path(filepath).name}")

                # Check for placeholder images
                if 'placeholder-' in content:
                    posts_with_placeholders += 1
                    safe_print(f"  ‚ö†Ô∏è  Placeholder image: {Path(filepath).name}")
        except Exception as e:
            safe_print(f"  ‚ö†Ô∏è  Could not check: {Path(filepath).name}")

    safe_print("")

    if posts_without_references > 0:
        safe_print(f"üö® WARNING: {posts_without_references}/{len(generated_files)} posts have NO references!")
        safe_print(f"   This reduces content credibility and SEO value.")
        safe_print(f"   FIX: Ensure Google Custom Search API is configured in keyword curation\n")

    if posts_with_placeholders > 0:
        safe_print(f"üö® WARNING: {posts_with_placeholders}/{len(generated_files)} posts use PLACEHOLDER images!")
        safe_print(f"   This hurts user experience and engagement.")
        safe_print(f"   FIX: Ensure UNSPLASH_ACCESS_KEY is set in environment variables\n")

    if posts_without_references == 0 and posts_with_placeholders == 0:
        safe_print(f"‚úÖ Quality Check PASSED: All posts have references and real images!\n")

    safe_print(f"{'='*60}")
    safe_print(f"  ‚úì Generated {len(generated_files)} posts")
    safe_print(f"  File list saved to: {output_file}")
    safe_print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
