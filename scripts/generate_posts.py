#!/usr/bin/env python3
"""
Content Generation Script

Generates blog posts using Claude API with two-stage process:
1. Draft Agent: Creates initial content
2. Editor Agent: Refines and improves the draft

Usage:
    python generate_posts.py --count 3
    python generate_posts.py --topic-id 001-en-tech-ai-coding
"""

import os
import sys
import json
import argparse
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

# Add scripts to path
sys.path.insert(0, str(Path(__file__).parent))

from topic_queue import reserve_topics, mark_completed, mark_failed
from utils.security import safe_print, mask_secrets
from utils.content_classifier import ContentClassifier
from affiliate_config import (
    detect_product_mentions,
    generate_affiliate_link,
    create_affiliate_box,
    get_affiliate_disclosure,
    should_add_affiliate_links
)
from internal_linker import InternalLinker
from ab_test_manager import ABTestManager
from prompts import get_tutorial_prompt, get_analysis_prompt, get_news_prompt
from utils.rag_pipeline import RAGPipeline
from utils.community_miner import CommunityMiner
from utils.guru_miner import GuruMiner
from utils.korean_community_miner import KoreanCommunityMiner
from utils.few_shot_examples import get_examples

try:
    from anthropic import Anthropic
except ImportError:
    safe_print("Error: anthropic package not installed")
    safe_print("Install with: pip install anthropic")
    sys.exit(1)

try:
    import requests
except ImportError:
    safe_print("Error: requests package not installed")
    safe_print("Install with: pip install requests")
    sys.exit(1)

try:
    import certifi
except ImportError:
    safe_print("Warning: certifi not installed - SSL verification may fail on Windows")
    safe_print("Install with: pip install certifi")
    certifi = None


# System prompts for different languages
SYSTEM_PROMPTS = {
    "en": f"""You are a professional writer for Jake's Tech Insights blog.

ðŸ“… IMPORTANT: Today's date is {datetime.now().year}-{datetime.now().month:02d}-{datetime.now().day:02d}
When referencing current information, ALWAYS use {datetime.now().year}, NOT previous years.

ðŸŽ¯ Goal: 800-1,100 words of concise, high-impact content (AdSense optimized)

[EDITORIAL POLICY - READ FIRST]
This is an in-depth tech analysis publication covering:
- Technology trends, SaaS analysis, data-driven industry reports, developer tools
- Reader value and analytical depth are PRIMARY goals
- Data-backed claims and evidence-based analysis ARE required

Your role:
- Provide evidence-based analysis with specific data points
- Structure content as analytical reports, not news recaps
- Include comparison data, market context, and actionable conclusions
- Every claim should reference a specific source, study, or dataset
- Assume all provided topics are already approved for publication
- Your job is to deliver clear, data-driven analysis

Output requirements:
- Include data-backed conclusions and practical recommendations
- Provide context: why this matters NOW and what comes next
- Support every claim with evidence or reasoning
- Be specific: use numbers, dates, and named sources

Every topic must clearly answer:
- What does the data show?
- Why does this matter for the reader?
- What should they do or watch for next?

[Length Guide - Brevity is Key!]
- Total: 800-1,100 words (optimized completion rate)
- Each ## section: 120-180 words (core insights only)
- Intro: 80-100 words (strong hook)
- Conclusion: 60-80 words (clear CTA)
- **Finish completely**: No mid-sentence cutoffs

[Key Takeaways Block - MANDATORY]
After the introduction and before the first ## heading, ALWAYS include:
> **Key Takeaways**
> - 3-5 bullet points summarizing key insights
> - Each must be a complete, declarative sentence with specific data
> - Written as standalone statements (quotable by AI search engines)

[Content Principles]
1. First paragraph: Hook with the core finding or data point (1-2 sentences)
2. Structure: Context â†’ Data Analysis â†’ Comparison â†’ Practical Implications â†’ Conclusion
3. Tone: Medium/Substack style - conversational, personal, direct
4. SEO: Keyword "{{keyword}}" naturally 4-6 times
5. Sections: 3-4 ## headings (scannable)
6. End: Clear CTA - question or next step

[Writing Style (Required!)]
- **PERSONA**: Write as a senior software engineer (7+ years production experience) explaining to a smart colleague
- **Sentence Variation (CRITICAL - Anti-AI Detection)**:
  * Mix extremely short sentences (5-7 words) with longer explanatory ones (25-35 words)
  * Example pattern: "Short. Long explanation with context and reasoning. Short. Medium."
  * High Perplexity: Use unexpected word choices occasionally
  * This is how humans write - AI uses uniform sentence lengths
- **Contractions (MANDATORY)**:
  * MUST use: you're, it's, doesn't, I've, we're, can't, won't
  * Avoid formal: "you are", "it is", "does not" (sounds robotic)
- Use "you" and "I" sparingly for conversational tone
- Short direct sentences without filler phrases
- Natural transitions: "So", "And", "But" (NOT "Moreover/Furthermore")
- Strong sentence starters: State facts directly, use specific data points
- Include recent dates, statistics, and concrete examples (2025-2026 data preferred)

[AI PHRASE BLACKLIST - CRITICAL - NEVER USE THESE]:
BANNED TRANSITIONS (causes instant AI detection):
- "Here's the thing" / "Here's where it gets interesting" / "Here's what makes it remarkable"
- "Sound familiar?" / "Look," / "Let me explain" / "Let me break down"
- "You might be thinking" / "You've probably been there" / "You've seen this before"
- "But here's where it gets interesting" / "This is where it gets tricky"

BANNED WORDS (2025 research - AI overuse):
- delve, tapestry, realm, testament, pivotal, multifaceted
- foster, endeavour, facilitate, optimize, utilize
- comprehensive, robust, seamless, synergy

BANNED OPENERS:
- "In today's rapidly evolving..." / "In the ever-changing landscape..."
- "In the realm of..." / "In essence" / "In summary" / "In conclusion"

BANNED HEDGING:
- "It's important to note that..." / "It's worth mentioning..."
- "Certainly!" / "Absolutely!" / "Great question!"
- "Moreover", "Furthermore", "Additionally" (max 1 total across entire article)

[Style - Completion Optimized]
- Active voice, short sentences (1-2 lines)
- Core value only (cut fluff)
- Specific numbers/examples (1-2 selective)
- Bullet points for scannability
- End with punch: "Here's the bottom line."

[Absolutely Avoid]
- Redundancy: repeating same points âŒ
- AI tells: "certainly", "it's important to note", "moreover", "furthermore"
- Academic tone: formal, distant language
- Abstract buzzwords: "revolutionary", "game-changer", "cutting-edge"
- Excessive emojis, unnecessary case studies
- Aggro triggers: "shock", "expose", "truth revealed", "jaw-dropping", "unbelievable"

[Headline Patterns - Analytical (Use ONLY these patterns)]
A. Comparison: "[X] vs [Y]: What the Data Shows in [year]"
B. Deep Dive: "Why [topic] Matters: [specific data point]"
C. Market Analysis: "The State of [topic] in [year]: Key Findings"
D. Practical Guide: "How [topic] Changes [outcome]: Data-Driven Analysis"

âš ï¸ Core: Complete 800-1,100 word article. Plenty of headroom in 12,000 tokens!""",

    "ko": f"""ë‹¹ì‹ ì€ Jake's Tech Insights ë¸”ë¡œê·¸ì˜ ì „ë¬¸ ìž‘ê°€ìž…ë‹ˆë‹¤.

ðŸ“… ì¤‘ìš”: ì˜¤ëŠ˜ ë‚ ì§œëŠ” {datetime.now().year}ë…„ {datetime.now().month}ì›” {datetime.now().day}ì¼ìž…ë‹ˆë‹¤
í˜„ìž¬ ì •ë³´ë¥¼ ì–¸ê¸‰í•  ë•Œ ë°˜ë“œì‹œ {datetime.now().year}ë…„ì„ ì‚¬ìš©í•˜ì„¸ìš”. ê³¼ê±° ì—°ë„ ì‚¬ìš© ê¸ˆì§€.

ðŸŽ¯ í•µì‹¬ ëª©í‘œ: 800-1,100 ë‹¨ì–´ì˜ ê°„ê²°í•˜ê³  ìž„íŒ©íŠ¸ ìžˆëŠ” ê¸€ ìž‘ì„± (ì• ë“œì„¼ìŠ¤ ìµœì í™”)

[íŽ¸ì§‘ ë°©ì¹¨ - ë°˜ë“œì‹œ ìˆ™ì§€]
ì´ ì‚¬ì´íŠ¸ëŠ” ì‹¬ì¸µ ê¸°ìˆ  ë¶„ì„ ë¯¸ë””ì–´ìž…ë‹ˆë‹¤:
- ê¸°ìˆ  íŠ¸ë Œë“œ, SaaS ë¶„ì„, ë°ì´í„° ê¸°ë°˜ ë¦¬í¬íŠ¸, ê°œë°œìž ë„êµ¬
- ë…ìž ê°€ì¹˜ì™€ ë¶„ì„ì˜ ê¹Šì´ê°€ í•µì‹¬ ëª©í‘œ
- ë°ì´í„° ê¸°ë°˜ ì£¼ìž¥ê³¼ ê·¼ê±° ìžˆëŠ” ë¶„ì„ì´ í•„ìˆ˜

ë‹¹ì‹ ì˜ ì—­í• :
- êµ¬ì²´ì  ë°ì´í„°ì™€ ê·¼ê±°ë¥¼ í¬í•¨í•œ ë¶„ì„ ì œê³µ
- ë‰´ìŠ¤ ìš”ì•½ì´ ì•„ë‹Œ ë¶„ì„ ë¦¬í¬íŠ¸ í˜•íƒœë¡œ êµ¬ì„±
- ë¹„êµ ë°ì´í„°, ì‹œìž¥ ë§¥ë½, ì‹¤ìš©ì  ê²°ë¡  í¬í•¨
- ëª¨ë“  ì£¼ìž¥ì— ì¶œì²˜, ì—°êµ¬, ë°ì´í„°ì…‹ ì°¸ì¡° í•„ìˆ˜
- ëª¨ë“  í† í”½ì€ ì´ë¯¸ íŽ¸ì§‘íŒ€ì´ ìŠ¹ì¸í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì„¸ìš”

ì¶œë ¥ ìš”êµ¬ì‚¬í•­:
- ë°ì´í„° ê¸°ë°˜ ê²°ë¡ ê³¼ ì‹¤ìš©ì  ê¶Œìž¥ ì‚¬í•­ í¬í•¨
- ë§¥ë½ ì œê³µ: ì™œ ì§€ê¸ˆ ì¤‘ìš”í•œì§€, ë‹¤ìŒì— ë¬´ì—‡ì´ ì˜¬ì§€
- ëª¨ë“  ì£¼ìž¥ì— ê·¼ê±°ë‚˜ ë…¼ë¦¬ ë’·ë°›ì¹¨
- êµ¬ì²´ì ìœ¼ë¡œ: ìˆ«ìž, ë‚ ì§œ, ì‹¤ëª… ì¶œì²˜ ì‚¬ìš©

ëª¨ë“  í† í”½ì€ ëª…í™•ížˆ ë‹µí•´ì•¼ í•©ë‹ˆë‹¤:
- ë°ì´í„°ê°€ ë¬´ì—‡ì„ ë³´ì—¬ì£¼ëŠ”ê°€?
- ë…ìžì—ê²Œ ì™œ ì¤‘ìš”í•œê°€?
- ë‹¤ìŒì— ë¬´ì—‡ì„ ì£¼ì‹œí•´ì•¼ í•˜ëŠ”ê°€?

[ê¸¸ì´ ê°€ì´ë“œ - ê°„ê²°í•¨ì´ í•µì‹¬!]
- ì „ì²´ ê¸€: 800-1,100 ë‹¨ì–´ (ì™„ë…ë¥  ìµœì í™”)
- ê° ## ì„¹ì…˜: 120-180 ë‹¨ì–´ (í•µì‹¬ë§Œ ì „ë‹¬)
- ë„ìž…ë¶€: 80-100 ë‹¨ì–´ (ê°•ë ¥í•œ í›„í‚¹)
- ê²°ë¡ : 60-80 ë‹¨ì–´ (ëª…í™•í•œ CTA)
- **ë§ˆì§€ë§‰ ë¬¸ìž¥ê¹Œì§€ ë°˜ë“œì‹œ ì™„ì„±**: ëŠê¹€ ì—†ì´ ì™„ê²°í•˜ì„¸ìš”

[í•µì‹¬ ìš”ì•½ ë¸”ë¡ - í•„ìˆ˜]
ë„ìž…ë¶€ ì´í›„, ì²« ## í—¤ë”© ì „ì— ë°˜ë“œì‹œ í¬í•¨:
> **í•µì‹¬ ìš”ì•½**
> - 3-5ê°œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë¶ˆë¦¿ìœ¼ë¡œ ìš”ì•½
> - ê° í¬ì¸íŠ¸ëŠ” êµ¬ì²´ì  ë°ì´í„°ë¥¼ í¬í•¨í•œ ì™„ê²°ëœ ë¬¸ìž¥
> - ì„ ì–¸ì  ë¬¸ìž¥ìœ¼ë¡œ ìž‘ì„± (AI ê²€ìƒ‰ ì—”ì§„ì´ ì¸ìš© ê°€ëŠ¥í•œ í˜•íƒœ)

[ì½˜í…ì¸  ì›ì¹™]
1. ì²« ë¬¸ë‹¨: í•µì‹¬ ë°œê²¬ì´ë‚˜ ë°ì´í„° í¬ì¸íŠ¸ë¡œ í›„í‚¹ (1-2ë¬¸ìž¥)
2. êµ¬ì¡°: ë§¥ë½ â†’ ë°ì´í„° ë¶„ì„ â†’ ë¹„êµ â†’ ì‹¤ìš©ì  ì‹œì‚¬ì  â†’ ê²°ë¡ 
3. í†¤: í† ìŠ¤(Toss) ìŠ¤íƒ€ì¼ - ì „ë¬¸ì ì´ì§€ë§Œ íŽ¸ì•ˆí•œ ì¹œêµ¬ ê°™ì€ ëŠë‚Œ
4. SEO: í‚¤ì›Œë“œ "{{keyword}}"ë¥¼ ìžì—°ìŠ¤ëŸ½ê²Œ 4-6íšŒ í¬í•¨
5. ì„¹ì…˜: 3-4ê°œ ## í—¤ë”© (ê° ì„¹ì…˜ì€ ì½ê¸° ì‰½ê²Œ)
6. ë: ëª…í™•í•œ CTA - ì§ˆë¬¸ì´ë‚˜ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ

[í† ìŠ¤ ìŠ¤íƒ€ì¼ ë§íˆ¬ (í•„ìˆ˜!)]
- **ë¬¸ìž¥ ê¸¸ì´ ë³€ì£¼ (ì¤‘ìš” - AI íƒì§€ íšŒí”¼)**:
  * ì§§ì€ ë¬¸ìž¥(5-7ë‹¨ì–´)ê³¼ ê¸´ ì„¤ëª…ë¬¸(25-35ë‹¨ì–´)ì„ ê·¹ë‹¨ì ìœ¼ë¡œ ì„žê¸°
  * ì˜ˆ: "ë§žì•„ìš”. ì‹¤ì œë¡œ ì§€ë‚œ 6ê°œì›”ê°„ ì¡°ì‚¬í•œ ê²°ê³¼ ì´ íŒ¨í„´ì´ ê°€ìž¥ íš¨ê³¼ì ì´ì—ˆê±°ë“ ìš”. ë†€ëžì£ ?"
  * ì‚¬ëžŒì€ ì´ë ‡ê²Œ ì”ë‹ˆë‹¤ - AIëŠ” ë¹„ìŠ·í•œ ê¸¸ì´ë¡œ ì¨ìš”
- "~í•´ìš”" ë°˜ë§ ì¡´ëŒ“ë§ ì‚¬ìš© (ìŠµë‹ˆë‹¤/í•©ë‹ˆë‹¤ âŒ)
- "ì–´ë–¤ê°€ìš”?", "í•œë²ˆ ë³¼ê¹Œìš”?", "ê¶ê¸ˆí•˜ì§€ ì•Šìœ¼ì„¸ìš”?" ê°™ì€ ì¹œê·¼í•œ ì§ˆë¬¸
- "ì‚¬ì‹¤", "ì‹¤ì œë¡œ", "ê·¸ëŸ°ë°", "ì°¸ê³ ë¡œ" ê°™ì€ ìžì—°ìŠ¤ëŸ¬ìš´ ì ‘ì†ì‚¬
- ìˆ«ìžë¥¼ ì¹œê·¼í•˜ê²Œ: "10ê°œ â†’ ì—´ ê°œ", "50% â†’ ì ˆë°˜", "3ë°° â†’ ì„¸ ë°°"
- ì§§ê³  ê°•ë ¬í•œ ë¬¸ìž¥: "ë†€ëžì£ ?", "ë§žì•„ìš”.", "ì´ê²Œ í•µì‹¬ì´ì—ìš”."

[AI ê¸ˆì§€ í‘œí˜„ - ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€]:
ê¸ˆì§€ ì „í™˜ í‘œí˜„ (AI ì¦‰ì‹œ íƒì§€):
- "ì´ëŸ° ê²½í—˜ ìžˆìœ¼ì‹œì£ ?" / "ì´ê²Œ í•µì‹¬ì´ì—ìš”" / "ì—¬ê¸°ì„œ ìž¬ë¯¸ìžˆëŠ” ê±´"
- "ì†”ì§ížˆ ë§í•˜ë©´" / "í•œë²ˆ ë³¼ê¹Œìš”" (ê³¼ë„ ì‚¬ìš© ì‹œ)
- "ì•„ë§ˆ ì´ë ‡ê²Œ ìƒê°í•˜ì‹¤ ê±°ì˜ˆìš”" / "ì´ëŸ° ê²½ìš° ë§Žì£ ?"

ê¸ˆì§€ ë‹¨ì–´ (2025 ì—°êµ¬ - AI ê³¼ë‹¤ ì‚¬ìš©):
- "ë¬¼ë¡ ", "~í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤", "ì¤‘ìš”í•©ë‹ˆë‹¤", "ì£¼ëª©í• ë§Œí•œ"
- "í˜ì‹ ì ", "ê²Œìž„ì²´ì¸ì €", "íšê¸°ì "
- "í™œìš©", "ê²°í•©", "ìµœì í™”" â†’ "ì“°ë‹¤", "í•©ì¹˜ë‹¤", "ìž˜ ë§žì¶”ë‹¤"
- "ì „ëžµì ", "íš¨ìœ¨ì ", "ì²´ê³„ì " (ì¶”ìƒì )

ê¸ˆì§€ ì—°ê²°ì–´:
- "ë˜í•œ", "ê²Œë‹¤ê°€", "ë”ìš±ì´" (ìµœëŒ€ 1íšŒ)

[ìŠ¤íƒ€ì¼ - ì™„ë…ë¥  ìµœì í™”]
- ëŠ¥ë™íƒœ ìœ„ì£¼, ì§§ì€ ë¬¸ìž¥ (1-2ì¤„)
- í•µì‹¬ë§Œ ì „ë‹¬ (ë¶ˆí•„ìš”í•œ ì„¤ëª… ì œê±°)
- êµ¬ì²´ì  ìˆ«ìž/ì˜ˆì‹œ (1-2ê°œë§Œ ì„ íƒì ìœ¼ë¡œ)
- ë¶ˆë¦¿ í¬ì¸íŠ¸ ì ê·¹ í™œìš© (ìŠ¤ìº” ê°€ëŠ¥í•˜ê²Œ)
- ë¬¸ë‹¨ ë ê°•ì¡°: "ì™œ ê·¸ëŸ´ê¹Œìš”?", "ì´ê²Œ í•µì‹¬ì´ì—ìš”."

[ì ˆëŒ€ ê¸ˆì§€]
- ì¤‘ì–¸ë¶€ì–¸: ê°™ì€ ë‚´ìš© ë°˜ë³µ âŒ
- AI í‹°: "ë¬¼ë¡ ", "~í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤", "~í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤"
- ë”±ë”±í•œ ë¬¸ì²´: "~ìŠµë‹ˆë‹¤/~í•©ë‹ˆë‹¤" (í•´ìš”ì²´ë§Œ!)
- ì¶”ìƒì  í‘œí˜„: "í˜ì‹ ì ", "ê²Œìž„ì²´ì¸ì €", "ì£¼ëª©í•  ë§Œí•œ"
- ê³¼ë„í•œ ì´ëª¨ì§€, ë¶ˆí•„ìš”í•œ ì‚¬ë¡€ ë‚˜ì—´
- ì–´ê·¸ë¡œ ë‹¨ì–´: "ì¶©ê²©", "í­ë¡œ", "ì‹¤ì²´", "ì§„ì‹¤", "ì†Œë¦„", "ì¶©ê²©ì ", "ì™„ë²½ ì •ë¦¬", "í•œ ë²ˆì— ì´í•´"

[í—¤ë“œë¼ì¸ íŒ¨í„´ - ë¶„ì„í˜• (ì´ íŒ¨í„´ë§Œ ì‚¬ìš©)]
A. ë¹„êµ: "[X] vs [Y]: ë°ì´í„°ê°€ ë³´ì—¬ì£¼ëŠ” [year]ë…„ í˜„í™©"
B. ì‹¬ì¸µ ë¶„ì„: "[ì£¼ì œ]ê°€ ì¤‘ìš”í•œ ì´ìœ : [êµ¬ì²´ì  ë°ì´í„° í¬ì¸íŠ¸]"
C. ì‹œìž¥ ë¶„ì„: "[year]ë…„ [ì£¼ì œ] í˜„í™©: í•µì‹¬ ë°œê²¬"
D. ì‹¤ì „ ê°€ì´ë“œ: "[ì£¼ì œ]ê°€ [ê²°ê³¼]ë¥¼ ë°”ê¾¸ëŠ” ë°©ë²•: ë°ì´í„° ê¸°ë°˜ ë¶„ì„"

âš ï¸ í•µì‹¬: 800-1,100 ë‹¨ì–´ë¡œ ì™„ê²°ëœ ê¸€ì„ ìž‘ì„±í•˜ì„¸ìš”. 12,000 í† í° ë‚´ì—ì„œ ì—¬ìœ ìžˆê²Œ!"""
}


class ContentGenerator:
    def __init__(self, api_key: Optional[str] = None, unsplash_key: Optional[str] = None):
        """Initialize content generator with Claude API and Unsplash API"""
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            safe_print("âŒ ERROR: ANTHROPIC_API_KEY not found")
            safe_print("   Please set it as environment variable or pass to constructor")
            safe_print("   Example: export ANTHROPIC_API_KEY='your-key-here'")
            raise ValueError(
                "ANTHROPIC_API_KEY not found. Set it as environment variable or pass to constructor."
            )

        # Initialize with Prompt Caching beta header
        try:
            self.client = Anthropic(
                api_key=self.api_key,
                default_headers={
                    "anthropic-beta": "prompt-caching-2024-07-31"
                }
            )
            self.model = "claude-sonnet-4-6"
            safe_print("  âœ“ Anthropic API client initialized successfully")
        except Exception as e:
            safe_print(f"âŒ ERROR: Failed to initialize Anthropic client: {mask_secrets(str(e))}")
            raise

        # Unsplash API (optional)
        self.unsplash_key = unsplash_key or os.environ.get("UNSPLASH_ACCESS_KEY")
        if self.unsplash_key:
            safe_print("  ðŸ–¼ï¸  Unsplash API enabled")
        else:
            safe_print("  âš ï¸  Unsplash API key not found (images will be skipped)")
            safe_print("     Set UNSPLASH_ACCESS_KEY environment variable to enable")

        # Initialize A/B Test Manager
        self.ab_test_manager = ABTestManager()
        safe_print("  ðŸ§ª A/B Test Manager initialized")

        # Initialize RAG Pipeline
        self.rag_pipeline = RAGPipeline()
        if self.rag_pipeline.enabled:
            safe_print("  ðŸ” RAG Pipeline enabled (Brave Search + Content Fetching)")
        else:
            safe_print("  âš ï¸  RAG Pipeline disabled (missing Brave or Anthropic API key)")

        # Initialize Community Miner
        self.community_miner = CommunityMiner()
        if self.community_miner.enabled:
            safe_print("  ðŸ’¬ Community Miner enabled (HackerNews + Dev.to)")
        else:
            safe_print("  âš ï¸  Community Miner disabled (missing Anthropic API key)")

        # Initialize Guru Miner
        self.guru_miner = GuruMiner()
        if self.guru_miner.enabled:
            safe_print("  ðŸŽ“ Guru Miner enabled (Lenny, a16z, Pragmatic Engineer)")
        else:
            safe_print("  âš ï¸  Guru Miner disabled (missing dependencies)")

        # Initialize Korean Community Miner
        self.korean_miner = KoreanCommunityMiner()
        if self.korean_miner.enabled:
            safe_print("  ðŸ‡°ðŸ‡· Korean Community Miner enabled (GeekNews, Toss, Kakao)")
        else:
            safe_print("  âš ï¸  Korean Community Miner disabled (missing dependencies)")

    def generate_draft(self, topic: Dict) -> tuple[str, str]:
        """Generate initial draft using Draft Agent with Prompt Caching

        Returns:
            tuple: (draft_content, content_type)
        """
        keyword = topic['keyword']
        lang = topic['lang']
        category = topic['category']
        references = topic.get('references', [])  # Get references from topic

        # Classify content type
        classifier = ContentClassifier()
        keywords = [keyword]  # Use topic keyword
        content_type = classifier.classify(keyword, keywords, category)

        safe_print(f"  ðŸŽ¯ Content type: {content_type}")
        safe_print(f"  ðŸ“ Generating draft for: {keyword}")

        # Get RAG context (if enabled)
        rag_context = None
        if self.rag_pipeline.enabled:
            rag_context = self.rag_pipeline.get_context(keyword, max_sources=3)

        # Get community insights (if enabled)
        community_insights = None
        if self.community_miner.enabled:
            community_insights = self.community_miner.get_insights(keyword)

        # Get guru insights (if enabled)
        guru_insights = None
        if self.guru_miner.enabled:
            guru_insights = self.guru_miner.get_insights(keyword)

        # Get Korean community insights (if enabled)
        korean_insights = None
        if self.korean_miner.enabled:
            korean_insights = self.korean_miner.get_insights(keyword)

        # Get few-shot examples for this language and content type
        few_shot = get_examples(lang, content_type)

        # Get type-specific prompt
        if content_type == 'tutorial':
            user_prompt = get_tutorial_prompt(keyword, keywords, lang)
        elif content_type == 'analysis':
            user_prompt = get_analysis_prompt(keyword, keywords, lang)
        else:  # news
            user_prompt = get_news_prompt(keyword, keywords, lang)

        # Build full context
        context_parts = []

        # Add few-shot examples first (style learning)
        if few_shot:
            context_parts.append(f"# Writing Style Examples\n\n{few_shot}")
            safe_print(f"  âœ… Few-shot examples added")

        # Add content sources
        if rag_context:
            context_parts.append(rag_context)
            safe_print(f"  âœ… RAG context added ({len(rag_context)} chars)")

        if community_insights:
            context_parts.append(community_insights)
            safe_print(f"  âœ… Community insights added ({len(community_insights)} chars)")

        if guru_insights:
            context_parts.append(guru_insights)
            safe_print(f"  âœ… Guru insights added ({len(guru_insights)} chars)")

        if korean_insights:
            context_parts.append(korean_insights)
            safe_print(f"  âœ… Korean community insights added ({len(korean_insights)} chars)")

        # Combine all context
        if context_parts:
            full_context = "\n\n---\n\n".join(context_parts)
            user_prompt = f"{full_context}\n\n---\n\n{user_prompt}"

        # Append references if available
        if references and len(references) > 0:
            user_prompt += "\n\n## References (Use for factual accuracy)\n\n"
            for i, ref in enumerate(references[:3], 1):  # Use top 3
                user_prompt += f"{i}. {ref['title']}\n   URL: {ref['url']}\n"
                if ref.get('snippet'):
                    user_prompt += f"   Summary: {ref['snippet']}\n"
                user_prompt += "\n"
            user_prompt += "Use these references for factual information, but DO NOT plagiarize. Write in your own words.\n"

        system_prompt = SYSTEM_PROMPTS[lang].format(keyword=keyword)

        # Use Prompt Caching: cache the system prompt
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=12000,
                system=[
                    {
                        "type": "text",
                        "text": system_prompt,
                        "cache_control": {"type": "ephemeral"}
                    }
                ],
                messages=[{
                    "role": "user",
                    "content": user_prompt
                }]
            )
        except Exception as e:
            error_msg = mask_secrets(str(e))
            safe_print(f"  âŒ ERROR: API call failed during draft generation")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {error_msg}")
            raise

        if not response or not response.content:
            safe_print(f"  âŒ ERROR: Empty response from API")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            raise ValueError("Empty response from Claude API")

        draft = response.content[0].text

        # Log cache performance
        usage = response.usage
        cache_read = getattr(usage, 'cache_read_input_tokens', 0)
        cache_create = getattr(usage, 'cache_creation_input_tokens', 0)

        # Always show cache status
        if cache_read > 0:
            safe_print(f"  ðŸ’¾ Cache HIT: {cache_read} tokens saved!")
        elif cache_create > 0:
            safe_print(f"  ðŸ’¾ Cache created: {cache_create} tokens")
        else:
            safe_print(f"  â„¹ï¸  No caching (usage: input={usage.input_tokens}, output={usage.output_tokens})")

        safe_print(f"  âœ“ Draft generated ({len(draft)} chars)")
        return draft, content_type

    def edit_draft(self, draft: str, topic: Dict, content_type: str = 'analysis') -> str:
        """Refine draft using Editor Agent with Prompt Caching

        Args:
            draft: Draft content to edit
            topic: Topic dictionary
            content_type: Type of content (tutorial/analysis/news)

        Returns:
            str: Edited content
        """
        lang = topic['lang']

        safe_print(f"  âœï¸  Editing draft...")

        if not draft or len(draft.strip()) == 0:
            safe_print(f"  âš ï¸  WARNING: Empty draft provided for editing")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            raise ValueError("Cannot edit empty draft")

        editor_prompt = self._get_editor_prompt(lang, content_type)

        # Use Prompt Caching: cache the editor instructions
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=12000,
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "text",
                                "text": editor_prompt,
                                "cache_control": {"type": "ephemeral"}
                            },
                            {
                                "type": "text",
                                "text": f"\n\n---\n\n{draft}"
                            }
                        ]
                    }
                ]
            )
        except Exception as e:
            error_msg = mask_secrets(str(e))
            safe_print(f"  âŒ ERROR: API call failed during draft editing")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            safe_print(f"     Draft length: {len(draft)} chars")
            safe_print(f"     Error: {error_msg}")
            raise

        if not response or not response.content:
            safe_print(f"  âŒ ERROR: Empty response from editing API")
            safe_print(f"     Topic: {topic.get('id', 'unknown')}")
            raise ValueError("Empty response from Claude API during editing")

        edited = response.content[0].text

        # Log cache performance
        usage = response.usage
        cache_read = getattr(usage, 'cache_read_input_tokens', 0)
        cache_create = getattr(usage, 'cache_creation_input_tokens', 0)

        # Always show cache status
        if cache_read > 0:
            safe_print(f"  ðŸ’¾ Cache HIT: {cache_read} tokens saved!")
        elif cache_create > 0:
            safe_print(f"  ðŸ’¾ Cache created: {cache_create} tokens")
        else:
            safe_print(f"  â„¹ï¸  No caching (usage: input={usage.input_tokens}, output={usage.output_tokens})")

        safe_print(f"  âœ“ Draft edited ({len(edited)} chars)")
        return edited

    def _get_draft_prompt(self, keyword: str, category: str, lang: str, references: List[Dict] = None) -> str:
        """Get draft generation prompt based on language"""
        # Get current date in KST
        from datetime import datetime, timezone, timedelta
        kst = timezone(timedelta(hours=9))
        today = datetime.now(kst)
        current_date = today.strftime("%Yë…„ %mì›” %dì¼")  # Korean format
        current_date_en = today.strftime("%B %d, %Y")  # English format
        current_year = today.year

        # Format references for prompt
        refs_section = ""
        if references and len(references) > 0:
            refs_list = "\n".join([
                f"- [{ref.get('title', 'Source')}]({ref.get('url', '')}) - {ref.get('source', '')}"
                for ref in references[:3]
            ])
            refs_section = f"\n\nðŸ“š USE THESE REFERENCES:\n{refs_list}\n"

        prompts = {
            "en": f"""ðŸ“… TODAY'S DATE: {current_date_en}
âš ï¸ IMPORTANT: You are writing this article as of TODAY ({current_date_en}). All information must be current as of {current_year}. Do NOT use outdated information from 2024 or earlier years.

Write a comprehensive blog post about: {keyword}{refs_section}

Category: {category}

â±ï¸ Reading Time Target: 4-5 minutes
- Write 3-4 main sections (## headings)
- Each section: 1-2 minutes to read, one key point
- Short paragraphs (2-4 sentences each)
- End with a thought-provoking question

ðŸŽ¯ HOOKING STRATEGY (Critical!):
1. **Opening Hook** (First 2-3 sentences):
   - Start with a concrete number, cost figure, or failure stat â€” not empathy fluff
   - Good: "Running local AI in 2026 costs $80,000 upfront. Cloud AI costs $20/month. The math isn't close."
   - Good: "GitHub Copilot cut PR review time by 40% at Microsoft. But 60% of enterprises still haven't deployed it."
   - BAD: "You've probably been there..." / "Sound familiar?" / "Here's the thing..." (banned phrases)
   - NOT generic: "X is becoming popular..."

2. **Real Success/Failure Cases â€” Named Sources Required**:
   - Use named companies from your RAG context/references FIRST: Stripe, Vercel, Notion, GitHub, etc.
   - Only fall back to "One e-commerce startup..." when NO named source is available in context
   - NEVER invent company names â€” only use companies mentioned in the provided references
   - Show what DOESN'T work, not just what works

3. **Limitations & Pitfalls**:
   - Dedicate 1 section to "When X Actually Hurts"
   - "In these 3 situations, X is counterproductive..."
   - This makes content feel authentic and trustworthy

4. **Data-Driven**:
   - Include 2-3 specific statistics (even if approximate)
   - "2024 survey shows 60% failure rate..."
   - "Companies saw 35% productivity increase..."

Content Guidelines:
- Target audience: Decision-makers seeking practical advice
- Focus on "What to avoid" as much as "What to do"
- Concrete examples over abstract concepts
- Mention current trends (2025-2026)
- Be concise and impactful - avoid unnecessary explanations

ðŸ“š REFERENCES SECTION:
- If references were provided above in the prompt, you MUST add a "## References" section at the end
- Use those EXACT URLs - do not modify or create new ones
- Format: `- [Source Title](URL) - Organization/Publisher`
- Example:
  ## References
  - [The State of AI in 2025](https://example.com/ai-report) - McKinsey & Company
  - [Remote Work Statistics 2025](https://example.com/remote) - Buffer
- **IMPORTANT**: If NO references were provided above, DO NOT add a References section at all

**This section is REQUIRED for all posts - even Entertainment/Society topics!**

Write the complete blog post now (body only, no title or metadata):""",

            "ko": f"""ðŸ“… ì˜¤ëŠ˜ ë‚ ì§œ: {current_date}
âš ï¸ ì¤‘ìš”: ì´ ê¸€ì€ ì˜¤ëŠ˜({current_date}) ê¸°ì¤€ìœ¼ë¡œ ìž‘ì„±í•©ë‹ˆë‹¤. ëª¨ë“  ì •ë³´ëŠ” {current_year}ë…„ í˜„ìž¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤. 2024ë…„ ì´í•˜ì˜ ì˜¤ëž˜ëœ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

ë‹¤ìŒ ì£¼ì œë¡œ í¬ê´„ì ì¸ ë¸”ë¡œê·¸ ê¸€ì„ ìž‘ì„±í•˜ì„¸ìš”: {keyword}{refs_section}

ì¹´í…Œê³ ë¦¬: {category}

â±ï¸ ì½ê¸° ì‹œê°„ ëª©í‘œ: 4-5ë¶„
- 3-4ê°œì˜ ì£¼ìš” ì„¹ì…˜ (## í—¤ë”©) ìž‘ì„±
- ê° ì„¹ì…˜: 1-2ë¶„ ì½ê¸° ë¶„ëŸ‰, í•˜ë‚˜ì˜ í•µì‹¬ í¬ì¸íŠ¸
- ì§§ì€ ë¬¸ë‹¨ ì‚¬ìš© (2-4 ë¬¸ìž¥ì”©)
- ìƒê°ì„ ìžê·¹í•˜ëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë§ˆë¬´ë¦¬

ðŸŽ¯ í›„í‚¹ ì „ëžµ (í•„ìˆ˜!):
1. **ì˜¤í”„ë‹ í›„í‚¹ â€” í† ìŠ¤ ìŠ¤íƒ€ì¼** (ì²« 2-3ë¬¸ìž¥):
   ë…ìžì˜ ìƒí™©/ê³ ë¯¼ì„ ë¨¼ì €, ì •ë³´ëŠ” ê·¸ ë‹¤ìŒì—.

   âœ… ì¢‹ì€ ì˜ˆì‹œ:
   - "íŒ€ìž¥ì´ ë˜ ë¬¼ì–´ë´¤ì–´ìš”. 'ê·¸ëž˜ì„œ ìš°ë¦¬ AI ì–¸ì œ ì¨ìš”?' ê·¼ë° ë§‰ìƒ ë­˜ ì¨ì•¼ í• ì§€ ëª¨ë¥´ê² ì£ ."
   - "ìž‘ë…„ì— êµ¬ë…í•œ íˆ´, ì§€ê¸ˆë„ ì“°ê³  ìžˆë‚˜ìš”? ëŒ€ë¶€ë¶„ ì•„ë‹ ê±°ì˜ˆìš”. AI ë„êµ¬ë„ ë˜‘ê°™ì•„ìš”."
   - "ë¹„ìš© ê³„ì‚°í•´ë´¤ì–´ìš”? í´ë¼ìš°ë“œ AI í•œ ë‹¬ 2ë§Œì›, ë¡œì»¬ ì„œë²„ êµ¬ì¶•í•˜ë©´ 8ì²œë§Œì›ì´ì—ìš”."

   âŒ ë‚˜ìœ ì˜ˆì‹œ (ì ˆëŒ€ ê¸ˆì§€):
   - "2026ë…„ 2ì›”, XëŠ” ë” ì´ìƒ ì‹¤í—˜ì‹¤ ê°œë…ì´ ì•„ë‹™ë‹ˆë‹¤..." (ì •ë³´ ì„ í–‰)
   - "Xê°€ ì£¼ëª©ë°›ê³  ìžˆìŠµë‹ˆë‹¤. ì™œ ê·¸ëŸ´ê¹Œìš”?" (ê¸°ìžì‹ ë„ìž…)
   - "XëŠ” ì¤‘ìš”í•©ë‹ˆë‹¤. ì´ ê¸€ì—ì„œ ì•Œì•„ë´…ë‹ˆë‹¤." (ë³´ê³ ì„œì‹)

   í•µì‹¬: ë…ìžê°€ "ë§žì•„, ë‚˜ë„ ê·¸ëž˜"ë¼ê³  ëŠë¼ê²Œ ì‹œìž‘í•˜ì„¸ìš”.

2. **ì‹¤ì œ ì„±ê³µ/ì‹¤íŒ¨ ì‚¬ë¡€ â€” ì‹¤ëª… ìš°ì„ **:
   - RAGë‚˜ ì°¸ê³ ìžë£Œì— ì‹¤ëª… ê¸°ì—…ì´ ìžˆìœ¼ë©´ ë°˜ë“œì‹œ ì‚¬ìš© (í† ìŠ¤, ì¹´ì¹´ì˜¤, ë„¤ì´ë²„, Stripe, Notion ë“±)
   - ì‹¤ëª…ì´ ì—†ì„ ë•Œë§Œ: "í•œ í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—…ì€..." ì‚¬ìš©
   - ì ˆëŒ€ ê¸ˆì§€: ê·¼ê±° ì—†ì´ ê¸°ì—…ëª… ì§€ì–´ë‚´ê¸°
   - ì•ˆ ë˜ëŠ” ê²ƒë„ ë³´ì—¬ì£¼ê¸° (ì„±ê³µë§Œ ë§í•˜ì§€ ë§ê¸°)

3. **í•œê³„ì ê³¼ í•¨ì •**:
   - "Xê°€ ì˜¤ížˆë ¤ ì—­íš¨ê³¼ì¸ ê²½ìš°" ì„¹ì…˜ 1ê°œ í• ì• 
   - "ì´ 3ê°€ì§€ ìƒí™©ì—ì„œëŠ” Xê°€ ë¹„íš¨ìœ¨ì ..."
   - ì´ê²ƒì´ ì§„ì •ì„±ê³¼ ì‹ ë¢°ë¥¼ ë§Œë“¦

4. **ë°ì´í„° ê¸°ë°˜**:
   - êµ¬ì²´ì  í†µê³„ 2-3ê°œ í¬í•¨ (ëŒ€ëžµì ì´ì–´ë„ OK)
   - "2024ë…„ ì¡°ì‚¬ì— ë”°ë¥´ë©´ 60% ì‹¤íŒ¨ìœ¨..."
   - "ê¸°ì—…ë“¤ì´ 35% ìƒì‚°ì„± ì¦ê°€ ê²½í—˜..."

ì½˜í…ì¸  ê°€ì´ë“œë¼ì¸:
- ëŒ€ìƒ ë…ìž: ì‹¤ìš©ì  ì¡°ì–¸ì„ ì°¾ëŠ” ì˜ì‚¬ê²°ì •ìž
- "í”¼í•´ì•¼ í•  ê²ƒ"ì„ "í•´ì•¼ í•  ê²ƒ"ë§Œí¼ ê°•ì¡°
- ì¶”ìƒì  ê°œë…ë³´ë‹¤ êµ¬ì²´ì  ì˜ˆì‹œ
- í˜„ìž¬ íŠ¸ë Œë“œ ì–¸ê¸‰ (2025-2026ë…„)
- ê°„ê²°í•˜ê³  ìž„íŒ©íŠ¸ ìžˆê²Œ - ë¶ˆí•„ìš”í•œ ì„¤ëª… ì œê±°

ðŸ“š ì°¸ê³ ìžë£Œ ì„¹ì…˜:
- ìœ„ í”„ë¡¬í”„íŠ¸ì— ì°¸ê³ ìžë£Œê°€ ì œê³µëœ ê²½ìš°, ë°˜ë“œì‹œ ê¸€ ë§ˆì§€ë§‰ì— "## ì°¸ê³ ìžë£Œ" ì„¹ì…˜ ì¶”ê°€
- ì œê³µëœ URLì„ ì •í™•ížˆ ì‚¬ìš© - ìˆ˜ì •í•˜ê±°ë‚˜ ìƒˆë¡œ ë§Œë“¤ì§€ ë§ ê²ƒ
- í˜•ì‹: `- [ì¶œì²˜ ì œëª©](URL) - ì¡°ì§/ì¶œíŒì‚¬`
- ì˜ˆì‹œ:
  ## ì°¸ê³ ìžë£Œ
  - [2025 AI í˜„í™© ë³´ê³ ì„œ](https://example.com/ai-report) - ë§¥í‚¨ì§€ì•¤ì»´í¼ë‹ˆ
  - [ì›ê²© ê·¼ë¬´ í†µê³„ 2025](https://example.com/remote) - Buffer
- **ì¤‘ìš”**: ìœ„ì— ì°¸ê³ ìžë£Œê°€ ì œê³µë˜ì§€ ì•Šì•˜ë‹¤ë©´, ì°¸ê³ ìžë£Œ ì„¹ì…˜ì„ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”

ì§€ê¸ˆ ë°”ë¡œ ì™„ì „í•œ ë¸”ë¡œê·¸ ê¸€ì„ ìž‘ì„±í•˜ì„¸ìš” (ë³¸ë¬¸ë§Œ, ì œëª©ì´ë‚˜ ë©”íƒ€ë°ì´í„° ì œì™¸):"""
        }

        return prompts[lang]

    def _get_editor_prompt(self, lang: str, content_type: str = 'analysis') -> str:
        """Get editor prompt based on language and content type

        Args:
            lang: Language code (en/ko)
            content_type: Content type (tutorial/analysis/news)

        Returns:
            str: Editor prompt with type-specific length targets
        """
        # Get type-specific word count targets
        classifier = ContentClassifier()
        config = classifier.get_config(content_type, lang)
        min_count, max_count = config['word_count']

        # Format length requirements based on language
        if lang == 'ko':
            count_unit = 'ê¸€ìž'
            length_req = f"""ðŸ“ ê¸¸ì´ ìš”êµ¬ì‚¬í•­ (CRITICAL - ë°˜ë“œì‹œ ì¤€ìˆ˜):
ðŸŽ¯ ëª©í‘œ ë²”ìœ„: {min_count:,}-{max_count:,}{count_unit}

**ì ˆëŒ€ ê·œì¹™**:
- ì´ˆì•ˆì´ {int(min_count*0.8):,}{count_unit} ë¯¸ë§Œ: ì˜ˆì‹œ/ì„¤ëª… ì¶”ê°€ë¡œ ìµœì†Œ {min_count:,}{count_unit} ì´ìƒ í™•ìž¥
- ì´ˆì•ˆì´ {min_count:,}-{max_count:,}{count_unit}: ê¸¸ì´ ì ˆëŒ€ ìœ ì§€ (ì´ìƒì  ë²”ìœ„ - ì••ì¶• ê¸ˆì§€!)
- ì´ˆì•ˆì´ {int(max_count*1.3):,}{count_unit} ì´ìƒ: ì¤‘ë³µë§Œ ì œê±°í•˜ì—¬ {max_count:,}{count_unit} ê·¼ì²˜ë¡œ ì¡°ì •

âš ï¸  ê²½ê³ : ì´ìƒì  ë²”ìœ„({min_count:,}-{max_count:,}{count_unit})ì— ìžˆìœ¼ë©´ ì ˆëŒ€ ì¤„ì´ì§€ ë§ˆì„¸ìš”!"""
        else:
            length_req = f"""ðŸ“ Length Requirements (CRITICAL - Must Follow):
ðŸŽ¯ Target Range: {min_count:,}-{max_count:,} words

**Absolute Rules**:
- If draft is under {int(min_count*0.8):,} words: EXPAND with examples/explanations to reach at least {min_count:,} words
- If draft is {min_count:,}-{max_count:,} words: MAINTAIN exact length (ideal range - DO NOT compress!)
- If draft is over {int(max_count*1.3):,} words: Remove only redundancy to reach near {max_count:,} words

âš ï¸  Warning: If draft is in ideal range ({min_count:,}-{max_count:,} words), DO NOT shorten it!"""

        prompts = {
            "en": f"""You are an expert editor. Transform this into Medium-style content with authentic human touch:

{length_req}

ðŸš¨ BANNED PHRASE PURGE (Do this FIRST before anything else):
Scan the entire draft and REPLACE every instance of these phrases â€” they trigger AI detection:
- "Here's the thing" â†’ rewrite the sentence directly
- "Here's where it gets interesting" / "Here's where it gets tricky" â†’ cut filler, state the point
- "Sound familiar?" â†’ delete or rephrase as a direct statement
- "You've been there" / "You've probably been there" â†’ cut entirely
- "You might be thinking" â†’ rewrite as "The obvious question is..." or just make the point
- "Look," (as sentence opener) â†’ state the claim directly
- "Let me explain" / "Let me break down" â†’ just explain/break it down
- "But here's where..." â†’ start with the actual content
- "In today's rapidly evolving..." / "In the ever-changing..." â†’ cut, start with the data
- "Moreover" / "Furthermore" / "Additionally" â†’ use "And", "But", "So" or restructure

ðŸŽ¯ CRITICAL ENHANCEMENTS:
1. **Strengthen Opening Hook**:
   - If opening is generic, rewrite to start with a specific problem, failure stat, or cost figure
   - Make it concrete: "Running local AI in 2026 costs $80,000 upfront." (not vague empathy)
   - Avoid clichÃ©d empathy openers

2. **Add Authenticity Markers** (NO personal anecdotes):
   - Use authoritative references: "Industry reports show...", "According to recent data..."
   - Add failure acknowledgment: "This approach can fail when..."
   - Show balanced perspective: "This isn't always the answer..."
   - AVOID: "In my experience...", "I spoke with...", "I thought..."

3. **Enhance Examples â€” Named Sources Required**:
   - Replace anonymous examples with named companies when RAG data provides them
   - "Many companies" â†’ "Stripe", "Notion", "Vercel" (use names from the draft's context)
   - If no named source exists, use: "Case studies show...", "Reports indicate..."
   - NEVER invent names: only use companies already mentioned in the draft
   - Include what went WRONG, not just success stories

4. **Balance Perspective**:
   - Ensure there's a "When this doesn't work" section
   - Add nuance: "This works IF...", "But in these cases..."
   - Avoid absolute claims: "always", "never", "guaranteed"

Tasks:
1. **Banned phrase purge** (MANDATORY FIRST STEP â€” see above)
2. **Medium style conversion**: conversational tone, use "you" naturally
3. **Eliminate all AI tells**: "certainly", "moreover", "it's important to note", "delve", "tapestry"
4. **Natural connectors**: "So", "And", "But", "The truth is", "That's why"
5. **Punchy sentences**: vary length â€” mix 5-word sentences with 30-word ones
6. Keep all factual information intact
7. **Complete ending**: Finish conclusion fully
8. **Preserve Key Takeaways block**: Do not remove or restructure the blockquote Key Takeaways. Improve wording only.

Return improved version (body only, no title):""",

            "ko": f"""ë‹¹ì‹ ì€ ì „ë¬¸ ì—ë””í„°ìž…ë‹ˆë‹¤. ì´ ë¸”ë¡œê·¸ ê¸€ì„ ì§„ì§œ ì‚¬ëžŒì´ ì“´ ê²ƒ ê°™ì€ í† ìŠ¤ ìŠ¤íƒ€ì¼ë¡œ ê°œì„ í•˜ì„¸ìš”:

{length_req}

ðŸš¨ AI ê¸ˆì§€ í‘œí˜„ ì œê±° (ê°€ìž¥ ë¨¼ì € í•  ê²ƒ):
ì´ˆì•ˆ ì „ì²´ë¥¼ ìŠ¤ìº”í•˜ì—¬ ì•„ëž˜ í‘œí˜„ì„ ë°˜ë“œì‹œ êµì²´í•˜ì„¸ìš”:
- "ì™œ ê·¸ëŸ´ê¹Œìš”?" (2íšŒ ì´ìƒ ì‚¬ìš© ì‹œ) â†’ 1íšŒ ì´ˆê³¼ë¶„ì€ ì‚­ì œí•˜ê±°ë‚˜ ì§ì ‘ ì„¤ëª…ìœ¼ë¡œ ëŒ€ì²´
- "ì´ëŸ° ê²½í—˜ ìžˆìœ¼ì‹œì£ ?" â†’ "ë§žì£ ?", "ê·¸ë ‡ì£ ?" ë˜ëŠ” ì§ì ‘ ì„œìˆ ë¡œ ëŒ€ì²´
- "ì†”ì§ížˆ ë§í•˜ë©´" â†’ ê·¸ëƒ¥ ì§ì ‘ ë§í•˜ê¸°
- "ì•„ë§ˆ ì´ë ‡ê²Œ ìƒê°í•˜ì‹¤ ê±°ì˜ˆìš”" â†’ ì‚­ì œí•˜ê³  ë°”ë¡œ ìš”ì  ì „ë‹¬
- "ì´ê²Œ í•µì‹¬ì´ì—ìš”" (ê³¼ë„ ì‚¬ìš© ì‹œ) â†’ 1-2íšŒë¡œ ì œí•œ
- "ë˜í•œ", "ê²Œë‹¤ê°€", "ë”ìš±ì´" â†’ ìµœëŒ€ 1íšŒë§Œ í—ˆìš©, ë‚˜ë¨¸ì§€ëŠ” "ê·¸ë¦¬ê³ ", "í•˜ì§€ë§Œ", "ê·¸ëž˜ì„œ"ë¡œ êµì²´
- "ë¬¼ë¡ ", "~í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤", "ì¤‘ìš”í•©ë‹ˆë‹¤" â†’ ì‚­ì œí•˜ê±°ë‚˜ êµ¬ì²´ì  í‘œí˜„ìœ¼ë¡œ ëŒ€ì²´

ðŸŽ¯ í•µì‹¬ ê°œì„ ì‚¬í•­:
1. **ì˜¤í”„ë‹ ê°•í™” â€” í† ìŠ¤ ìŠ¤íƒ€ì¼**:
   - ì •ë³´ ì„ í–‰ ë„ìž…ë¶€ë©´ ë…ìž ìƒí™©ìœ¼ë¡œ ìž¬ìž‘ì„±
   - ì˜ˆ: "2026ë…„ Xê°€ ì£¼ëª©ë°›ê³  ìžˆì–´ìš”" âŒ â†’ "íŒ€ìž¥ì´ ë˜ ë¬¼ì–´ë´ìš”. 'ê·¸ëž˜ì„œ ìš°ë¦¬ X ì–¸ì œ ì¨ìš”?'" âœ…
   - ë…ìžê°€ "ë§žì•„, ë‚˜ë„ ê·¸ëž˜"ë¼ê³  ëŠë¼ê²Œ ì‹œìž‘

2. **ì •ë³´ ë°€ë„ ìµœìš°ì„ ** (í•œêµ­ ë…ìž = ë¹ ë¥¸ ì •ë³´ ì„ í˜¸):
   - í•µì‹¬ ì •ë³´ ë¨¼ì €: ìˆ˜ì¹˜, ë‹¨ê³„, ë°©ë²•
   - ì‹¤ìš© ì •ë³´ ì¦‰ì‹œ ì œê³µ: "ê³„ì‚°ë²•: 1) ~ 2) ~"
   - í•œê³„ ì–¸ê¸‰: "í•­ìƒ ë‹µì€ ì•„ë‹ˆì—ìš”..."

3. **ì˜ˆì‹œ êµ¬ì²´í™” â€” ì‹¤ëª… ìš°ì„ ** (ê°œì¸ ê²½í—˜ ë°°ì œ):
   - ì´ˆì•ˆì— ì‹¤ëª… ê¸°ì—…ì´ ìžˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš© (í† ìŠ¤, ì¹´ì¹´ì˜¤, ë„¤ì´ë²„ ë“±)
   - ì‹¤ëª…ì´ ì—†ì„ ë•Œë§Œ: "í•œ í•€í…Œí¬ ìŠ¤íƒ€íŠ¸ì—…ì€..." ì‚¬ìš©
   - ì ˆëŒ€ ê¸ˆì§€: ê·¼ê±° ì—†ì´ ê¸°ì—…ëª… ë§Œë“¤ì–´ë‚´ê¸°
   - ì‹¤íŒ¨ ì‚¬ë¡€ í¬í•¨: ì„±ê³µë§Œ ë§í•˜ì§€ ë§ê¸°

4. **ê· í˜•ìž¡ížŒ ê´€ì **:
   - "ì´ëŸ° ê²½ìš°ì—” ì•ˆ í†µí•´ìš”" ì„¹ì…˜ í™•ì¸/ì¶”ê°€
   - ì ˆëŒ€ì  í‘œí˜„ í”¼í•˜ê¸°: "í•­ìƒ", "ì ˆëŒ€", "ë¬´ì¡°ê±´"

ìž‘ì—…:
1. **AI ê¸ˆì§€ í‘œí˜„ ì œê±°** (í•„ìˆ˜ ì²« ë‹¨ê³„ â€” ìœ„ ëª©ë¡ ì°¸ì¡°)
2. **í† ìŠ¤ ë§íˆ¬ë¡œ ë³€í™˜**: "~ìŠµë‹ˆë‹¤" â†’ "~í•´ìš”", ì¹œê·¼í•œ ì§ˆë¬¸í˜• ì¶”ê°€
3. ìžì—°ìŠ¤ëŸ¬ìš´ ì ‘ì†ì‚¬: "ì‚¬ì‹¤", "ì‹¤ì œë¡œ", "ê·¸ëŸ°ë°", "ì°¸ê³ ë¡œ"
4. ìˆ«ìžë¥¼ ì¹œê·¼í•˜ê²Œ: "50% â†’ ì ˆë°˜", "3ë°° â†’ ì„¸ ë°°"
5. ì§§ê³  ê°•ë ¬í•œ ë¬¸ìž¥ ì¶”ê°€: "ë†€ëžì£ ?", "ë§žì•„ìš”."
6. ì„¹ì…˜ ê°„ ë§¤ë„ëŸ¬ìš´ ì „í™˜: "ìž, ì´ì œ ~", "ê·¸ëŸ¼ ~"
7. ëª¨ë“  ì‚¬ì‹¤ ì •ë³´ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
8. **ë§ˆì§€ë§‰ ë¬¸ìž¥ê¹Œì§€ ì™„ê²°**: ê²°ë¡ ì„ ë°˜ë“œì‹œ ì™„ì„±
9. **í•µì‹¬ ìš”ì•½ ë¸”ë¡ ìœ ì§€**: Key Takeaways ë¸”ë¡ì„ ì œê±°í•˜ê±°ë‚˜ êµ¬ì¡° ë³€ê²½í•˜ì§€ ë§ ê²ƒ. ë¬¸êµ¬ë§Œ ê°œì„ .

ê°œì„ ëœ ë²„ì „ì„ ë°˜í™˜í•˜ì„¸ìš” (ë³¸ë¬¸ë§Œ, ì œëª© ì œì™¸):"""
        }

        return prompts[lang]

    def generate_title(self, content: str, keyword: str, lang: str, references: List[Dict] = None) -> str:
        """Generate SEO-friendly title based on actual content and references"""
        # Get current year in KST
        from datetime import datetime, timezone, timedelta
        kst = timezone(timedelta(hours=9))
        current_year = datetime.now(kst).year

        # Extract strategic samples from content for better context
        # Take beginning (intro), middle (main content), and end (conclusion)
        content_length = len(content)
        if content_length <= 1200:
            content_preview = content
        else:
            # Get first 500, middle 400, last 300 chars
            beginning = content[:500]
            middle_start = content_length // 2 - 200
            middle = content[middle_start:middle_start + 400]
            ending = content[-300:]
            content_preview = f"{beginning}\n\n[...middle section...]\n{middle}\n\n[...conclusion...]\n{ending}"

        # Format references if available
        refs_context = ""
        if references and len(references) > 0:
            refs_list = "\n".join([
                f"- {ref.get('title', 'Source')}"
                for ref in references[:3]
            ])
            refs_context = f"\n\nREFERENCE TOPICS:\n{refs_list}\n"

        prompts = {
            "en": f"Generate a factual, SEO-friendly blog title (50-60 chars) for this post about '{keyword}'.\n\nCONTENT SAMPLES (beginning, middle, end):\n{content_preview}{refs_context}\n\nCRITICAL RULES - VIOLATION WILL FAIL:\n1. Title MUST describe what the content ACTUALLY discusses (not what sounds catchy)\n2. NO exaggeration, speculation, or clickbait (e.g., \"confirmed\", \"revealed\", \"secret\")\n3. If content is about \"how to watch\", title must say \"how to watch\" (not \"rankings\")\n4. If content discusses problems/issues, title must reflect that (not promise solutions)\n5. ONLY use facts explicitly stated in the content samples\n6. Do NOT promise specific numbers/data unless clearly stated in content\n7. Include keyword '{keyword}' naturally\n8. Current year is {current_year}\n9. Return ONLY the title",
            "ko": f"'{keyword}'ì— ëŒ€í•œ ì‚¬ì‹¤ì ì´ê³  SEO ì¹œí™”ì ì¸ ì œëª©ì„ ìƒì„±í•˜ì„¸ìš” (50-60ìž).\n\në³¸ë¬¸ ìƒ˜í”Œ (ì‹œìž‘, ì¤‘ê°„, ë):\n{content_preview}{refs_context}\n\ní•µì‹¬ ê·œì¹™ - ìœ„ë°˜ ì‹œ ì‹¤íŒ¨:\n1. ì œëª©ì€ ë³¸ë¬¸ì´ ì‹¤ì œë¡œ ë‹¤ë£¨ëŠ” ë‚´ìš©ì„ ì„¤ëª…í•´ì•¼ í•¨ (ë§¤ë ¥ì ìœ¼ë¡œ ë“¤ë¦¬ëŠ” ê²ƒì´ ì•„ë‹˜)\n2. ê³¼ìž¥, ì¶”ì¸¡, í´ë¦­ë² ì´íŠ¸ ê¸ˆì§€ (ì˜ˆ: \"í™•ì •\", \"í­ë¡œ\", \"ì¶©ê²©\")\n3. ë³¸ë¬¸ì´ \"ì‹œì²­ ë°©ë²•\"ì— ëŒ€í•œ ê²ƒì´ë©´ ì œëª©ë„ \"ì‹œì²­ ë°©ë²•\"ì´ì–´ì•¼ í•¨ (\"ëž­í‚¹\" ì•„ë‹˜)\n4. ë³¸ë¬¸ì´ ë¬¸ì œì ì„ ë…¼ì˜í•˜ë©´ ì œëª©ë„ ê·¸ê²ƒì„ ë°˜ì˜í•´ì•¼ í•¨ (í•´ê²°ì±… ì•½ì† ê¸ˆì§€)\n5. ë³¸ë¬¸ ìƒ˜í”Œì— ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ì‚¬ì‹¤ë§Œ ì‚¬ìš©\n6. ë³¸ë¬¸ì— ëª…í™•ížˆ ë‚˜ì™€ìžˆì§€ ì•Šìœ¼ë©´ êµ¬ì²´ì  ìˆ«ìž/ë°ì´í„° ì•½ì† ê¸ˆì§€\n7. '{keyword}' í‚¤ì›Œë“œë¥¼ ìžì—°ìŠ¤ëŸ½ê²Œ í¬í•¨\n8. í˜„ìž¬ ì—°ë„ëŠ” {current_year}ë…„\n9. ì œëª©ë§Œ ë°˜í™˜"
        }

        response = self.client.messages.create(
            model=self.model,
            max_tokens=100,
            messages=[{
                "role": "user",
                "content": prompts[lang]
            }]
        )

        generated_title = response.content[0].text.strip().strip('"').strip("'")

        # Validate title-content alignment (STRICT check for critical mismatches only)
        validation_prompts = {
            "en": f"Does this title accurately match what the content ACTUALLY discusses?\n\nTITLE: {generated_title}\n\nCONTENT: {content_preview}\n\nCheck for CRITICAL mismatches ONLY:\n- Title promises specific data (e.g., \"$800\", \"75%\") but content doesn't provide it\n- Title says \"how to watch\" but content discusses problems/history instead\n- Title says \"confirmed\" but content is speculation/rumors\n- Title topic completely different from content topic\n\nIGNORE minor issues like:\n- Year mentions (2026 is acceptable for future-looking content)\n- Slight emphasis differences\n- Language mixing (Korean keyword in English title is OK)\n\nAnswer 'yes' if title reasonably matches content. Answer 'no' ONLY for critical mismatches. If no, explain in max 15 words.",
            "ko": f"ì´ ì œëª©ì´ ë³¸ë¬¸ì´ ì‹¤ì œë¡œ ë…¼ì˜í•˜ëŠ” ë‚´ìš©ê³¼ ì •í™•ížˆ ì¼ì¹˜í•©ë‹ˆê¹Œ?\n\nì œëª©: {generated_title}\n\në³¸ë¬¸: {content_preview}\n\nì¹˜ëª…ì  ë¶ˆì¼ì¹˜ë§Œ í™•ì¸:\n- ì œëª©ì´ êµ¬ì²´ì  ë°ì´í„°(ì˜ˆ: \"75%\", \"$800\")ë¥¼ ì•½ì†í•˜ì§€ë§Œ ë³¸ë¬¸ì— ì—†ìŒ\n- ì œëª©ì€ \"ì‹œì²­ ë°©ë²•\"ì¸ë° ë³¸ë¬¸ì€ ë¬¸ì œì /ì—­ì‚¬ë¥¼ ë…¼ì˜\n- ì œëª©ì€ \"í™•ì •\"ì¸ë° ë³¸ë¬¸ì€ ì¶”ì¸¡/ì†Œë¬¸\n- ì œëª© ì£¼ì œì™€ ë³¸ë¬¸ ì£¼ì œê°€ ì™„ì „ížˆ ë‹¤ë¦„\n\në¬´ì‹œí•  ì‚¬ì†Œí•œ ë¬¸ì œ:\n- ì—°ë„ ì–¸ê¸‰ (ë¯¸ëž˜ ì§€í–¥ì  ì½˜í…ì¸ ì— 2026 ì‚¬ìš© ê°€ëŠ¥)\n- ì•½ê°„ì˜ ê°•ì¡° ì°¨ì´\n- ì–¸ì–´ í˜¼ìš© (ì˜ì–´ ì œëª©ì— í•œêµ­ì–´ í‚¤ì›Œë“œ ì‚¬ìš© ê°€ëŠ¥)\n\nì œëª©ì´ ë³¸ë¬¸ê³¼ í•©ë¦¬ì ìœ¼ë¡œ ì¼ì¹˜í•˜ë©´ 'ì˜ˆ'. ì¹˜ëª…ì  ë¶ˆì¼ì¹˜ë§Œ 'ì•„ë‹ˆì˜¤'. ì•„ë‹ˆì˜¤ë¼ë©´ 15ë‹¨ì–´ ì´ë‚´ ì„¤ëª…."
        }

        validation_response = self.client.messages.create(
            model=self.model,
            max_tokens=50,
            messages=[{
                "role": "user",
                "content": validation_prompts[lang]
            }]
        )

        validation_result = validation_response.content[0].text.strip().lower()

        # If validation fails, regenerate title with strict instructions
        if not validation_result.startswith('yes') and not validation_result.startswith('ì˜ˆ'):
            safe_print(f"  âš ï¸  Title-content mismatch detected: {validation_result}")
            safe_print(f"     Original title: {generated_title}")
            safe_print(f"  ðŸ”„ Regenerating title with strict content alignment...")

            # Regenerate with stricter prompt
            regenerate_prompts = {
                "en": f"Generate a title that EXACTLY matches what this content discusses. Do NOT promise specifics that aren't in the content. Do NOT use words like 'confirmed', 'breaking', or future dates unless explicitly stated.\n\nContent preview:\n{content_preview}\n\nKeyword to include: {keyword}\n\nTitle (60-70 chars):",
                "ko": f"ë³¸ë¬¸ì´ ì‹¤ì œë¡œ ë‹¤ë£¨ëŠ” ë‚´ìš©ê³¼ ì •í™•ížˆ ì¼ì¹˜í•˜ëŠ” ì œëª©ì„ ìƒì„±í•˜ì„¸ìš”. ë³¸ë¬¸ì— ì—†ëŠ” êµ¬ì²´ì  ë‚´ìš©ì„ ì•½ì†í•˜ì§€ ë§ˆì„¸ìš”. 'í™•ì •', 'ì†ë³´', ë¯¸ëž˜ ë‚ ì§œëŠ” ë³¸ë¬¸ì— ëª…ì‹œë˜ì§€ ì•Šìœ¼ë©´ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n\në³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°:\n{content_preview}\n\ní¬í•¨í•  í‚¤ì›Œë“œ: {keyword}\n\nì œëª© (40-50ìž):"
            }

            regenerate_response = self.client.messages.create(
                model=self.model,
                max_tokens=100,
                messages=[{
                    "role": "user",
                    "content": regenerate_prompts[lang]
                }]
            )

            generated_title = regenerate_response.content[0].text.strip().strip('"').strip("'")
            safe_print(f"  âœ“ Regenerated title: {generated_title}")

        return generated_title

    def generate_description(self, content: str, keyword: str, lang: str) -> str:
        """Generate meta description optimized for SEO (120-160 chars)"""
        prompts = {
            "en": f"Generate a compelling meta description for a blog post about '{keyword}'.\n\nREQUIREMENTS:\n- Length: EXACTLY 120-160 characters (strict)\n- Include keyword naturally\n- Action-oriented and engaging\n- NO quotes, NO marketing fluff\n\nReturn ONLY the description, nothing else.",
            "ko": f"'{keyword}'ì— ëŒ€í•œ ë¸”ë¡œê·¸ ê¸€ì˜ ë©”íƒ€ ì„¤ëª…ì„ ìƒì„±í•˜ì„¸ìš”.\n\nìš”êµ¬ì‚¬í•­:\n- ê¸¸ì´: ì •í™•ížˆ 120-160ìž (ì—„ê²©)\n- í‚¤ì›Œë“œ ìžì—°ìŠ¤ëŸ½ê²Œ í¬í•¨\n- í–‰ë™ ì§€í–¥ì ì´ê³  ë§¤ë ¥ì ìœ¼ë¡œ\n- ë”°ì˜´í‘œ ì—†ì´, ë§ˆì¼€íŒ… ë¬¸êµ¬ ê¸ˆì§€\n\nì„¤ëª…ë§Œ ë°˜í™˜í•˜ì„¸ìš”."
        }

        response = self.client.messages.create(
            model=self.model,
            max_tokens=100,
            messages=[{
                "role": "user",
                "content": prompts[lang]
            }]
        )

        return response.content[0].text.strip().strip('"').strip("'")

    def generate_faq(self, content: str, keyword: str, lang: str) -> list:
        """Generate 3-5 FAQ pairs based on article content for FAQPage schema"""
        prompts = {
            "en": f"""Based on this blog post about '{keyword}', generate 3-5 FAQ pairs that someone might search for.

RULES:
- Questions must be natural search queries (how people actually type in Google)
- Answers must be 2-3 sentences, factual, directly answering the question
- Include the keyword '{keyword}' naturally in at least 2 questions
- Each answer should be self-contained (understandable without reading the article)

Return ONLY a JSON array:
[
  {{"question": "...", "answer": "..."}},
  {{"question": "...", "answer": "..."}}
]

Article content:
{content[:3000]}""",
            "ko": f"""ì´ '{keyword}' ë¸”ë¡œê·¸ ê¸€ì„ ê¸°ë°˜ìœ¼ë¡œ 3-5ê°œ FAQ ìŒì„ ìƒì„±í•˜ì„¸ìš”.

ê·œì¹™:
- ì§ˆë¬¸ì€ ìžì—°ìŠ¤ëŸ¬ìš´ ê²€ìƒ‰ ì¿¼ë¦¬ì—¬ì•¼ í•¨ (ì‹¤ì œ êµ¬ê¸€ì— íƒ€ì´í•‘í•˜ëŠ” í˜•íƒœ)
- ë‹µë³€ì€ 2-3ë¬¸ìž¥, ì‚¬ì‹¤ì , ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€
- ìµœì†Œ 2ê°œ ì§ˆë¬¸ì— '{keyword}' í‚¤ì›Œë“œ ìžì—°ìŠ¤ëŸ½ê²Œ í¬í•¨
- ê° ë‹µë³€ì€ ë…ë¦½ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•´ì•¼ í•¨

JSON ë°°ì—´ë§Œ ë°˜í™˜:
[
  {{"question": "...", "answer": "..."}},
  {{"question": "...", "answer": "..."}}
]

ê¸€ ë‚´ìš©:
{content[:3000]}"""
        }

        response = self.client.messages.create(
            model=self.model,
            max_tokens=1500,
            messages=[{"role": "user", "content": prompts[lang]}]
        )

        text = response.content[0].text
        json_match = re.search(r'\[[\s\S]*\]', text)
        if json_match:
            faq_items = json.loads(json_match.group())
            return faq_items[:5]
        return []

    def extract_technologies(self, content: str, keyword: str) -> list:
        """Extract technology names mentioned in content for taxonomy"""
        known_techs = [
            'Python', 'JavaScript', 'TypeScript', 'React', 'Next.js', 'Vue.js',
            'Angular', 'Node.js', 'Django', 'FastAPI', 'Flask', 'Docker',
            'Kubernetes', 'AWS', 'Azure', 'GCP', 'PostgreSQL', 'MongoDB',
            'Redis', 'GraphQL', 'REST API', 'Claude', 'GPT', 'OpenAI',
            'Anthropic', 'LangChain', 'Vercel', 'Terraform', 'GitHub Actions',
            'Linux', 'Rust', 'Go', 'Swift', 'Kotlin', 'Java', 'C++',
            'WebAssembly', 'Cloudflare', 'Figma', 'Tailwind CSS', 'Hugo',
            'Webpack', 'Vite', 'Supabase', 'Firebase', 'Stripe', 'Notion',
            'Slack', 'VS Code', 'ChatGPT', 'Gemini', 'Copilot', 'Cursor',
            'TensorFlow', 'PyTorch', 'Stable Diffusion', 'Midjourney',
            'Ollama', 'Hugging Face', 'Mistral', 'Llama', 'DALL-E', 'Sora'
        ]

        found = []
        content_lower = content.lower()
        for tech in known_techs:
            if tech.lower() in content_lower:
                found.append(tech)

        return found[:5]

    def translate_to_english(self, text: str) -> str:
        """Translate non-English keywords to English for Unsplash search"""
        # Simple keyword translations for common tech/business/lifestyle terms
        translations = {
            # Korean
            'ì±—ë´‡': 'chatbot', 'AI': 'artificial intelligence', 'ë„ìž…': 'implementation',
            'ì‹¤íŒ¨': 'failure', 'ì´ìœ ': 'reasons', 'ë…¸ì½”ë“œ': 'no-code', 'íˆ´': 'tool',
            'í•œê³„ì ': 'limitations', 'ìž¬íƒê·¼ë¬´': 'remote work', 'í•˜ì´ë¸Œë¦¬ë“œ': 'hybrid',
            'ê·¼ë¬´': 'work', 'íš¨ìœ¨ì„±': 'efficiency', 'MZì„¸ëŒ€': 'gen z', 'ê´€ë¦¬': 'management',
            'ë°©ë²•': 'method', 'ì‚¬ë¡€': 'case', 'ë¯¸ë‹ˆë©€': 'minimal', 'ë¼ì´í”„': 'lifestyle',
            'ì¤‘ë‹¨': 'quit', 'ìƒì‚°ì„±': 'productivity', 'íŒ': 'tips',
        }

        # Split and translate each word
        words = text.split()
        translated_words = []
        for word in words:
            # Try exact match first
            if word in translations:
                translated_words.append(translations[word])
            else:
                # Check if word contains any translatable substring
                found = False
                for kr, en in translations.items():
                    if kr in word:
                        translated_words.append(en)
                        found = True
                        break
                if not found:
                    # Keep as-is if ASCII (likely already English)
                    try:
                        word.encode('ascii')
                        translated_words.append(word)
                    except UnicodeEncodeError:
                        pass  # Skip non-ASCII untranslatable words

        return ' '.join(translated_words) if translated_words else 'technology'

    def fetch_featured_image(self, keyword: str, category: str) -> Optional[Dict]:
        """Fetch featured image from Unsplash API"""
        if not self.unsplash_key:
            return None

        try:
            # Clean keyword for better Unsplash search
            # Remove years (2020-2030) to avoid year-specific images
            import re
            clean_keyword = re.sub(r'20[2-3][0-9]ë…„?', '', keyword)  # Match years + optional ë…„ (Korean year)
            # Remove common prefixes/suffixes that reduce search quality
            clean_keyword = re.sub(r'ã€.*?ã€‘', '', clean_keyword)  # Remove ã€bracketsã€‘
            clean_keyword = re.sub(r'\[.*?\]', '', clean_keyword)  # Remove [brackets]
            clean_keyword = clean_keyword.strip()

            # Translation dictionary for meaningful keywords
            keyword_translations = {
                # Korean - AI/Jobs/Employment
                'AI': 'artificial intelligence',
                'ì¸ê³µì§€ëŠ¥': 'artificial intelligence',
                'ëŒ€ì²´': 'replacement automation',
                'ì¼ìžë¦¬': 'job employment work',
                'ì‹¤ì—…': 'unemployment jobless',
                'ì§ì—…': 'occupation career profession',
                'ì·¨ì—…': 'employment hiring recruitment',
                'ìžë™í™”': 'automation robot',
                'ê¸°ìˆ ': 'technology tech',
                'ë””ì§€í„¸': 'digital technology',
                'ë¡œë´‡': 'robot automation',
                'ë¯¸ëž˜': 'future',
                'ë³€í™”': 'change transformation',
                'ìœ„í—˜': 'risk danger',
                # Korean - Finance/Business
                'ë‚˜ë¼ì‚¬ëž‘ì¹´ë“œ': 'patriot card credit card',
                'ì¹´ë“œ': 'card credit',
                'ì—°ë ¹': 'age limit',
                'ì œí•œ': 'restriction limit',
                'ì „ì„¸': 'housing lease deposit',
                'ë³´ì¦ê¸ˆ': 'deposit guarantee',
                'ë°°ë‹¬': 'delivery food',
                'ìˆ˜ìˆ˜ë£Œ': 'fee commission',
                'ìžì˜ì—…': 'small business owner',
                'íì—…': 'business closure bankruptcy',
                'ì§€ì›ê¸ˆ': 'subsidy support fund',
                'ì •ë¶€': 'government policy',
                'ì‹ ì²­': 'application registration',
                'í˜œíƒ': 'benefit advantage',
                # Korean - Entertainment/Society
                'ì‚¬ê³¼ë¬¸': 'apology statement',
                'íŒ¬': 'fan supporter',
                'ë“±ëŒë¦¼': 'backlash criticism',
                'ìŠ¤ë§ˆíŠ¸í°': 'smartphone mobile',
                'ê±´ê°•': 'health wellness'
            }

            # Extract meaningful keywords from title
            title_words = clean_keyword.split()
            translated_keywords = []

            # Try to find and translate key phrases
            for ko_word, en_translation in keyword_translations.items():
                if ko_word in clean_keyword:
                    translated_keywords.append(en_translation)

            # If no translation found, extract meaningful words (skip common noise and non-ASCII)
            if not translated_keywords:
                noise_words = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for']
                for word in title_words[:3]:  # Take first 3 words
                    # Filter out non-ASCII words to prevent non-English queries
                    try:
                        word.encode('ascii')
                        is_ascii = True
                    except UnicodeEncodeError:
                        is_ascii = False

                    if is_ascii and len(word) > 2 and word.lower() not in noise_words:
                        translated_keywords.append(word)

            # Add category context (tech-only strategy)
            category_context = {
                'tech': ['technology', 'digital innovation', 'tech workspace', 'coding', 'software'],
            }

            # Build flexible, contextual query with fallback strategies
            if translated_keywords:
                base_keywords = ' '.join(translated_keywords[:2])
                # Primary search: specific keywords + main category term
                context_list = category_context.get(category, ['technology'])
                context = context_list[0]
                query = f"{base_keywords} {context}".strip()
                # Store fallback queries
                fallback_queries = [f"{base_keywords} {ctx}" for ctx in context_list[1:3]]
                fallback_queries.append(context_list[0])  # Pure category as last resort
            else:
                # Fallback to category-specific queries if no English keywords found
                context_list = category_context.get(category, ['technology'])
                query = context_list[0]
                fallback_queries = context_list[1:3]

            # Unsplash API endpoint
            url = "https://api.unsplash.com/search/photos"
            headers = {
                "Authorization": f"Client-ID {self.unsplash_key}"
            }
            params = {
                "query": query,
                "per_page": 30,  # Increased from 5 to 30 for larger image pool
                "orientation": "landscape"
            }

            safe_print(f"  ðŸ” Searching Unsplash for: {query}")

            # Use certifi for SSL verification (Windows compatibility)
            verify_ssl = certifi.where() if certifi else True
            response = requests.get(url, headers=headers, params=params, timeout=10, verify=verify_ssl)
            response.raise_for_status()

            data = response.json()

            # Load used images tracking file
            used_images_file = Path(__file__).parent.parent / "data" / "used_images.json"
            used_images_meta_file = Path(__file__).parent.parent / "data" / "used_images_metadata.json"

            # Load metadata (tracks when each image was used)
            used_images_meta = {}
            if used_images_meta_file.exists():
                try:
                    with open(used_images_meta_file, 'r') as f:
                        used_images_meta = json.load(f)
                except:
                    pass

            # Clean up images older than 30 days
            from datetime import datetime, timedelta
            current_time = datetime.now().timestamp()
            cutoff_time = (datetime.now() - timedelta(days=30)).timestamp()

            cleaned_meta = {}
            for img_id, timestamp in used_images_meta.items():
                # Skip corrupted entries (non-numeric timestamps)
                if not isinstance(timestamp, (int, float)):
                    continue
                if timestamp > cutoff_time:
                    cleaned_meta[img_id] = timestamp

            # Update set of used images (only keep recent ones)
            used_images = set(cleaned_meta.keys())

            # Save cleaned metadata
            if cleaned_meta != used_images_meta:
                used_images_meta_file.parent.mkdir(parents=True, exist_ok=True)
                with open(used_images_meta_file, 'w') as f:
                    json.dump(cleaned_meta, f, indent=2)
                if len(used_images_meta) > len(cleaned_meta):
                    safe_print(f"  ðŸ—‘ï¸  Cleaned up {len(used_images_meta) - len(cleaned_meta)} images older than 30 days")

            used_images_meta = cleaned_meta

            # Find first unused image from results
            photo = None
            if data.get('results'):
                for result in data['results']:
                    image_id = result['id']
                    if image_id not in used_images:
                        photo = result
                        used_images.add(image_id)
                        used_images_meta[image_id] = current_time
                        break
            else:
                safe_print(f"  âš ï¸  No images found for '{query}'")

            # If no results or all images are used, try fallback queries
            if photo is None and fallback_queries:
                for fallback_query in fallback_queries:
                    safe_print(f"  âš ï¸  No unused images for '{query}', trying: {fallback_query}")
                    params['query'] = fallback_query

                    response = requests.get(url, headers=headers, params=params, timeout=10, verify=verify_ssl)
                    response.raise_for_status()
                    data = response.json()

                    if data.get('results'):
                        for result in data['results']:
                            image_id = result['id']
                            if image_id not in used_images:
                                photo = result
                                used_images.add(image_id)
                                used_images_meta[image_id] = current_time
                                safe_print(f"  âœ“ Found unused image with fallback: {fallback_query}")
                                break

                    # If found, stop trying fallbacks
                    if photo is not None:
                        break

                # If still no unused image found, return None (use placeholder)
                if photo is None:
                    safe_print(f"  âŒ No unused images available after trying all fallbacks for category '{category}'")
                    return None

            # Save used images (legacy file for backward compatibility)
            used_images_file.parent.mkdir(parents=True, exist_ok=True)
            with open(used_images_file, 'w') as f:
                json.dump(list(used_images), f)

            # Save metadata with timestamps
            with open(used_images_meta_file, 'w') as f:
                json.dump(used_images_meta, f, indent=2)

            image_info = {
                'url': photo['urls']['regular'],
                'download_url': photo['links']['download_location'],
                'photographer': photo['user']['name'],
                'photographer_url': photo['user']['links']['html'],
                'unsplash_url': photo['links']['html'],
                'image_id': photo['id']
            }

            safe_print(f"  âœ“ Found image by {image_info['photographer']}")
            return image_info

        except requests.exceptions.Timeout as e:
            safe_print(f"  âš ï¸  Unsplash API timeout: Request took too long")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None
        except requests.exceptions.HTTPError as e:
            safe_print(f"  âš ï¸  Unsplash API HTTP error: {e.response.status_code if e.response else 'unknown'}")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None
        except requests.exceptions.RequestException as e:
            safe_print(f"  âš ï¸  Unsplash API network error")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None
        except json.JSONDecodeError as e:
            safe_print(f"  âš ï¸  Unsplash API response parsing failed")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: Invalid JSON response")
            return None
        except Exception as e:
            safe_print(f"  âš ï¸  Image fetch failed with unexpected error")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None

    def download_image(self, image_info: Dict, keyword: str) -> Optional[str]:
        """Download optimized image to static/images/ directory with hash-based duplicate detection"""
        if not image_info:
            return None

        try:
            # Create images directory
            images_dir = Path("static/images")
            images_dir.mkdir(parents=True, exist_ok=True)

            # Generate filename
            slug = keyword.lower()
            # Allow Unicode characters (CJK) in filenames
            slug = ''.join(c if c.isalnum() or c.isspace() or ord(c) > 127 else '' for c in slug)
            slug = slug.replace(' ', '-')[:30]
            # Use KST for image filename
            from datetime import timezone, timedelta
            kst = timezone(timedelta(hours=9))
            date_str = datetime.now(kst).strftime("%Y%m%d")
            filename = f"{date_str}-{slug}.jpg"
            filepath = images_dir / filename

            # Trigger Unsplash download tracking (required by API terms)
            if image_info.get('download_url'):
                verify_ssl = certifi.where() if certifi else True
                requests.get(
                    image_info['download_url'],
                    headers={"Authorization": f"Client-ID {self.unsplash_key}"},
                    timeout=5,
                    verify=verify_ssl
                )

            # Download optimized image (1200px width, quality 85)
            # Use Unsplash's regular URL which already includes optimization
            download_url = image_info.get('url', '')
            # Add additional optimization parameters
            if '?' in download_url:
                optimized_url = f"{download_url}&w=1200&q=85&fm=jpg"
            else:
                optimized_url = f"{download_url}?w=1200&q=85&fm=jpg"

            safe_print(f"  ðŸ“¥ Downloading optimized image (1200px, q85)...")
            # Use certifi for SSL verification (Windows compatibility)
            verify_ssl = certifi.where() if certifi else True
            response = requests.get(optimized_url, timeout=15, verify=verify_ssl)
            response.raise_for_status()

            # Calculate MD5 hash of downloaded content
            import hashlib
            content_hash = hashlib.md5(response.content).hexdigest()

            # Check for duplicate images by hash
            duplicate_found = False
            for existing_file in images_dir.glob("*.jpg"):
                if existing_file.exists():
                    try:
                        with open(existing_file, 'rb') as f:
                            existing_hash = hashlib.md5(f.read()).hexdigest()
                        if existing_hash == content_hash:
                            safe_print(f"  âš ï¸  Duplicate image detected (same content as {existing_file.name})")
                            duplicate_found = True
                            break
                    except:
                        pass

            # If duplicate found, skip saving but still return the new filename
            # (The image will be saved with a new name to maintain unique URLs)
            if duplicate_found:
                safe_print(f"  â„¹ï¸  Saving with new filename to maintain unique URL: {filename}")

            # Save image (even if duplicate, to maintain URL uniqueness per post)
            with open(filepath, 'wb') as f:
                f.write(response.content)

            size_kb = len(response.content) / 1024
            safe_print(f"  âœ“ Image saved: {filepath} ({size_kb:.1f} KB)")

            # Log hash for debugging
            safe_print(f"  ðŸ”‘ Image hash: {content_hash[:8]}...")

            # Return relative path for Hugo
            return f"/images/{filename}"

        except requests.exceptions.Timeout as e:
            safe_print(f"  âš ï¸  Image download timeout")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     URL: {optimized_url[:80]}...")
            return None
        except requests.exceptions.HTTPError as e:
            safe_print(f"  âš ï¸  Image download HTTP error: {e.response.status_code if e.response else 'unknown'}")
            safe_print(f"     Keyword: {keyword}")
            return None
        except IOError as e:
            safe_print(f"  âš ï¸  File system error during image save")
            safe_print(f"     Path: {filepath}")
            safe_print(f"     Error: {str(e)}")
            return None
        except Exception as e:
            safe_print(f"  âš ï¸  Image download failed with unexpected error")
            safe_print(f"     Keyword: {keyword}")
            safe_print(f"     Error: {mask_secrets(str(e))}")
            return None

    def save_post(self, topic: Dict, title: str, description: str, content: str, image_path: Optional[str] = None, image_credit: Optional[Dict] = None, faq_items: Optional[list] = None, technologies: Optional[list] = None) -> Path:
        """Save post to Hugo content directory"""
        lang = topic['lang']
        category = topic['category']
        keyword = topic['keyword']

        # Generate filename from keyword
        slug = keyword.lower()
        # Remove special characters, keep alphanumeric and spaces
        slug = ''.join(c if c.isalnum() or c.isspace() else '' for c in slug)
        slug = slug.replace(' ', '-')[:50]

        # Create directory
        content_dir = Path(f"content/{lang}/{category}")
        content_dir.mkdir(parents=True, exist_ok=True)

        # Generate filename with date in KST
        from datetime import timezone, timedelta
        kst = timezone(timedelta(hours=9))
        date_str = datetime.now(kst).strftime("%Y-%m-%d")
        filename = f"{date_str}-{slug}.md"
        filepath = content_dir / filename

        # A/B Testing Integration (50% chance)
        import random
        ab_test_id = None
        ab_variant = None

        if self.ab_test_manager.should_run_test("title_style"):
            # Generate post ID for consistent assignment
            post_id = f"{date_str}-{slug}"

            # Assign variant
            ab_variant = self.ab_test_manager.assign_variant(post_id, "title_style")

            if ab_variant:
                ab_test_id = "title_style"

                # Generate title variants
                title_variants = self.ab_test_manager.generate_title_variants(title, lang)

                # Apply variant title
                if ab_variant in title_variants:
                    original_title = title
                    title = title_variants[ab_variant]
                    safe_print(f"  ðŸ§ª A/B Test: title_style (Variant {ab_variant})")
                    safe_print(f"     Original: {original_title}")
                    safe_print(f"     Modified: {title}")

        # Hugo frontmatter with required image field
        # Use placeholder if no Unsplash image available
        if not image_path:
            # Use category-based placeholder
            image_path = f"/images/placeholder-{category}.jpg"

        # Use KST timezone for date
        from datetime import timezone, timedelta
        kst = timezone(timedelta(hours=9))
        now_kst = datetime.now(kst)

        # Escape nested quotes in YAML frontmatter
        safe_title = title.replace('"', "'")
        safe_description = description.replace('"', "'")

        # Build frontmatter with optional A/B test metadata
        frontmatter_lines = [
            "---",
            f'title: "{safe_title}"',
            f'date: {now_kst.strftime("%Y-%m-%dT%H:%M:%S%z")}',
            "draft: false",
            'author: "Jake Park"',
            f'categories: ["{category}"]',
            f'tags: {json.dumps(keyword.split()[:3])}',
            f'description: "{safe_description}"',
            f'image: "{image_path}"'
        ]

        # Add A/B test metadata if applicable
        if ab_test_id and ab_variant:
            frontmatter_lines.append(f'ab_test_id: "{ab_test_id}"')
            frontmatter_lines.append(f'ab_variant: "{ab_variant}"')

        # Add technologies taxonomy for tech posts
        if technologies and category == 'tech':
            frontmatter_lines.append(f'technologies: {json.dumps(technologies)}')

        # Add FAQ items for FAQPage schema
        if faq_items:
            frontmatter_lines.append("faq:")
            for item in faq_items:
                safe_q = item.get('question', '').replace('"', "'")
                safe_a = item.get('answer', '').replace('"', "'")
                frontmatter_lines.append(f'  - question: "{safe_q}"')
                frontmatter_lines.append(f'    answer: "{safe_a}"')

        frontmatter_lines.extend(["---", ""])

        frontmatter = "\n".join(frontmatter_lines) + "\n"

        # Hero image removed - PaperMod theme renders frontmatter image: as cover
        hero_image = ""

        # Add image credit at the end of content if available
        credit_line = ""
        if image_credit:
            credit_line = f"\n\n---\n\n*Photo by [{image_credit['photographer']}]({image_credit['photographer_url']}) on [Unsplash]({image_credit['unsplash_url']})*\n"

        # Validate References section and remove if it contains fake URLs
        def has_fake_reference_url(url: str) -> bool:
            """Check if URL is a fake reference"""
            fake_patterns = [
                r'example\.com',
                r'example\.org',
                r'\.gov/[a-z-]+-202[0-9]',
                r'\.org/[a-z-]+-survey',
                r'\.gov/[a-z-]+-compliance',
                r'\.gov/[a-z-]+-report',
            ]
            for pattern in fake_patterns:
                if re.search(pattern, url, re.IGNORECASE):
                    return True
            return False

        # Check if References section exists - if not, just skip it (don't add fake references)
        ref_headers = {
            'en': '## References',
            'ko': '## ì°¸ê³ ìžë£Œ'
        }
        ref_header = ref_headers.get(lang, '## References')

        # First, normalize any non-standard reference formats to standard format
        # Remove bold "**References:**" format if exists (common Claude output)
        bold_ref_patterns = [
            (r'\*\*References?:\*\*\n', ''),  # **References:**
            (r'\*\*ì°¸ê³ ìžë£Œ:\*\*\n', '')  # **ì°¸ê³ ìžë£Œ:**
        ]
        for pattern, replacement in bold_ref_patterns:
            content = re.sub(pattern, replacement, content)

        # Extract References section if exists
        has_references = ref_header in content or '## Reference' in content or '## ì°¸ê³ ' in content

        if has_references:
            # Extract URLs from References section using regex
            # Pattern: [text](url) or bare URLs
            url_pattern = r'https?://[^\s\)\]<>"]+'  
            urls_in_content = re.findall(url_pattern, content)

            # Check if any URLs are fake
            fake_urls = [url for url in urls_in_content if has_fake_reference_url(url)]

            if fake_urls:
                safe_print(f"  âš ï¸  Fake reference URLs detected: {len(fake_urls)} found")
                safe_print(f"      Examples: {fake_urls[:3]}")

                # Remove References section entirely
                # Match from any References header to the next ## header or end of content
                ref_pattern = r'\n## (?:References?|ì°¸ê³ ìžë£Œ)\n.*?(?=\n## |\Z)'
                content = re.sub(ref_pattern, '', content, flags=re.DOTALL)
                safe_print(f"  ðŸ—‘ï¸  Removed References section with fake URLs")
                has_references = False  # Mark as no valid references
            else:
                safe_print(f"  âœ… References section validated ({len(urls_in_content)} URLs)")

        # If no valid References section exists, add from queue
        if not has_references and topic.get('references'):
            references = topic['references']
            safe_print(f"  â„¹ï¸  No References section in content, adding from queue ({len(references)} refs)")

            # Build References section
            ref_section = f"\n\n{ref_header}\n\n"
            for i, ref in enumerate(references, 1):
                ref_section += f"{i}. [{ref['title']}]({ref['url']})\n"

            # Append to content
            content = content.rstrip() + ref_section
            safe_print(f"  âœ… Added {len(references)} references from queue")
        elif not has_references:
            safe_print(f"  â„¹ï¸  No references available (neither in content nor queue)")

        # Add affiliate links if applicable
        affiliate_programs_used = []
        if should_add_affiliate_links(category):
            safe_print(f"  ðŸ”— Checking for product mentions to add affiliate links...")

            # Detect products mentioned in content
            detected_products = detect_product_mentions(content, lang, category)

            if detected_products:
                safe_print(f"  ðŸ“¦ Detected {len(detected_products)} products: {', '.join(detected_products[:3])}")

                # Add affiliate link for the first detected product only (to avoid being too commercial)
                primary_product = detected_products[0]
                link_data = generate_affiliate_link(primary_product, lang)

                if link_data:
                    # Find insertion point: after first ## section
                    sections = content.split('\n## ')
                    if len(sections) > 1:
                        # Insert after first section
                        affiliate_box = create_affiliate_box(primary_product, lang, link_data)
                        sections[1] = sections[1] + '\n' + affiliate_box
                        content = '\n## '.join(sections)

                        affiliate_programs_used.append(link_data['program'])
                        safe_print(f"  âœ… Added affiliate link for '{primary_product}' ({link_data['program']})")
                    else:
                        safe_print(f"  âš ï¸  Could not find insertion point for affiliate link")
                else:
                    safe_print(f"  â„¹ï¸  No affiliate program configured for {lang}")
            else:
                safe_print(f"  â„¹ï¸  No product mentions detected")
        else:
            safe_print(f"  â„¹ï¸  Affiliate links disabled for category: {category}")

        # Add affiliate disclosure if links were added
        if affiliate_programs_used:
            disclosure = get_affiliate_disclosure(lang, affiliate_programs_used)
            content = content.rstrip() + disclosure
            safe_print(f"  âš ï¸  Added affiliate disclosure")

        # Internal linking removed - Hugo template handles related posts automatically

        # Write file with hero image at top
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(frontmatter)
            f.write(hero_image)
            f.write(content)
            f.write(credit_line)

        safe_print(f"  ðŸ’¾ Saved to: {filepath}")
        return filepath


def main():
    parser = argparse.ArgumentParser(description="Generate blog posts")
    parser.add_argument("--count", type=int, default=3, help="Number of posts to generate")
    parser.add_argument("--topic-id", type=str, help="Specific topic ID to generate")
    args = parser.parse_args()

    # Pre-flight checks
    safe_print(f"\n{'='*60}")
    safe_print(f"  ðŸ” Pre-flight Environment Checks")
    safe_print(f"{'='*60}\n")

    anthropic_key = os.environ.get("ANTHROPIC_API_KEY")
    unsplash_key = os.environ.get("UNSPLASH_ACCESS_KEY")

    if anthropic_key:
        safe_print("  âœ“ ANTHROPIC_API_KEY: Configured")
    else:
        safe_print("  âŒ ANTHROPIC_API_KEY: NOT FOUND")

    if unsplash_key:
        safe_print("  âœ“ UNSPLASH_ACCESS_KEY: Configured")
    else:
        safe_print("  âš ï¸  UNSPLASH_ACCESS_KEY: NOT FOUND")
        safe_print("     Posts will use placeholder images!")

    safe_print("")

    # Initialize generator
    try:
        generator = ContentGenerator()
    except ValueError as e:
        safe_print(f"Error: {str(e)}")
        safe_print("\nSet ANTHROPIC_API_KEY environment variable:")
        safe_print("  export ANTHROPIC_API_KEY='your-api-key'")
        sys.exit(1)

    # Get topics
    if args.topic_id:
        # Load specific topic (for testing)
        from topic_queue import get_queue
        queue = get_queue()
        data = queue._load_queue()
        topics = [t for t in data['topics'] if t['id'] == args.topic_id]
        if not topics:
            safe_print(f"Error: Topic {args.topic_id} not found")
            sys.exit(1)
    else:
        # Reserve topics from queue
        topics = reserve_topics(count=args.count)

    if not topics:
        safe_print("No topics available in queue")
        sys.exit(0)

    safe_print(f"\n{'='*60}")
    safe_print(f"  Generating {len(topics)} posts")
    safe_print(f"{'='*60}\n")

    generated_files = []

    for i, topic in enumerate(topics, 1):
        safe_print(f"[{i}/{len(topics)}] {topic['id']}")
        safe_print(f"  Keyword: {topic['keyword']}")
        safe_print(f"  Category: {topic['category']}")
        safe_print(f"  Language: {topic['lang']}")

        try:
            # Generate content
            safe_print(f"  â†’ Step 1/7: Generating draft...")
            draft, content_type = generator.generate_draft(topic)

            safe_print(f"  â†’ Step 2/7: Editing draft...")
            final_content = generator.edit_draft(draft, topic, content_type)

            # Generate metadata
            safe_print(f"  â†’ Step 3/7: Generating metadata...")
            try:
                title = generator.generate_title(final_content, topic['keyword'], topic['lang'], topic.get('references'))
                description = generator.generate_description(final_content, topic['keyword'], topic['lang'])
            except Exception as e:
                safe_print(f"  âš ï¸  WARNING: Metadata generation failed, using defaults")
                safe_print(f"     Error: {mask_secrets(str(e))}")
                title = topic['keyword']
                description = f"Article about {topic['keyword']}"

            # Fetch featured image
            safe_print(f"  â†’ Step 4/7: Fetching image...")
            image_path = None
            image_credit = None
            try:
                image_info = generator.fetch_featured_image(topic['keyword'], topic['category'])
                if image_info:
                    image_path = generator.download_image(image_info, topic['keyword'])
                    if image_path:
                        image_credit = image_info
            except Exception as e:
                safe_print(f"  âš ï¸  WARNING: Image fetch failed, will use placeholder")
                safe_print(f"     Error: {mask_secrets(str(e))}")

            # Generate FAQ for AEO
            faq_items = []
            try:
                safe_print(f"  â†’ Step 5/7: Generating FAQ...")
                faq_items = generator.generate_faq(final_content, topic['keyword'], topic['lang'])
                safe_print(f"     FAQ: {len(faq_items)} items generated")
            except Exception as e:
                safe_print(f"  âš ï¸  WARNING: FAQ generation failed: {mask_secrets(str(e))}")

            # Extract technologies for tech posts
            technologies = None
            if topic.get('category') == 'tech':
                try:
                    safe_print(f"  â†’ Step 6/7: Extracting technologies...")
                    technologies = generator.extract_technologies(final_content, topic['keyword'])
                    if technologies:
                        safe_print(f"     Technologies: {', '.join(technologies)}")
                except Exception as e:
                    safe_print(f"  âš ï¸  WARNING: Technology extraction failed: {mask_secrets(str(e))}")

            # Save post with image, FAQ, and technologies
            safe_print(f"  â†’ Step 7/7: Saving post...")
            try:
                filepath = generator.save_post(topic, title, description, final_content, image_path, image_credit, faq_items, technologies)
            except IOError as e:
                safe_print(f"  âŒ ERROR: Failed to save post to filesystem")
                safe_print(f"     Error: {str(e)}")
                raise
            except Exception as e:
                safe_print(f"  âŒ ERROR: Unexpected error during save")
                safe_print(f"     Error: {mask_secrets(str(e))}")
                raise

            # Mark as completed
            if not args.topic_id:
                try:
                    mark_completed(topic['id'])
                except Exception as e:
                    safe_print(f"  âš ï¸  WARNING: Failed to mark topic as completed in queue")
                    safe_print(f"     Topic ID: {topic['id']}")
                    safe_print(f"     Error: {str(e)}")
                    # Don't fail the whole process if queue update fails

            generated_files.append(str(filepath))
            safe_print(f"  âœ… Completed!\n")

        except KeyError as e:
            safe_print(f"  âŒ FAILED: Missing required field in topic data")
            safe_print(f"     Topic ID: {topic.get('id', 'unknown')}")
            safe_print(f"     Missing field: {str(e)}\n")
            if not args.topic_id:
                mark_failed(topic['id'], f"Missing field: {str(e)}")
        except ValueError as e:
            safe_print(f"  âŒ FAILED: Invalid data or API response")
            safe_print(f"     Topic ID: {topic.get('id', 'unknown')}")
            safe_print(f"     Error: {mask_secrets(str(e))}\n")
            if not args.topic_id:
                mark_failed(topic['id'], mask_secrets(str(e)))
        except Exception as e:
            safe_print(f"  âŒ FAILED: Unexpected error")
            safe_print(f"     Topic ID: {topic.get('id', 'unknown')}")
            safe_print(f"     Error type: {type(e).__name__}")
            safe_print(f"     Error: {mask_secrets(str(e))}\n")
            if not args.topic_id:
                mark_failed(topic['id'], mask_secrets(str(e)))

    # Save generated files list for quality gate
    output_file = Path("generated_files.json")
    with open(output_file, 'w') as f:
        json.dump(generated_files, f, indent=2)

    # Post-generation quality check
    safe_print(f"\n{'='*60}")
    safe_print(f"  ðŸ“Š Post-Generation Quality Check")
    safe_print(f"{'='*60}\n")

    posts_without_references = 0
    posts_with_placeholders = 0

    for filepath in generated_files:
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()

                # Check for references section
                has_references = '## References' in content or '## å‚è€ƒ' in content or '## ì°¸ê³ ìžë£Œ' in content
                if not has_references:
                    posts_without_references += 1
                    safe_print(f"  âš ï¸  No references: {Path(filepath).name}")

                # Check for placeholder images
                if 'placeholder-' in content:
                    posts_with_placeholders += 1
                    safe_print(f"  âš ï¸  Placeholder image: {Path(filepath).name}")
        except Exception as e:
            safe_print(f"  âš ï¸  Could not check: {Path(filepath).name}")

    safe_print("")

    if posts_without_references > 0:
        safe_print(f"ðŸš¨ WARNING: {posts_without_references}/{len(generated_files)} posts have NO references!")
        safe_print(f"   This reduces content credibility and SEO value.")
        safe_print(f"   FIX: Ensure Google Custom Search API is configured in keyword curation\n")

    if posts_with_placeholders > 0:
        safe_print(f"ðŸš¨ WARNING: {posts_with_placeholders}/{len(generated_files)} posts use PLACEHOLDER images!")
        safe_print(f"   This hurts user experience and engagement.")
        safe_print(f"   FIX: Ensure UNSPLASH_ACCESS_KEY is set in environment variables\n")

    if posts_without_references == 0 and posts_with_placeholders == 0:
        safe_print(f"âœ… Quality Check PASSED: All posts have references and real images!\n")

    safe_print(f"{'='*60}")
    safe_print(f"  âœ“ Generated {len(generated_files)} posts")
    safe_print(f"  File list saved to: {output_file}")
    safe_print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
